{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cria sessão Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instância Spark\n",
    "spark = SparkSession.builder.appName('Curso Pyspark') \\\n",
    "         .config('spark.sql.repl.eagerEval.enabled', True) \\\n",
    "         .getOrCreate()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module pyspark.sql.functions in pyspark.sql:\n",
      "\n",
      "NAME\n",
      "    pyspark.sql.functions - A collections of builtin functions\n",
      "\n",
      "FUNCTIONS\n",
      "    abs(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the absolute value.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(abs(lit(-1))).show()\n",
      "        +-------+\n",
      "        |abs(-1)|\n",
      "        +-------+\n",
      "        |      1|\n",
      "        +-------+\n",
      "    \n",
      "    acos(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes inverse cosine of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            inverse cosine of `col`, as if computed by `java.lang.Math.acos()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1, 3)\n",
      "        >>> df.select(acos(df.id)).show()\n",
      "        +--------+\n",
      "        |ACOS(id)|\n",
      "        +--------+\n",
      "        |     0.0|\n",
      "        |     NaN|\n",
      "        +--------+\n",
      "    \n",
      "    acosh(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes inverse hyperbolic cosine of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(2)\n",
      "        >>> df.select(acosh(col(\"id\"))).show()\n",
      "        +---------+\n",
      "        |ACOSH(id)|\n",
      "        +---------+\n",
      "        |      NaN|\n",
      "        |      0.0|\n",
      "        +---------+\n",
      "    \n",
      "    add_months(start: 'ColumnOrName', months: Union[ForwardRef('ColumnOrName'), int]) -> pyspark.sql.column.Column\n",
      "        Returns the date that is `months` months after `start`. If `months` is a negative value\n",
      "        then these amount of months will be deducted from the `start`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        start : :class:`~pyspark.sql.Column` or str\n",
      "            date column to work on.\n",
      "        months : :class:`~pyspark.sql.Column` or str or int\n",
      "            how many months after the given date to calculate.\n",
      "            Accepts negative value as well to calculate backwards.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a date after/before given number of months.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08', 2)], ['dt', 'add'])\n",
      "        >>> df.select(add_months(df.dt, 1).alias('next_month')).collect()\n",
      "        [Row(next_month=datetime.date(2015, 5, 8))]\n",
      "        >>> df.select(add_months(df.dt, df.add.cast('integer')).alias('next_month')).collect()\n",
      "        [Row(next_month=datetime.date(2015, 6, 8))]\n",
      "        >>> df.select(add_months('dt', -2).alias('prev_month')).collect()\n",
      "        [Row(prev_month=datetime.date(2015, 2, 8))]\n",
      "    \n",
      "    aes_decrypt(input: 'ColumnOrName', key: 'ColumnOrName', mode: Optional[ForwardRef('ColumnOrName')] = None, padding: Optional[ForwardRef('ColumnOrName')] = None, aad: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Returns a decrypted value of `input` using AES in `mode` with `padding`. Key lengths of 16,\n",
      "        24 and 32 bits are supported. Supported combinations of (`mode`, `padding`) are ('ECB',\n",
      "        'PKCS'), ('GCM', 'NONE') and ('CBC', 'PKCS'). Optional additional authenticated data (AAD) is\n",
      "        only supported for GCM. If provided for encryption, the identical AAD value must be provided\n",
      "        for decryption. The default mode is GCM.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        input : :class:`~pyspark.sql.Column` or str\n",
      "            The binary value to decrypt.\n",
      "        key : :class:`~pyspark.sql.Column` or str\n",
      "            The passphrase to use to decrypt the data.\n",
      "        mode : :class:`~pyspark.sql.Column` or str, optional\n",
      "            Specifies which block cipher mode should be used to decrypt messages. Valid modes: ECB,\n",
      "            GCM, CBC.\n",
      "        padding : :class:`~pyspark.sql.Column` or str, optional\n",
      "            Specifies how to pad messages whose length is not a multiple of the block size. Valid\n",
      "            values: PKCS, NONE, DEFAULT. The DEFAULT padding means PKCS for ECB, NONE for GCM and PKCS\n",
      "            for CBC.\n",
      "        aad : :class:`~pyspark.sql.Column` or str, optional\n",
      "            Optional additional authenticated data. Only supported for GCM mode. This can be any\n",
      "            free-form input and must be provided for both encryption and decryption.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\n",
      "        ...     \"AAAAAAAAAAAAAAAAQiYi+sTLm7KD9UcZ2nlRdYDe/PX4\",\n",
      "        ...     \"abcdefghijklmnop12345678ABCDEFGH\", \"GCM\", \"DEFAULT\",\n",
      "        ...     \"This is an AAD mixed into the input\",)],\n",
      "        ...     [\"input\", \"key\", \"mode\", \"padding\", \"aad\"]\n",
      "        ... )\n",
      "        >>> df.select(aes_decrypt(\n",
      "        ...     unbase64(df.input), df.key, df.mode, df.padding, df.aad).alias('r')\n",
      "        ... ).collect()\n",
      "        [Row(r=bytearray(b'Spark'))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\n",
      "        ...     \"AAAAAAAAAAAAAAAAAAAAAPSd4mWyMZ5mhvjiAPQJnfg=\",\n",
      "        ...     \"abcdefghijklmnop12345678ABCDEFGH\", \"CBC\", \"DEFAULT\",)],\n",
      "        ...     [\"input\", \"key\", \"mode\", \"padding\"]\n",
      "        ... )\n",
      "        >>> df.select(aes_decrypt(\n",
      "        ...     unbase64(df.input), df.key, df.mode, df.padding).alias('r')\n",
      "        ... ).collect()\n",
      "        [Row(r=bytearray(b'Spark'))]\n",
      "        \n",
      "        >>> df.select(aes_decrypt(unbase64(df.input), df.key, df.mode).alias('r')).collect()\n",
      "        [Row(r=bytearray(b'Spark'))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\n",
      "        ...     \"83F16B2AA704794132802D248E6BFD4E380078182D1544813898AC97E709B28A94\",\n",
      "        ...     \"0000111122223333\",)],\n",
      "        ...     [\"input\", \"key\"]\n",
      "        ... )\n",
      "        >>> df.select(aes_decrypt(unhex(df.input), df.key).alias('r')).collect()\n",
      "        [Row(r=bytearray(b'Spark'))]\n",
      "    \n",
      "    aes_encrypt(input: 'ColumnOrName', key: 'ColumnOrName', mode: Optional[ForwardRef('ColumnOrName')] = None, padding: Optional[ForwardRef('ColumnOrName')] = None, iv: Optional[ForwardRef('ColumnOrName')] = None, aad: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Returns an encrypted value of `input` using AES in given `mode` with the specified `padding`.\n",
      "        Key lengths of 16, 24 and 32 bits are supported. Supported combinations of (`mode`,\n",
      "        `padding`) are ('ECB', 'PKCS'), ('GCM', 'NONE') and ('CBC', 'PKCS'). Optional initialization\n",
      "        vectors (IVs) are only supported for CBC and GCM modes. These must be 16 bytes for CBC and 12\n",
      "        bytes for GCM. If not provided, a random vector will be generated and prepended to the\n",
      "        output. Optional additional authenticated data (AAD) is only supported for GCM. If provided\n",
      "        for encryption, the identical AAD value must be provided for decryption. The default mode is\n",
      "        GCM.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        input : :class:`~pyspark.sql.Column` or str\n",
      "            The binary value to encrypt.\n",
      "        key : :class:`~pyspark.sql.Column` or str\n",
      "            The passphrase to use to encrypt the data.\n",
      "        mode : :class:`~pyspark.sql.Column` or str, optional\n",
      "            Specifies which block cipher mode should be used to encrypt messages. Valid modes: ECB,\n",
      "            GCM, CBC.\n",
      "        padding : :class:`~pyspark.sql.Column` or str, optional\n",
      "            Specifies how to pad messages whose length is not a multiple of the block size. Valid\n",
      "            values: PKCS, NONE, DEFAULT. The DEFAULT padding means PKCS for ECB, NONE for GCM and PKCS\n",
      "            for CBC.\n",
      "        iv : :class:`~pyspark.sql.Column` or str, optional\n",
      "            Optional initialization vector. Only supported for CBC and GCM modes. Valid values: None or\n",
      "            \"\". 16-byte array for CBC mode. 12-byte array for GCM mode.\n",
      "        aad : :class:`~pyspark.sql.Column` or str, optional\n",
      "            Optional additional authenticated data. Only supported for GCM mode. This can be any\n",
      "            free-form input and must be provided for both encryption and decryption.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\n",
      "        ...     \"Spark\", \"abcdefghijklmnop12345678ABCDEFGH\", \"GCM\", \"DEFAULT\",\n",
      "        ...     \"000000000000000000000000\", \"This is an AAD mixed into the input\",)],\n",
      "        ...     [\"input\", \"key\", \"mode\", \"padding\", \"iv\", \"aad\"]\n",
      "        ... )\n",
      "        >>> df.select(base64(aes_encrypt(\n",
      "        ...     df.input, df.key, df.mode, df.padding, to_binary(df.iv, lit(\"hex\")), df.aad)\n",
      "        ... ).alias('r')).collect()\n",
      "        [Row(r='AAAAAAAAAAAAAAAAQiYi+sTLm7KD9UcZ2nlRdYDe/PX4')]\n",
      "        \n",
      "        >>> df.select(base64(aes_encrypt(\n",
      "        ...     df.input, df.key, df.mode, df.padding, to_binary(df.iv, lit(\"hex\")))\n",
      "        ... ).alias('r')).collect()\n",
      "        [Row(r='AAAAAAAAAAAAAAAAQiYi+sRNYDAOTjdSEcYBFsAWPL1f')]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\n",
      "        ...     \"Spark SQL\", \"1234567890abcdef\", \"ECB\", \"PKCS\",)],\n",
      "        ...     [\"input\", \"key\", \"mode\", \"padding\"]\n",
      "        ... )\n",
      "        >>> df.select(aes_decrypt(aes_encrypt(df.input, df.key, df.mode, df.padding),\n",
      "        ...     df.key, df.mode, df.padding).alias('r')\n",
      "        ... ).collect()\n",
      "        [Row(r=bytearray(b'Spark SQL'))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\n",
      "        ...     \"Spark SQL\", \"0000111122223333\", \"ECB\",)],\n",
      "        ...     [\"input\", \"key\", \"mode\"]\n",
      "        ... )\n",
      "        >>> df.select(aes_decrypt(aes_encrypt(df.input, df.key, df.mode),\n",
      "        ...     df.key, df.mode).alias('r')\n",
      "        ... ).collect()\n",
      "        [Row(r=bytearray(b'Spark SQL'))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\n",
      "        ...     \"Spark SQL\", \"abcdefghijklmnop\",)],\n",
      "        ...     [\"input\", \"key\"]\n",
      "        ... )\n",
      "        >>> df.select(aes_decrypt(\n",
      "        ...     unbase64(base64(aes_encrypt(df.input, df.key))), df.key\n",
      "        ... ).cast(\"STRING\").alias('r')).collect()\n",
      "        [Row(r='Spark SQL')]\n",
      "    \n",
      "    aggregate(col: 'ColumnOrName', initialValue: 'ColumnOrName', merge: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column], finish: Optional[Callable[[pyspark.sql.column.Column], pyspark.sql.column.Column]] = None) -> pyspark.sql.column.Column\n",
      "        Applies a binary operator to an initial state and all elements in the array,\n",
      "        and reduces this to a single state. The final state is converted into the final result\n",
      "        by applying a finish function.\n",
      "        \n",
      "        Both functions can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "        :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "        Python ``UserDefinedFunctions`` are not supported\n",
      "        (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        initialValue : :class:`~pyspark.sql.Column` or str\n",
      "            initial value. Name of column or expression\n",
      "        merge : function\n",
      "            a binary function ``(acc: Column, x: Column) -> Column...`` returning expression\n",
      "            of the same type as ``zero``\n",
      "        finish : function\n",
      "            an optional unary function ``(x: Column) -> Column: ...``\n",
      "            used to convert accumulated value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            final value after aggregate function is applied.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, [20.0, 4.0, 2.0, 6.0, 10.0])], (\"id\", \"values\"))\n",
      "        >>> df.select(aggregate(\"values\", lit(0.0), lambda acc, x: acc + x).alias(\"sum\")).show()\n",
      "        +----+\n",
      "        | sum|\n",
      "        +----+\n",
      "        |42.0|\n",
      "        +----+\n",
      "        \n",
      "        >>> def merge(acc, x):\n",
      "        ...     count = acc.count + 1\n",
      "        ...     sum = acc.sum + x\n",
      "        ...     return struct(count.alias(\"count\"), sum.alias(\"sum\"))\n",
      "        ...\n",
      "        >>> df.select(\n",
      "        ...     aggregate(\n",
      "        ...         \"values\",\n",
      "        ...         struct(lit(0).alias(\"count\"), lit(0.0).alias(\"sum\")),\n",
      "        ...         merge,\n",
      "        ...         lambda acc: acc.sum / acc.count,\n",
      "        ...     ).alias(\"mean\")\n",
      "        ... ).show()\n",
      "        +----+\n",
      "        |mean|\n",
      "        +----+\n",
      "        | 8.4|\n",
      "        +----+\n",
      "    \n",
      "    any_value(col: 'ColumnOrName', ignoreNulls: Union[bool, pyspark.sql.column.Column, NoneType] = None) -> pyspark.sql.column.Column\n",
      "        Returns some value of `col` for a group of rows.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        ignorenulls : :class:`~pyspark.sql.Column` or bool\n",
      "            if first value is null then look for first non-null value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            some value of `col` for a group of rows.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(None, 1),\n",
      "        ...                             (\"a\", 2),\n",
      "        ...                             (\"a\", 3),\n",
      "        ...                             (\"b\", 8),\n",
      "        ...                             (\"b\", 2)], [\"c1\", \"c2\"])\n",
      "        >>> df.select(any_value('c1'), any_value('c2')).collect()\n",
      "        [Row(any_value(c1)=None, any_value(c2)=1)]\n",
      "        >>> df.select(any_value('c1', True), any_value('c2', True)).collect()\n",
      "        [Row(any_value(c1)='a', any_value(c2)=1)]\n",
      "    \n",
      "    approxCountDistinct(col: 'ColumnOrName', rsd: Optional[float] = None) -> pyspark.sql.column.Column\n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        .. deprecated:: 2.1.0\n",
      "            Use :func:`approx_count_distinct` instead.\n",
      "    \n",
      "    approx_count_distinct(col: 'ColumnOrName', rsd: Optional[float] = None) -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns a new :class:`~pyspark.sql.Column` for approximate distinct count\n",
      "        of column `col`.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "        rsd : float, optional\n",
      "            maximum relative standard deviation allowed (default = 0.05).\n",
      "            For rsd < 0.01, it is more efficient to use :func:`count_distinct`\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column of computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([1,2,2,3], \"INT\")\n",
      "        >>> df.agg(approx_count_distinct(\"value\").alias('distinct_values')).show()\n",
      "        +---------------+\n",
      "        |distinct_values|\n",
      "        +---------------+\n",
      "        |              3|\n",
      "        +---------------+\n",
      "    \n",
      "    approx_percentile(col: 'ColumnOrName', percentage: Union[pyspark.sql.column.Column, float, List[float], Tuple[float]], accuracy: Union[pyspark.sql.column.Column, float] = 10000) -> pyspark.sql.column.Column\n",
      "        Returns the approximate `percentile` of the numeric column `col` which is the smallest value\n",
      "        in the ordered `col` values (sorted from least to greatest) such that no more than `percentage`\n",
      "        of `col` values is less than the value or equal to that value.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column.\n",
      "        percentage : :class:`~pyspark.sql.Column`, float, list of floats or tuple of floats\n",
      "            percentage in decimal (must be between 0.0 and 1.0).\n",
      "            When percentage is an array, each value of the percentage array must be between 0.0 and 1.0.\n",
      "            In this case, returns the approximate percentile array of column col\n",
      "            at the given percentage array.\n",
      "        accuracy : :class:`~pyspark.sql.Column` or float\n",
      "            is a positive numeric literal which controls approximation accuracy\n",
      "            at the cost of memory. Higher value of accuracy yields better accuracy,\n",
      "            1.0/accuracy is the relative error of the approximation. (default: 10000).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            approximate `percentile` of the numeric column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> key = (sf.col(\"id\") % 3).alias(\"key\")\n",
      "        >>> value = (sf.randn(42) + key * 10).alias(\"value\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(key, value)\n",
      "        >>> df.select(\n",
      "        ...     sf.approx_percentile(\"value\", [0.25, 0.5, 0.75], 1000000)\n",
      "        ... ).printSchema()\n",
      "        root\n",
      "         |-- approx_percentile(value, array(0.25, 0.5, 0.75), 1000000): array (nullable = true)\n",
      "         |    |-- element: double (containsNull = false)\n",
      "        \n",
      "        >>> df.groupBy(\"key\").agg(\n",
      "        ...     sf.approx_percentile(\"value\", 0.5, sf.lit(1000000))\n",
      "        ... ).printSchema()\n",
      "        root\n",
      "         |-- key: long (nullable = true)\n",
      "         |-- approx_percentile(value, 0.5, 1000000): double (nullable = true)\n",
      "    \n",
      "    array(*cols: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName_')], Tuple[ForwardRef('ColumnOrName_'), ...]]) -> pyspark.sql.column.Column\n",
      "        Creates a new array column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s that have\n",
      "            the same data type.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a column of array type.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5)], (\"name\", \"age\"))\n",
      "        >>> df.select(array('age', 'age').alias(\"arr\")).collect()\n",
      "        [Row(arr=[2, 2]), Row(arr=[5, 5])]\n",
      "        >>> df.select(array([df.age, df.age]).alias(\"arr\")).collect()\n",
      "        [Row(arr=[2, 2]), Row(arr=[5, 5])]\n",
      "        >>> df.select(array('age', 'age').alias(\"col\")).printSchema()\n",
      "        root\n",
      "         |-- col: array (nullable = false)\n",
      "         |    |-- element: long (containsNull = true)\n",
      "    \n",
      "    array_agg(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns a list of objects with duplicates.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            list of objects with duplicates.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n",
      "        >>> df.agg(array_agg('c').alias('r')).collect()\n",
      "        [Row(r=[1, 1, 2])]\n",
      "    \n",
      "    array_append(col: 'ColumnOrName', value: Any) -> pyspark.sql.column.Column\n",
      "        Collection function: returns an array of the elements in col1 along\n",
      "        with the added element in col2 at the last of the array.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        value :\n",
      "            a literal value, or a :class:`~pyspark.sql.Column` expression.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of values from first array along with the element.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Supports Spark Connect.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=\"c\")])\n",
      "        >>> df.select(array_append(df.c1, df.c2)).collect()\n",
      "        [Row(array_append(c1, c2)=['b', 'a', 'c', 'c'])]\n",
      "        >>> df.select(array_append(df.c1, 'x')).collect()\n",
      "        [Row(array_append(c1, x)=['b', 'a', 'c', 'x'])]\n",
      "    \n",
      "    array_compact(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: removes null values from the array.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array by excluding the null values.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Supports Spark Connect.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, None, 2, 3],), ([4, 5, None, 4],)], ['data'])\n",
      "        >>> df.select(array_compact(df.data)).collect()\n",
      "        [Row(array_compact(data)=[1, 2, 3]), Row(array_compact(data)=[4, 5, 4])]\n",
      "    \n",
      "    array_contains(col: 'ColumnOrName', value: Any) -> pyspark.sql.column.Column\n",
      "        Collection function: returns null if the array is null, true if the array contains the\n",
      "        given value, and false otherwise.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        value :\n",
      "            value or column to check for in array\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a column of Boolean type.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], ['data'])\n",
      "        >>> df.select(array_contains(df.data, \"a\")).collect()\n",
      "        [Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]\n",
      "        >>> df.select(array_contains(df.data, lit(\"a\"))).collect()\n",
      "        [Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]\n",
      "    \n",
      "    array_distinct(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: removes duplicate values from the array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of unique values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 2, 3, 2],), ([4, 5, 5, 4],)], ['data'])\n",
      "        >>> df.select(array_distinct(df.data)).collect()\n",
      "        [Row(array_distinct(data)=[1, 2, 3]), Row(array_distinct(data)=[4, 5])]\n",
      "    \n",
      "    array_except(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: returns an array of the elements in col1 but not in col2,\n",
      "        without duplicates.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of values from first array that are not in the second.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
      "        >>> df.select(array_except(df.c1, df.c2)).collect()\n",
      "        [Row(array_except(c1, c2)=['b'])]\n",
      "    \n",
      "    array_insert(arr: 'ColumnOrName', pos: Union[ForwardRef('ColumnOrName'), int], value: Any) -> pyspark.sql.column.Column\n",
      "        Collection function: adds an item into a given array at a specified array index.\n",
      "        Array indices start at 1, or start from the end if index is negative.\n",
      "        Index above array size appends the array, or prepends the array if index is negative,\n",
      "        with 'null' elements.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        arr : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing an array\n",
      "        pos : :class:`~pyspark.sql.Column` or str or int\n",
      "            name of Numeric type column indicating position of insertion\n",
      "            (starting at index 1, negative position is a start from the back of the array)\n",
      "        value :\n",
      "            a literal value, or a :class:`~pyspark.sql.Column` expression.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of values, including the new specified value\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Supports Spark Connect.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(['a', 'b', 'c'], 2, 'd'), (['c', 'b', 'a'], -2, 'd')],\n",
      "        ...     ['data', 'pos', 'val']\n",
      "        ... )\n",
      "        >>> df.select(array_insert(df.data, df.pos.cast('integer'), df.val).alias('data')).collect()\n",
      "        [Row(data=['a', 'd', 'b', 'c']), Row(data=['c', 'b', 'd', 'a'])]\n",
      "        >>> df.select(array_insert(df.data, 5, 'hello').alias('data')).collect()\n",
      "        [Row(data=['a', 'b', 'c', None, 'hello']), Row(data=['c', 'b', 'a', None, 'hello'])]\n",
      "    \n",
      "    array_intersect(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: returns an array of the elements in the intersection of col1 and col2,\n",
      "        without duplicates.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of values in the intersection of two arrays.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
      "        >>> df.select(array_intersect(df.c1, df.c2)).collect()\n",
      "        [Row(array_intersect(c1, c2)=['a', 'c'])]\n",
      "    \n",
      "    array_join(col: 'ColumnOrName', delimiter: str, null_replacement: Optional[str] = None) -> pyspark.sql.column.Column\n",
      "        Concatenates the elements of `column` using the `delimiter`. Null values are replaced with\n",
      "        `null_replacement` if set, otherwise they are ignored.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        delimiter : str\n",
      "            delimiter used to concatenate elements\n",
      "        null_replacement : str, optional\n",
      "            if set then null values will be replaced by this value\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a column of string type. Concatenated values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([\"a\", None],)], ['data'])\n",
      "        >>> df.select(array_join(df.data, \",\").alias(\"joined\")).collect()\n",
      "        [Row(joined='a,b,c'), Row(joined='a')]\n",
      "        >>> df.select(array_join(df.data, \",\", \"NULL\").alias(\"joined\")).collect()\n",
      "        [Row(joined='a,b,c'), Row(joined='a,NULL')]\n",
      "    \n",
      "    array_max(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: returns the maximum value of the array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            maximum value of an array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], ['data'])\n",
      "        >>> df.select(array_max(df.data).alias('max')).collect()\n",
      "        [Row(max=3), Row(max=10)]\n",
      "    \n",
      "    array_min(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: returns the minimum value of the array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            minimum value of array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], ['data'])\n",
      "        >>> df.select(array_min(df.data).alias('min')).collect()\n",
      "        [Row(min=1), Row(min=-1)]\n",
      "    \n",
      "    array_position(col: 'ColumnOrName', value: Any) -> pyspark.sql.column.Column\n",
      "        Collection function: Locates the position of the first occurrence of the given value\n",
      "        in the given array. Returns null if either of the arguments are null.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not zero based, but 1 based index. Returns 0 if the given\n",
      "        value could not be found in the array.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        value : Any\n",
      "            value to look for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            position of the value in the given array if found and 0 otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"c\", \"b\", \"a\"],), ([],)], ['data'])\n",
      "        >>> df.select(array_position(df.data, \"a\")).collect()\n",
      "        [Row(array_position(data, a)=3), Row(array_position(data, a)=0)]\n",
      "    \n",
      "    array_prepend(col: 'ColumnOrName', value: Any) -> pyspark.sql.column.Column\n",
      "        Collection function: Returns an array containing element as\n",
      "        well as all elements from array. The new element is positioned\n",
      "        at the beginning of the array.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        value :\n",
      "            a literal value, or a :class:`~pyspark.sql.Column` expression.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array excluding given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 3, 4],), ([],)], ['data'])\n",
      "        >>> df.select(array_prepend(df.data, 1)).collect()\n",
      "        [Row(array_prepend(data, 1)=[1, 2, 3, 4]), Row(array_prepend(data, 1)=[1])]\n",
      "    \n",
      "    array_remove(col: 'ColumnOrName', element: Any) -> pyspark.sql.column.Column\n",
      "        Collection function: Remove all elements that equal to element from the given array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        element :\n",
      "            element to be removed from the array\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array excluding given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 2, 3, 1, 1],), ([],)], ['data'])\n",
      "        >>> df.select(array_remove(df.data, 1)).collect()\n",
      "        [Row(array_remove(data, 1)=[2, 3]), Row(array_remove(data, 1)=[])]\n",
      "    \n",
      "    array_repeat(col: 'ColumnOrName', count: Union[ForwardRef('ColumnOrName'), int]) -> pyspark.sql.column.Column\n",
      "        Collection function: creates an array containing a column repeated count times.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column name or column that contains the element to be repeated\n",
      "        count : :class:`~pyspark.sql.Column` or str or int\n",
      "            column name, column, or int containing the number of times to repeat the first argument\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of repeated elements.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('ab',)], ['data'])\n",
      "        >>> df.select(array_repeat(df.data, 3).alias('r')).collect()\n",
      "        [Row(r=['ab', 'ab', 'ab'])]\n",
      "    \n",
      "    array_size(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the total number of elements in the array. The function returns null for null input.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            total number of elements in the array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 1, 3],), (None,)], ['data'])\n",
      "        >>> df.select(array_size(df.data).alias('r')).collect()\n",
      "        [Row(r=3), Row(r=None)]\n",
      "    \n",
      "    array_sort(col: 'ColumnOrName', comparator: Optional[Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]] = None) -> pyspark.sql.column.Column\n",
      "        Collection function: sorts the input array in ascending order. The elements of the input array\n",
      "        must be orderable. Null elements will be placed at the end of the returned array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Can take a `comparator` function.\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        comparator : callable, optional\n",
      "            A binary ``(Column, Column) -> Column: ...``.\n",
      "            The comparator will take two\n",
      "            arguments representing two elements of the array. It returns a negative integer, 0, or a\n",
      "            positive integer as the first element is less than, equal to, or greater than the second\n",
      "            element. If the comparator function returns null, the function will fail and raise an error.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            sorted array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], ['data'])\n",
      "        >>> df.select(array_sort(df.data).alias('r')).collect()\n",
      "        [Row(r=[1, 2, 3, None]), Row(r=[1]), Row(r=[])]\n",
      "        >>> df = spark.createDataFrame([([\"foo\", \"foobar\", None, \"bar\"],),([\"foo\"],),([],)], ['data'])\n",
      "        >>> df.select(array_sort(\n",
      "        ...     \"data\",\n",
      "        ...     lambda x, y: when(x.isNull() | y.isNull(), lit(0)).otherwise(length(y) - length(x))\n",
      "        ... ).alias(\"r\")).collect()\n",
      "        [Row(r=['foobar', 'foo', None, 'bar']), Row(r=['foo']), Row(r=[])]\n",
      "    \n",
      "    array_union(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: returns an array of the elements in the union of col1 and col2,\n",
      "        without duplicates.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of values in union of two arrays.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
      "        >>> df.select(array_union(df.c1, df.c2)).collect()\n",
      "        [Row(array_union(c1, c2)=['b', 'a', 'c', 'd', 'f'])]\n",
      "    \n",
      "    arrays_overlap(a1: 'ColumnOrName', a2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: returns true if the arrays contain any common non-null element; if not,\n",
      "        returns null if both the arrays are non-empty and any of them contains a null element; returns\n",
      "        false otherwise.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a column of Boolean type.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\"], [\"b\", \"c\"]), ([\"a\"], [\"b\", \"c\"])], ['x', 'y'])\n",
      "        >>> df.select(arrays_overlap(df.x, df.y).alias(\"overlap\")).collect()\n",
      "        [Row(overlap=True), Row(overlap=False)]\n",
      "    \n",
      "    arrays_zip(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: Returns a merged array of structs in which the N-th struct contains all\n",
      "        N-th values of input arrays. If one of the arrays is shorter than others then\n",
      "        resulting struct type value will be a `null` for missing elements.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            columns of arrays to be merged.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            merged array of entries.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import arrays_zip\n",
      "        >>> df = spark.createDataFrame([([1, 2, 3], [2, 4, 6], [3, 6])], ['vals1', 'vals2', 'vals3'])\n",
      "        >>> df = df.select(arrays_zip(df.vals1, df.vals2, df.vals3).alias('zipped'))\n",
      "        >>> df.show(truncate=False)\n",
      "        +------------------------------------+\n",
      "        |zipped                              |\n",
      "        +------------------------------------+\n",
      "        |[{1, 2, 3}, {2, 4, 6}, {3, 6, NULL}]|\n",
      "        +------------------------------------+\n",
      "        >>> df.printSchema()\n",
      "        root\n",
      "         |-- zipped: array (nullable = true)\n",
      "         |    |-- element: struct (containsNull = false)\n",
      "         |    |    |-- vals1: long (nullable = true)\n",
      "         |    |    |-- vals2: long (nullable = true)\n",
      "         |    |    |-- vals3: long (nullable = true)\n",
      "    \n",
      "    asc(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a sort expression based on the ascending order of the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to sort by in the ascending order.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column specifying the order.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Sort by the column 'id' in the descending order.\n",
      "        \n",
      "        >>> df = spark.range(5)\n",
      "        >>> df = df.sort(desc(\"id\"))\n",
      "        >>> df.show()\n",
      "        +---+\n",
      "        | id|\n",
      "        +---+\n",
      "        |  4|\n",
      "        |  3|\n",
      "        |  2|\n",
      "        |  1|\n",
      "        |  0|\n",
      "        +---+\n",
      "        \n",
      "        Sort by the column 'id' in the ascending order.\n",
      "        \n",
      "        >>> df.orderBy(asc(\"id\")).show()\n",
      "        +---+\n",
      "        | id|\n",
      "        +---+\n",
      "        |  0|\n",
      "        |  1|\n",
      "        |  2|\n",
      "        |  3|\n",
      "        |  4|\n",
      "        +---+\n",
      "    \n",
      "    asc_nulls_first(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a sort expression based on the ascending order of the given\n",
      "        column name, and null values return before non-null values.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to sort by in the ascending order.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column specifying the order.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df1 = spark.createDataFrame([(1, \"Bob\"),\n",
      "        ...                              (0, None),\n",
      "        ...                              (2, \"Alice\")], [\"age\", \"name\"])\n",
      "        >>> df1.sort(asc_nulls_first(df1.name)).show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  0| NULL|\n",
      "        |  2|Alice|\n",
      "        |  1|  Bob|\n",
      "        +---+-----+\n",
      "    \n",
      "    asc_nulls_last(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a sort expression based on the ascending order of the given\n",
      "        column name, and null values appear after non-null values.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to sort by in the ascending order.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column specifying the order.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df1 = spark.createDataFrame([(0, None),\n",
      "        ...                              (1, \"Bob\"),\n",
      "        ...                              (2, \"Alice\")], [\"age\", \"name\"])\n",
      "        >>> df1.sort(asc_nulls_last(df1.name)).show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  2|Alice|\n",
      "        |  1|  Bob|\n",
      "        |  0| NULL|\n",
      "        +---+-----+\n",
      "    \n",
      "    ascii(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the numeric value of the first character of the string column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            numeric value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\"Spark\", \"PySpark\", \"Pandas API\"], \"STRING\")\n",
      "        >>> df.select(ascii(\"value\")).show()\n",
      "        +------------+\n",
      "        |ascii(value)|\n",
      "        +------------+\n",
      "        |          83|\n",
      "        |          80|\n",
      "        |          80|\n",
      "        +------------+\n",
      "    \n",
      "    asin(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes inverse sine of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            inverse sine of `col`, as if computed by `java.lang.Math.asin()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(0,), (2,)])\n",
      "        >>> df.select(asin(df.schema.fieldNames()[0])).show()\n",
      "        +--------+\n",
      "        |ASIN(_1)|\n",
      "        +--------+\n",
      "        |     0.0|\n",
      "        |     NaN|\n",
      "        +--------+\n",
      "    \n",
      "    asinh(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes inverse hyperbolic sine of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(asinh(col(\"id\"))).show()\n",
      "        +---------+\n",
      "        |ASINH(id)|\n",
      "        +---------+\n",
      "        |      0.0|\n",
      "        +---------+\n",
      "    \n",
      "    assert_true(col: 'ColumnOrName', errMsg: Union[pyspark.sql.column.Column, str, NoneType] = None) -> pyspark.sql.column.Column\n",
      "        Returns `null` if the input column is `true`; throws an exception\n",
      "        with the provided error message otherwise.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column name or column that represents the input column to test\n",
      "        errMsg : :class:`~pyspark.sql.Column` or str, optional\n",
      "            A Python string literal or column containing the error message\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            `null` if the input column is `true` otherwise throws an error with specified message.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(0,1)], ['a', 'b'])\n",
      "        >>> df.select(assert_true(df.a < df.b).alias('r')).collect()\n",
      "        [Row(r=None)]\n",
      "        >>> df.select(assert_true(df.a < df.b, df.a).alias('r')).collect()\n",
      "        [Row(r=None)]\n",
      "        >>> df.select(assert_true(df.a < df.b, 'error').alias('r')).collect()\n",
      "        [Row(r=None)]\n",
      "        >>> df.select(assert_true(df.a > df.b, 'My error msg').alias('r')).collect() # doctest: +SKIP\n",
      "        ...\n",
      "        java.lang.RuntimeException: My error msg\n",
      "        ...\n",
      "    \n",
      "    atan(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Compute inverse tangent of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            inverse tangent of `col`, as if computed by `java.lang.Math.atan()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(atan(df.id)).show()\n",
      "        +--------+\n",
      "        |ATAN(id)|\n",
      "        +--------+\n",
      "        |     0.0|\n",
      "        +--------+\n",
      "    \n",
      "    atan2(col1: Union[ForwardRef('ColumnOrName'), float], col2: Union[ForwardRef('ColumnOrName'), float]) -> pyspark.sql.column.Column\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : str, :class:`~pyspark.sql.Column` or float\n",
      "            coordinate on y-axis\n",
      "        col2 : str, :class:`~pyspark.sql.Column` or float\n",
      "            coordinate on x-axis\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the `theta` component of the point\n",
      "            (`r`, `theta`)\n",
      "            in polar coordinates that corresponds to the point\n",
      "            (`x`, `y`) in Cartesian coordinates,\n",
      "            as if computed by `java.lang.Math.atan2()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(atan2(lit(1), lit(2))).first()\n",
      "        Row(ATAN2(1, 2)=0.46364...)\n",
      "    \n",
      "    atanh(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes inverse hyperbolic tangent of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(0,), (2,)], schema=[\"numbers\"])\n",
      "        >>> df.select(atanh(df[\"numbers\"])).show()\n",
      "        +--------------+\n",
      "        |ATANH(numbers)|\n",
      "        +--------------+\n",
      "        |           0.0|\n",
      "        |           NaN|\n",
      "        +--------------+\n",
      "    \n",
      "    avg(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the average of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(10)\n",
      "        >>> df.select(avg(col(\"id\"))).show()\n",
      "        +-------+\n",
      "        |avg(id)|\n",
      "        +-------+\n",
      "        |    4.5|\n",
      "        +-------+\n",
      "    \n",
      "    base64(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the BASE64 encoding of a binary column and returns it as a string column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            BASE64 encoding of string value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\"Spark\", \"PySpark\", \"Pandas API\"], \"STRING\")\n",
      "        >>> df.select(base64(\"value\")).show()\n",
      "        +----------------+\n",
      "        |   base64(value)|\n",
      "        +----------------+\n",
      "        |        U3Bhcms=|\n",
      "        |    UHlTcGFyaw==|\n",
      "        |UGFuZGFzIEFQSQ==|\n",
      "        +----------------+\n",
      "    \n",
      "    bin(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the string representation of the binary value of the given column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            binary representation of given value as string.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([2,5], \"INT\")\n",
      "        >>> df.select(bin(df.value).alias('c')).collect()\n",
      "        [Row(c='10'), Row(c='101')]\n",
      "    \n",
      "    bit_and(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the bitwise AND of all non-null input values, or null if none.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the bitwise AND of all non-null input values, or null if none.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n",
      "        >>> df.select(bit_and(\"c\")).first()\n",
      "        Row(bit_and(c)=0)\n",
      "    \n",
      "    bit_count(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the number of bits that are set in the argument expr as an unsigned 64-bit integer,\n",
      "        or NULL if the argument is NULL.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the number of bits that are set in the argument expr as an unsigned 64-bit integer,\n",
      "            or NULL if the argument is NULL.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n",
      "        >>> df.select(bit_count(\"c\")).show()\n",
      "        +------------+\n",
      "        |bit_count(c)|\n",
      "        +------------+\n",
      "        |           1|\n",
      "        |           1|\n",
      "        |           1|\n",
      "        +------------+\n",
      "    \n",
      "    bit_get(col: 'ColumnOrName', pos: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the value of the bit (0 or 1) at the specified position.\n",
      "        The positions are numbered from right to left, starting at zero.\n",
      "        The position argument cannot be negative.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        pos : :class:`~pyspark.sql.Column` or str\n",
      "            The positions are numbered from right to left, starting at zero.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the value of the bit (0 or 1) at the specified position.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n",
      "        >>> df.select(bit_get(\"c\", lit(1))).show()\n",
      "        +-------------+\n",
      "        |bit_get(c, 1)|\n",
      "        +-------------+\n",
      "        |            0|\n",
      "        |            0|\n",
      "        |            1|\n",
      "        +-------------+\n",
      "    \n",
      "    bit_length(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Calculates the bit length for the specified string column.\n",
      "        \n",
      "        .. versionadded:: 3.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            Source column or strings\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            Bit length of the col\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import bit_length\n",
      "        >>> spark.createDataFrame([('cat',), ( '🐈',)], ['cat']) \\\n",
      "        ...      .select(bit_length('cat')).collect()\n",
      "            [Row(bit_length(cat)=24), Row(bit_length(cat)=32)]\n",
      "    \n",
      "    bit_or(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the bitwise OR of all non-null input values, or null if none.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the bitwise OR of all non-null input values, or null if none.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n",
      "        >>> df.select(bit_or(\"c\")).first()\n",
      "        Row(bit_or(c)=3)\n",
      "    \n",
      "    bit_xor(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the bitwise XOR of all non-null input values, or null if none.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the bitwise XOR of all non-null input values, or null if none.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n",
      "        >>> df.select(bit_xor(\"c\")).first()\n",
      "        Row(bit_xor(c)=2)\n",
      "    \n",
      "    bitmap_bit_position(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the bit position for the given input column.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            The input column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(123,)], [\"a\"])\n",
      "        >>> df.select(bitmap_bit_position(df.a).alias(\"r\")).collect()\n",
      "        [Row(r=122)]\n",
      "    \n",
      "    bitmap_bucket_number(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the bucket number for the given input column.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            The input column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(123,)], [\"a\"])\n",
      "        >>> df.select(bitmap_bucket_number(df.a).alias(\"r\")).collect()\n",
      "        [Row(r=1)]\n",
      "    \n",
      "    bitmap_construct_agg(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a bitmap with the positions of the bits set from all the values from the input column.\n",
      "        The input column will most likely be bitmap_bit_position().\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            The input column will most likely be bitmap_bit_position().\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1,),(2,),(3,)], [\"a\"])\n",
      "        >>> df.select(substring(hex(\n",
      "        ...     bitmap_construct_agg(bitmap_bit_position(df.a))\n",
      "        ... ), 0, 6).alias(\"r\")).collect()\n",
      "        [Row(r='070000')]\n",
      "    \n",
      "    bitmap_count(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the number of set bits in the input bitmap.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            The input bitmap.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"FFFF\",)], [\"a\"])\n",
      "        >>> df.select(bitmap_count(to_binary(df.a, lit(\"hex\"))).alias('r')).collect()\n",
      "        [Row(r=16)]\n",
      "    \n",
      "    bitmap_or_agg(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a bitmap that is the bitwise OR of all of the bitmaps from the input column.\n",
      "        The input column should be bitmaps created from bitmap_construct_agg().\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            The input column should be bitmaps created from bitmap_construct_agg().\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"10\",),(\"20\",),(\"40\",)], [\"a\"])\n",
      "        >>> df.select(substring(hex(\n",
      "        ...     bitmap_or_agg(to_binary(df.a, lit(\"hex\")))\n",
      "        ... ), 0, 6).alias(\"r\")).collect()\n",
      "        [Row(r='700000')]\n",
      "    \n",
      "    bitwiseNOT(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes bitwise not.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        .. deprecated:: 3.2.0\n",
      "            Use :func:`bitwise_not` instead.\n",
      "    \n",
      "    bitwise_not(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes bitwise not.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(bitwise_not(lit(0))).show()\n",
      "        +---+\n",
      "        | ~0|\n",
      "        +---+\n",
      "        | -1|\n",
      "        +---+\n",
      "        >>> df.select(bitwise_not(lit(1))).show()\n",
      "        +---+\n",
      "        | ~1|\n",
      "        +---+\n",
      "        | -2|\n",
      "        +---+\n",
      "    \n",
      "    bool_and(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns true if all values of `col` are true.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to check if all values are true.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            true if all values of `col` are true, false otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[True], [True], [True]], [\"flag\"])\n",
      "        >>> df.select(bool_and(\"flag\")).show()\n",
      "        +--------------+\n",
      "        |bool_and(flag)|\n",
      "        +--------------+\n",
      "        |          true|\n",
      "        +--------------+\n",
      "        >>> df = spark.createDataFrame([[True], [False], [True]], [\"flag\"])\n",
      "        >>> df.select(bool_and(\"flag\")).show()\n",
      "        +--------------+\n",
      "        |bool_and(flag)|\n",
      "        +--------------+\n",
      "        |         false|\n",
      "        +--------------+\n",
      "        >>> df = spark.createDataFrame([[False], [False], [False]], [\"flag\"])\n",
      "        >>> df.select(bool_and(\"flag\")).show()\n",
      "        +--------------+\n",
      "        |bool_and(flag)|\n",
      "        +--------------+\n",
      "        |         false|\n",
      "        +--------------+\n",
      "    \n",
      "    bool_or(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns true if at least one value of `col` is true.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to check if at least one value is true.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            true if at least one value of `col` is true, false otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[True], [True], [True]], [\"flag\"])\n",
      "        >>> df.select(bool_or(\"flag\")).show()\n",
      "        +-------------+\n",
      "        |bool_or(flag)|\n",
      "        +-------------+\n",
      "        |         true|\n",
      "        +-------------+\n",
      "        >>> df = spark.createDataFrame([[True], [False], [True]], [\"flag\"])\n",
      "        >>> df.select(bool_or(\"flag\")).show()\n",
      "        +-------------+\n",
      "        |bool_or(flag)|\n",
      "        +-------------+\n",
      "        |         true|\n",
      "        +-------------+\n",
      "        >>> df = spark.createDataFrame([[False], [False], [False]], [\"flag\"])\n",
      "        >>> df.select(bool_or(\"flag\")).show()\n",
      "        +-------------+\n",
      "        |bool_or(flag)|\n",
      "        +-------------+\n",
      "        |        false|\n",
      "        +-------------+\n",
      "    \n",
      "    broadcast(df: pyspark.sql.dataframe.DataFrame) -> pyspark.sql.dataframe.DataFrame\n",
      "        Marks a DataFrame as small enough for use in broadcast joins.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.DataFrame`\n",
      "            DataFrame marked as ready for broadcast join.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import types\n",
      "        >>> df = spark.createDataFrame([1, 2, 3, 3, 4], types.IntegerType())\n",
      "        >>> df_small = spark.range(3)\n",
      "        >>> df_b = broadcast(df_small)\n",
      "        >>> df.join(df_b, df.value == df_small.id).show()\n",
      "        +-----+---+\n",
      "        |value| id|\n",
      "        +-----+---+\n",
      "        |    1|  1|\n",
      "        |    2|  2|\n",
      "        +-----+---+\n",
      "    \n",
      "    bround(col: 'ColumnOrName', scale: int = 0) -> pyspark.sql.column.Column\n",
      "        Round the given value to `scale` decimal places using HALF_EVEN rounding mode if `scale` >= 0\n",
      "        or at integral part when `scale` < 0.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column to round.\n",
      "        scale : int optional default 0\n",
      "            scale value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            rounded values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([(2.5,)], ['a']).select(bround('a', 0).alias('r')).collect()\n",
      "        [Row(r=2.0)]\n",
      "    \n",
      "    btrim(str: 'ColumnOrName', trim: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Remove the leading and trailing `trim` characters from `str`.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        trim : :class:`~pyspark.sql.Column` or str\n",
      "            The trim string characters to trim, the default value is a single space\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"SSparkSQLS\", \"SL\", )], ['a', 'b'])\n",
      "        >>> df.select(btrim(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r='parkSQ')]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"    SparkSQL   \",)], ['a'])\n",
      "        >>> df.select(btrim(df.a).alias('r')).collect()\n",
      "        [Row(r='SparkSQL')]\n",
      "    \n",
      "    bucket(numBuckets: Union[pyspark.sql.column.Column, int], col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Partition transform function: A transform for any type that partitions\n",
      "        by a hash of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").partitionedBy(  # doctest: +SKIP\n",
      "        ...     bucket(42, \"ts\")\n",
      "        ... ).createOrReplace()\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date or timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            data partitioned by given columns.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function can be used only in combination with\n",
      "        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n",
      "        method of the `DataFrameWriterV2`.\n",
      "    \n",
      "    call_function(funcName: str, *cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Call a SQL function.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        funcName : str\n",
      "            function name that follows the SQL identifier syntax (can be quoted, can be qualified)\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s to be used in the function\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            result of executed function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import call_udf, col\n",
      "        >>> from pyspark.sql.types import IntegerType, StringType\n",
      "        >>> df = spark.createDataFrame([(1, \"a\"),(2, \"b\"), (3, \"c\")],[\"id\", \"name\"])\n",
      "        >>> _ = spark.udf.register(\"intX2\", lambda i: i * 2, IntegerType())\n",
      "        >>> df.select(call_function(\"intX2\", \"id\")).show()\n",
      "        +---------+\n",
      "        |intX2(id)|\n",
      "        +---------+\n",
      "        |        2|\n",
      "        |        4|\n",
      "        |        6|\n",
      "        +---------+\n",
      "        >>> _ = spark.udf.register(\"strX2\", lambda s: s * 2, StringType())\n",
      "        >>> df.select(call_function(\"strX2\", col(\"name\"))).show()\n",
      "        +-----------+\n",
      "        |strX2(name)|\n",
      "        +-----------+\n",
      "        |         aa|\n",
      "        |         bb|\n",
      "        |         cc|\n",
      "        +-----------+\n",
      "        >>> df.select(call_function(\"avg\", col(\"id\"))).show()\n",
      "        +-------+\n",
      "        |avg(id)|\n",
      "        +-------+\n",
      "        |    2.0|\n",
      "        +-------+\n",
      "        >>> _ = spark.sql(\"CREATE FUNCTION custom_avg AS 'test.org.apache.spark.sql.MyDoubleAvg'\")\n",
      "        ... # doctest: +SKIP\n",
      "        >>> df.select(call_function(\"custom_avg\", col(\"id\"))).show()\n",
      "        ... # doctest: +SKIP\n",
      "        +------------------------------------+\n",
      "        |spark_catalog.default.custom_avg(id)|\n",
      "        +------------------------------------+\n",
      "        |                               102.0|\n",
      "        +------------------------------------+\n",
      "        >>> df.select(call_function(\"spark_catalog.default.custom_avg\", col(\"id\"))).show()\n",
      "        ... # doctest: +SKIP\n",
      "        +------------------------------------+\n",
      "        |spark_catalog.default.custom_avg(id)|\n",
      "        +------------------------------------+\n",
      "        |                               102.0|\n",
      "        +------------------------------------+\n",
      "    \n",
      "    call_udf(udfName: str, *cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Call an user-defined function.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        udfName : str\n",
      "            name of the user defined function (UDF)\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s to be used in the UDF\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            result of executed udf.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import call_udf, col\n",
      "        >>> from pyspark.sql.types import IntegerType, StringType\n",
      "        >>> df = spark.createDataFrame([(1, \"a\"),(2, \"b\"), (3, \"c\")],[\"id\", \"name\"])\n",
      "        >>> _ = spark.udf.register(\"intX2\", lambda i: i * 2, IntegerType())\n",
      "        >>> df.select(call_udf(\"intX2\", \"id\")).show()\n",
      "        +---------+\n",
      "        |intX2(id)|\n",
      "        +---------+\n",
      "        |        2|\n",
      "        |        4|\n",
      "        |        6|\n",
      "        +---------+\n",
      "        >>> _ = spark.udf.register(\"strX2\", lambda s: s * 2, StringType())\n",
      "        >>> df.select(call_udf(\"strX2\", col(\"name\"))).show()\n",
      "        +-----------+\n",
      "        |strX2(name)|\n",
      "        +-----------+\n",
      "        |         aa|\n",
      "        |         bb|\n",
      "        |         cc|\n",
      "        +-----------+\n",
      "    \n",
      "    cardinality(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: returns the length of the array or map stored in the column.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            length of the array/map.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [([1, 2, 3],),([1],),([],)], ['data']\n",
      "        ... ).select(sf.cardinality(\"data\")).show()\n",
      "        +-----------------+\n",
      "        |cardinality(data)|\n",
      "        +-----------------+\n",
      "        |                3|\n",
      "        |                1|\n",
      "        |                0|\n",
      "        +-----------------+\n",
      "    \n",
      "    cbrt(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the cube-root of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(cbrt(lit(27))).show()\n",
      "        +--------+\n",
      "        |CBRT(27)|\n",
      "        +--------+\n",
      "        |     3.0|\n",
      "        +--------+\n",
      "    \n",
      "    ceil(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the ceiling of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(ceil(lit(-0.1))).show()\n",
      "        +----------+\n",
      "        |CEIL(-0.1)|\n",
      "        +----------+\n",
      "        |         0|\n",
      "        +----------+\n",
      "    \n",
      "    ceiling(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the ceiling of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(1).select(sf.ceil(sf.lit(-0.1))).show()\n",
      "        +----------+\n",
      "        |CEIL(-0.1)|\n",
      "        +----------+\n",
      "        |         0|\n",
      "        +----------+\n",
      "    \n",
      "    char(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the ASCII character having the binary equivalent to `col`. If col is larger than 256 the\n",
      "        result is equivalent to char(col % 256)\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(1).select(sf.char(sf.lit(65))).show()\n",
      "        +--------+\n",
      "        |char(65)|\n",
      "        +--------+\n",
      "        |       A|\n",
      "        +--------+\n",
      "    \n",
      "    char_length(str: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the character length of string data or number of bytes of binary data.\n",
      "        The length of string data includes the trailing spaces.\n",
      "        The length of binary data includes binary zeros.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(1).select(sf.char_length(sf.lit(\"SparkSQL\"))).show()\n",
      "        +---------------------+\n",
      "        |char_length(SparkSQL)|\n",
      "        +---------------------+\n",
      "        |                    8|\n",
      "        +---------------------+\n",
      "    \n",
      "    character_length(str: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the character length of string data or number of bytes of binary data.\n",
      "        The length of string data includes the trailing spaces.\n",
      "        The length of binary data includes binary zeros.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(1).select(sf.character_length(sf.lit(\"SparkSQL\"))).show()\n",
      "        +--------------------------+\n",
      "        |character_length(SparkSQL)|\n",
      "        +--------------------------+\n",
      "        |                         8|\n",
      "        +--------------------------+\n",
      "    \n",
      "    coalesce(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the first column that is not null.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            list of columns to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value of the first column that is not null.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> cDf = spark.createDataFrame([(None, None), (1, None), (None, 2)], (\"a\", \"b\"))\n",
      "        >>> cDf.show()\n",
      "        +----+----+\n",
      "        |   a|   b|\n",
      "        +----+----+\n",
      "        |NULL|NULL|\n",
      "        |   1|NULL|\n",
      "        |NULL|   2|\n",
      "        +----+----+\n",
      "        \n",
      "        >>> cDf.select(coalesce(cDf[\"a\"], cDf[\"b\"])).show()\n",
      "        +--------------+\n",
      "        |coalesce(a, b)|\n",
      "        +--------------+\n",
      "        |          NULL|\n",
      "        |             1|\n",
      "        |             2|\n",
      "        +--------------+\n",
      "        \n",
      "        >>> cDf.select('*', coalesce(cDf[\"a\"], lit(0.0))).show()\n",
      "        +----+----+----------------+\n",
      "        |   a|   b|coalesce(a, 0.0)|\n",
      "        +----+----+----------------+\n",
      "        |NULL|NULL|             0.0|\n",
      "        |   1|NULL|             1.0|\n",
      "        |NULL|   2|             0.0|\n",
      "        +----+----+----------------+\n",
      "    \n",
      "    col(col: str) -> pyspark.sql.column.Column\n",
      "        Returns a :class:`~pyspark.sql.Column` based on the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : str\n",
      "            the name for the column\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the corresponding column instance.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> col('x')\n",
      "        Column<'x'>\n",
      "        >>> column('x')\n",
      "        Column<'x'>\n",
      "    \n",
      "    collect_list(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns a list of objects with duplicates.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic because the order of collected results depends\n",
      "        on the order of the rows which may be non-deterministic after a shuffle.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            list of objects with duplicates.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df2 = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n",
      "        >>> df2.agg(collect_list('age')).collect()\n",
      "        [Row(collect_list(age)=[2, 5, 5])]\n",
      "    \n",
      "    collect_set(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns a set of objects with duplicate elements eliminated.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic because the order of collected results depends\n",
      "        on the order of the rows which may be non-deterministic after a shuffle.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            list of objects with no duplicates.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df2 = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n",
      "        >>> df2.agg(array_sort(collect_set('age')).alias('c')).collect()\n",
      "        [Row(c=[2, 5])]\n",
      "    \n",
      "    column = col(col: str) -> pyspark.sql.column.Column\n",
      "        Returns a :class:`~pyspark.sql.Column` based on the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : str\n",
      "            the name for the column\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the corresponding column instance.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> col('x')\n",
      "        Column<'x'>\n",
      "        >>> column('x')\n",
      "        Column<'x'>\n",
      "    \n",
      "    concat(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Concatenates multiple input columns together into a single column.\n",
      "        The function works with strings, numeric, binary and compatible array columns.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            target column or columns to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            concatenated values. Type of the `Column` depends on input columns' type.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        :meth:`pyspark.sql.functions.array_join` : to concatenate string columns with delimiter\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
      "        >>> df = df.select(concat(df.s, df.d).alias('s'))\n",
      "        >>> df.collect()\n",
      "        [Row(s='abcd123')]\n",
      "        >>> df\n",
      "        DataFrame[s: string]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], ['a', 'b', 'c'])\n",
      "        >>> df = df.select(concat(df.a, df.b, df.c).alias(\"arr\"))\n",
      "        >>> df.collect()\n",
      "        [Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]\n",
      "        >>> df\n",
      "        DataFrame[arr: array<bigint>]\n",
      "    \n",
      "    concat_ws(sep: str, *cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Concatenates multiple input string columns together into a single string column,\n",
      "        using the given separator.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        sep : str\n",
      "            words separator.\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            list of columns to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            string of concatenated words.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
      "        >>> df.select(concat_ws('-', df.s, df.d).alias('s')).collect()\n",
      "        [Row(s='abcd-123')]\n",
      "    \n",
      "    contains(left: 'ColumnOrName', right: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a boolean. The value is True if right is found inside left.\n",
      "        Returns NULL if either input expression is NULL. Otherwise, returns False.\n",
      "        Both left or right must be of STRING or BINARY type.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        left : :class:`~pyspark.sql.Column` or str\n",
      "            The input column or strings to check, may be NULL.\n",
      "        right : :class:`~pyspark.sql.Column` or str\n",
      "            The input column or strings to find, may be NULL.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Spark SQL\", \"Spark\")], ['a', 'b'])\n",
      "        >>> df.select(contains(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r=True)]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"414243\", \"4243\",)], [\"c\", \"d\"])\n",
      "        >>> df = df.select(to_binary(\"c\").alias(\"c\"), to_binary(\"d\").alias(\"d\"))\n",
      "        >>> df.printSchema()\n",
      "        root\n",
      "         |-- c: binary (nullable = true)\n",
      "         |-- d: binary (nullable = true)\n",
      "        >>> df.select(contains(\"c\", \"d\"), contains(\"d\", \"c\")).show()\n",
      "        +--------------+--------------+\n",
      "        |contains(c, d)|contains(d, c)|\n",
      "        +--------------+--------------+\n",
      "        |          true|         false|\n",
      "        +--------------+--------------+\n",
      "    \n",
      "    conv(col: 'ColumnOrName', fromBase: int, toBase: int) -> pyspark.sql.column.Column\n",
      "        Convert a number in a string column from one base to another.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            a column to convert base for.\n",
      "        fromBase: int\n",
      "            from base number.\n",
      "        toBase: int\n",
      "            to base number.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            logariphm of given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"010101\",)], ['n'])\n",
      "        >>> df.select(conv(df.n, 2, 16).alias('hex')).collect()\n",
      "        [Row(hex='15')]\n",
      "    \n",
      "    convert_timezone(sourceTz: Optional[pyspark.sql.column.Column], targetTz: pyspark.sql.column.Column, sourceTs: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Converts the timestamp without time zone `sourceTs`\n",
      "        from the `sourceTz` time zone to `targetTz`.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        sourceTz : :class:`~pyspark.sql.Column`\n",
      "            the time zone for the input timestamp. If it is missed,\n",
      "            the current session time zone is used as the source time zone.\n",
      "        targetTz : :class:`~pyspark.sql.Column`\n",
      "            the time zone to which the input timestamp should be converted.\n",
      "        sourceTs : :class:`~pyspark.sql.Column`\n",
      "            a timestamp without time zone.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            timestamp for converted time zone.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(convert_timezone(   # doctest: +SKIP\n",
      "        ...     None, lit('Asia/Hong_Kong'), 'dt').alias('ts')\n",
      "        ... ).show()\n",
      "        +-------------------+\n",
      "        |                 ts|\n",
      "        +-------------------+\n",
      "        |2015-04-08 00:00:00|\n",
      "        +-------------------+\n",
      "        >>> df.select(convert_timezone(\n",
      "        ...     lit('America/Los_Angeles'), lit('Asia/Hong_Kong'), 'dt').alias('ts')\n",
      "        ... ).show()\n",
      "        +-------------------+\n",
      "        |                 ts|\n",
      "        +-------------------+\n",
      "        |2015-04-08 15:00:00|\n",
      "        +-------------------+\n",
      "    \n",
      "    corr(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new :class:`~pyspark.sql.Column` for the Pearson Correlation Coefficient for\n",
      "        ``col1`` and ``col2``.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            first column to calculate correlation.\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            second column to calculate correlation.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            Pearson Correlation Coefficient of these two column values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> a = range(20)\n",
      "        >>> b = [2 * x for x in range(20)]\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(corr(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=1.0)]\n",
      "    \n",
      "    cos(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes cosine of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            angle in radians\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            cosine of the angle, as if computed by `java.lang.Math.cos()`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import math\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(cos(lit(math.pi))).first()\n",
      "        Row(COS(3.14159...)=-1.0)\n",
      "    \n",
      "    cosh(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes hyperbolic cosine of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            hyperbolic angle\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hyperbolic cosine of the angle, as if computed by `java.lang.Math.cosh()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(cosh(lit(1))).first()\n",
      "        Row(COSH(1)=1.54308...)\n",
      "    \n",
      "    cot(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes cotangent of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            angle in radians.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            cotangent of the angle.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import math\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(cot(lit(math.radians(45)))).first()\n",
      "        Row(COT(0.78539...)=1.00000...)\n",
      "    \n",
      "    count(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the number of items in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Count by all columns (start), and by a column that does not count ``None``.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(None,), (\"a\",), (\"b\",), (\"c\",)], schema=[\"alphabets\"])\n",
      "        >>> df.select(count(expr(\"*\")), count(df.alphabets)).show()\n",
      "        +--------+----------------+\n",
      "        |count(1)|count(alphabets)|\n",
      "        +--------+----------------+\n",
      "        |       4|               3|\n",
      "        +--------+----------------+\n",
      "    \n",
      "    countDistinct(col: 'ColumnOrName', *cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new :class:`~pyspark.sql.Column` for distinct count of ``col`` or ``cols``.\n",
      "        \n",
      "        An alias of :func:`count_distinct`, and it is encouraged to use :func:`count_distinct`\n",
      "        directly.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "    \n",
      "    count_distinct(col: 'ColumnOrName', *cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new :class:`Column` for distinct count of ``col`` or ``cols``.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            first column to compute on.\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            other columns to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            distinct values of these two column values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import types\n",
      "        >>> df1 = spark.createDataFrame([1, 1, 3], types.IntegerType())\n",
      "        >>> df2 = spark.createDataFrame([1, 2], types.IntegerType())\n",
      "        >>> df1.join(df2).show()\n",
      "        +-----+-----+\n",
      "        |value|value|\n",
      "        +-----+-----+\n",
      "        |    1|    1|\n",
      "        |    1|    2|\n",
      "        |    1|    1|\n",
      "        |    1|    2|\n",
      "        |    3|    1|\n",
      "        |    3|    2|\n",
      "        +-----+-----+\n",
      "        >>> df1.join(df2).select(count_distinct(df1.value, df2.value)).show()\n",
      "        +----------------------------+\n",
      "        |count(DISTINCT value, value)|\n",
      "        +----------------------------+\n",
      "        |                           4|\n",
      "        +----------------------------+\n",
      "    \n",
      "    count_if(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the number of `TRUE` values for the `col`.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the number of `TRUE` values for the `col`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"a\", 1),\n",
      "        ...                             (\"a\", 2),\n",
      "        ...                             (\"a\", 3),\n",
      "        ...                             (\"b\", 8),\n",
      "        ...                             (\"b\", 2)], [\"c1\", \"c2\"])\n",
      "        >>> df.select(count_if(col('c2') % 2 == 0)).show()\n",
      "        +------------------------+\n",
      "        |count_if(((c2 % 2) = 0))|\n",
      "        +------------------------+\n",
      "        |                       3|\n",
      "        +------------------------+\n",
      "    \n",
      "    count_min_sketch(col: 'ColumnOrName', eps: 'ColumnOrName', confidence: 'ColumnOrName', seed: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a count-min sketch of a column with the given esp, confidence and seed.\n",
      "        The result is an array of bytes, which can be deserialized to a `CountMinSketch` before usage.\n",
      "        Count-min sketch is a probabilistic data structure used for cardinality estimation\n",
      "        using sub-linear space.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        eps : :class:`~pyspark.sql.Column` or str\n",
      "            relative error, must be positive\n",
      "        confidence : :class:`~pyspark.sql.Column` or str\n",
      "            confidence, must be positive and less than 1.0\n",
      "        seed : :class:`~pyspark.sql.Column` or str\n",
      "            random seed\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            count-min sketch of the column\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[1], [2], [1]], ['data'])\n",
      "        >>> df = df.agg(count_min_sketch(df.data, lit(0.5), lit(0.5), lit(1)).alias('sketch'))\n",
      "        >>> df.select(hex(df.sketch).alias('r')).collect()\n",
      "        [Row(r='0000000100000000000000030000000100000004000000005D8D6AB90000000000000000000000000000000200000000000000010000000000000000')]\n",
      "    \n",
      "    covar_pop(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new :class:`~pyspark.sql.Column` for the population covariance of ``col1`` and\n",
      "        ``col2``.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            first column to calculate covariance.\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            second column to calculate covariance.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            covariance of these two column values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> a = [1] * 10\n",
      "        >>> b = [1] * 10\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(covar_pop(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=0.0)]\n",
      "    \n",
      "    covar_samp(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new :class:`~pyspark.sql.Column` for the sample covariance of ``col1`` and\n",
      "        ``col2``.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            first column to calculate covariance.\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            second column to calculate covariance.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            sample covariance of these two column values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> a = [1] * 10\n",
      "        >>> b = [1] * 10\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(covar_samp(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=0.0)]\n",
      "    \n",
      "    crc32(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Calculates the cyclic redundancy check value  (CRC32) of a binary column and\n",
      "        returns the value as a bigint.\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(crc32('a').alias('crc32')).collect()\n",
      "        [Row(crc32=2743272264)]\n",
      "    \n",
      "    create_map(*cols: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName_')], Tuple[ForwardRef('ColumnOrName_'), ...]]) -> pyspark.sql.column.Column\n",
      "        Creates a new map column.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s that are\n",
      "            grouped as key-value pairs, e.g. (key1, value1, key2, value2, ...).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5)], (\"name\", \"age\"))\n",
      "        >>> df.select(create_map('name', 'age').alias(\"map\")).collect()\n",
      "        [Row(map={'Alice': 2}), Row(map={'Bob': 5})]\n",
      "        >>> df.select(create_map([df.name, df.age]).alias(\"map\")).collect()\n",
      "        [Row(map={'Alice': 2}), Row(map={'Bob': 5})]\n",
      "    \n",
      "    csc(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes cosecant of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            angle in radians.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            cosecant of the angle.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import math\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(csc(lit(math.radians(90)))).first()\n",
      "        Row(CSC(1.57079...)=1.0)\n",
      "    \n",
      "    cume_dist() -> pyspark.sql.column.Column\n",
      "        Window function: returns the cumulative distribution of values within a window partition,\n",
      "        i.e. the fraction of rows that are below the current row.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for calculating cumulative distribution.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window, types\n",
      "        >>> df = spark.createDataFrame([1, 2, 3, 3, 4], types.IntegerType())\n",
      "        >>> w = Window.orderBy(\"value\")\n",
      "        >>> df.withColumn(\"cd\", cume_dist().over(w)).show()\n",
      "        +-----+---+\n",
      "        |value| cd|\n",
      "        +-----+---+\n",
      "        |    1|0.2|\n",
      "        |    2|0.4|\n",
      "        |    3|0.8|\n",
      "        |    3|0.8|\n",
      "        |    4|1.0|\n",
      "        +-----+---+\n",
      "    \n",
      "    curdate() -> pyspark.sql.column.Column\n",
      "        Returns the current date at the start of query evaluation as a :class:`DateType` column.\n",
      "        All calls of current_date within the same query return the same value.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            current date.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(1).select(sf.curdate()).show() # doctest: +SKIP\n",
      "        +--------------+\n",
      "        |current_date()|\n",
      "        +--------------+\n",
      "        |    2022-08-26|\n",
      "        +--------------+\n",
      "    \n",
      "    current_catalog() -> pyspark.sql.column.Column\n",
      "        Returns the current catalog.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.range(1).select(current_catalog()).show()\n",
      "        +-----------------+\n",
      "        |current_catalog()|\n",
      "        +-----------------+\n",
      "        |    spark_catalog|\n",
      "        +-----------------+\n",
      "    \n",
      "    current_database() -> pyspark.sql.column.Column\n",
      "        Returns the current database.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.range(1).select(current_database()).show()\n",
      "        +------------------+\n",
      "        |current_database()|\n",
      "        +------------------+\n",
      "        |           default|\n",
      "        +------------------+\n",
      "    \n",
      "    current_date() -> pyspark.sql.column.Column\n",
      "        Returns the current date at the start of query evaluation as a :class:`DateType` column.\n",
      "        All calls of current_date within the same query return the same value.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            current date.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(current_date()).show() # doctest: +SKIP\n",
      "        +--------------+\n",
      "        |current_date()|\n",
      "        +--------------+\n",
      "        |    2022-08-26|\n",
      "        +--------------+\n",
      "    \n",
      "    current_schema() -> pyspark.sql.column.Column\n",
      "        Returns the current database.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(1).select(sf.current_schema()).show()\n",
      "        +------------------+\n",
      "        |current_database()|\n",
      "        +------------------+\n",
      "        |           default|\n",
      "        +------------------+\n",
      "    \n",
      "    current_timestamp() -> pyspark.sql.column.Column\n",
      "        Returns the current timestamp at the start of query evaluation as a :class:`TimestampType`\n",
      "        column. All calls of current_timestamp within the same query return the same value.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            current date and time.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(current_timestamp()).show(truncate=False) # doctest: +SKIP\n",
      "        +-----------------------+\n",
      "        |current_timestamp()    |\n",
      "        +-----------------------+\n",
      "        |2022-08-26 21:23:22.716|\n",
      "        +-----------------------+\n",
      "    \n",
      "    current_timezone() -> pyspark.sql.column.Column\n",
      "        Returns the current session local timezone.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            current session local timezone.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> spark.range(1).select(current_timezone()).show()\n",
      "        +-------------------+\n",
      "        | current_timezone()|\n",
      "        +-------------------+\n",
      "        |America/Los_Angeles|\n",
      "        +-------------------+\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    current_user() -> pyspark.sql.column.Column\n",
      "        Returns the current database.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.range(1).select(current_user()).show() # doctest: +SKIP\n",
      "        +--------------+\n",
      "        |current_user()|\n",
      "        +--------------+\n",
      "        | ruifeng.zheng|\n",
      "        +--------------+\n",
      "    \n",
      "    date_add(start: 'ColumnOrName', days: Union[ForwardRef('ColumnOrName'), int]) -> pyspark.sql.column.Column\n",
      "        Returns the date that is `days` days after `start`. If `days` is a negative value\n",
      "        then these amount of days will be deducted from `start`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        start : :class:`~pyspark.sql.Column` or str\n",
      "            date column to work on.\n",
      "        days : :class:`~pyspark.sql.Column` or str or int\n",
      "            how many days after the given date to calculate.\n",
      "            Accepts negative value as well to calculate backwards in time.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a date after/before given number of days.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08', 2,)], ['dt', 'add'])\n",
      "        >>> df.select(date_add(df.dt, 1).alias('next_date')).collect()\n",
      "        [Row(next_date=datetime.date(2015, 4, 9))]\n",
      "        >>> df.select(date_add(df.dt, df.add.cast('integer')).alias('next_date')).collect()\n",
      "        [Row(next_date=datetime.date(2015, 4, 10))]\n",
      "        >>> df.select(date_add('dt', -1).alias('prev_date')).collect()\n",
      "        [Row(prev_date=datetime.date(2015, 4, 7))]\n",
      "    \n",
      "    date_diff(end: 'ColumnOrName', start: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the number of days from `start` to `end`.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        end : :class:`~pyspark.sql.Column` or str\n",
      "            to date column to work on.\n",
      "        start : :class:`~pyspark.sql.Column` or str\n",
      "            from date column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            difference in days between two dates.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n",
      "        >>> df.select(date_diff(df.d2, df.d1).alias('diff')).collect()\n",
      "        [Row(diff=32)]\n",
      "    \n",
      "    date_format(date: 'ColumnOrName', format: str) -> pyspark.sql.column.Column\n",
      "        Converts a date/timestamp/string to a value of string in the format specified by the date\n",
      "        format given by the second argument.\n",
      "        \n",
      "        A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All\n",
      "        pattern letters of `datetime pattern`_. can be used.\n",
      "        \n",
      "        .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Whenever possible, use specialized functions like `year`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        date : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to format.\n",
      "        format: str\n",
      "            format to use to represent datetime values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            string value representing formatted datetime.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()\n",
      "        [Row(date='04/08/2015')]\n",
      "    \n",
      "    date_from_unix_date(days: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Create date from the number of `days` since 1970-01-01.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        days : :class:`~pyspark.sql.Column` or str\n",
      "            the target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the date from the number of days since 1970-01-01.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(date_from_unix_date(lit(1))).show()\n",
      "        +----------------------+\n",
      "        |date_from_unix_date(1)|\n",
      "        +----------------------+\n",
      "        |            1970-01-02|\n",
      "        +----------------------+\n",
      "    \n",
      "    date_part(field: 'ColumnOrName', source: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extracts a part of the date/timestamp or interval source.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        field : :class:`~pyspark.sql.Column` or str\n",
      "            selects which part of the source should be extracted, and supported string values\n",
      "            are as same as the fields of the equivalent function `extract`.\n",
      "        source : :class:`~pyspark.sql.Column` or str\n",
      "            a date/timestamp or interval column from where `field` should be extracted.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a part of the date/timestamp or interval source.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import datetime\n",
      "        >>> df = spark.createDataFrame([(datetime.datetime(2015, 4, 8, 13, 8, 15),)], ['ts'])\n",
      "        >>> df.select(\n",
      "        ...     date_part(lit('YEAR'), 'ts').alias('year'),\n",
      "        ...     date_part(lit('month'), 'ts').alias('month'),\n",
      "        ...     date_part(lit('WEEK'), 'ts').alias('week'),\n",
      "        ...     date_part(lit('D'), 'ts').alias('day'),\n",
      "        ...     date_part(lit('M'), 'ts').alias('minute'),\n",
      "        ...     date_part(lit('S'), 'ts').alias('second')\n",
      "        ... ).collect()\n",
      "        [Row(year=2015, month=4, week=15, day=8, minute=8, second=Decimal('15.000000'))]\n",
      "    \n",
      "    date_sub(start: 'ColumnOrName', days: Union[ForwardRef('ColumnOrName'), int]) -> pyspark.sql.column.Column\n",
      "        Returns the date that is `days` days before `start`. If `days` is a negative value\n",
      "        then these amount of days will be added to `start`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        start : :class:`~pyspark.sql.Column` or str\n",
      "            date column to work on.\n",
      "        days : :class:`~pyspark.sql.Column` or str or int\n",
      "            how many days before the given date to calculate.\n",
      "            Accepts negative value as well to calculate forward in time.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a date before/after given number of days.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08', 2,)], ['dt', 'sub'])\n",
      "        >>> df.select(date_sub(df.dt, 1).alias('prev_date')).collect()\n",
      "        [Row(prev_date=datetime.date(2015, 4, 7))]\n",
      "        >>> df.select(date_sub(df.dt, df.sub.cast('integer')).alias('prev_date')).collect()\n",
      "        [Row(prev_date=datetime.date(2015, 4, 6))]\n",
      "        >>> df.select(date_sub('dt', -1).alias('next_date')).collect()\n",
      "        [Row(next_date=datetime.date(2015, 4, 9))]\n",
      "    \n",
      "    date_trunc(format: str, timestamp: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns timestamp truncated to the unit specified by the format.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        format : str\n",
      "            'year', 'yyyy', 'yy' to truncate by year,\n",
      "            'month', 'mon', 'mm' to truncate by month,\n",
      "            'day', 'dd' to truncate by day,\n",
      "            Other options are:\n",
      "            'microsecond', 'millisecond', 'second', 'minute', 'hour', 'week', 'quarter'\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to truncate.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            truncated timestamp.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 05:02:11',)], ['t'])\n",
      "        >>> df.select(date_trunc('year', df.t).alias('year')).collect()\n",
      "        [Row(year=datetime.datetime(1997, 1, 1, 0, 0))]\n",
      "        >>> df.select(date_trunc('mon', df.t).alias('month')).collect()\n",
      "        [Row(month=datetime.datetime(1997, 2, 1, 0, 0))]\n",
      "    \n",
      "    dateadd(start: 'ColumnOrName', days: Union[ForwardRef('ColumnOrName'), int]) -> pyspark.sql.column.Column\n",
      "        Returns the date that is `days` days after `start`. If `days` is a negative value\n",
      "        then these amount of days will be deducted from `start`.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        start : :class:`~pyspark.sql.Column` or str\n",
      "            date column to work on.\n",
      "        days : :class:`~pyspark.sql.Column` or str or int\n",
      "            how many days after the given date to calculate.\n",
      "            Accepts negative value as well to calculate backwards in time.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a date after/before given number of days.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [('2015-04-08', 2,)], ['dt', 'add']\n",
      "        ... ).select(sf.dateadd(\"dt\", 1)).show()\n",
      "        +---------------+\n",
      "        |date_add(dt, 1)|\n",
      "        +---------------+\n",
      "        |     2015-04-09|\n",
      "        +---------------+\n",
      "        \n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [('2015-04-08', 2,)], ['dt', 'add']\n",
      "        ... ).select(sf.dateadd(\"dt\", sf.lit(2))).show()\n",
      "        +---------------+\n",
      "        |date_add(dt, 2)|\n",
      "        +---------------+\n",
      "        |     2015-04-10|\n",
      "        +---------------+\n",
      "        \n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [('2015-04-08', 2,)], ['dt', 'add']\n",
      "        ... ).select(sf.dateadd(\"dt\", -1)).show()\n",
      "        +----------------+\n",
      "        |date_add(dt, -1)|\n",
      "        +----------------+\n",
      "        |      2015-04-07|\n",
      "        +----------------+\n",
      "    \n",
      "    datediff(end: 'ColumnOrName', start: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the number of days from `start` to `end`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        end : :class:`~pyspark.sql.Column` or str\n",
      "            to date column to work on.\n",
      "        start : :class:`~pyspark.sql.Column` or str\n",
      "            from date column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            difference in days between two dates.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n",
      "        >>> df.select(datediff(df.d2, df.d1).alias('diff')).collect()\n",
      "        [Row(diff=32)]\n",
      "    \n",
      "    datepart(field: 'ColumnOrName', source: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extracts a part of the date/timestamp or interval source.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        field : :class:`~pyspark.sql.Column` or str\n",
      "            selects which part of the source should be extracted, and supported string values\n",
      "            are as same as the fields of the equivalent function `extract`.\n",
      "        source : :class:`~pyspark.sql.Column` or str\n",
      "            a date/timestamp or interval column from where `field` should be extracted.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a part of the date/timestamp or interval source.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import datetime\n",
      "        >>> df = spark.createDataFrame([(datetime.datetime(2015, 4, 8, 13, 8, 15),)], ['ts'])\n",
      "        >>> df.select(\n",
      "        ...     datepart(lit('YEAR'), 'ts').alias('year'),\n",
      "        ...     datepart(lit('month'), 'ts').alias('month'),\n",
      "        ...     datepart(lit('WEEK'), 'ts').alias('week'),\n",
      "        ...     datepart(lit('D'), 'ts').alias('day'),\n",
      "        ...     datepart(lit('M'), 'ts').alias('minute'),\n",
      "        ...     datepart(lit('S'), 'ts').alias('second')\n",
      "        ... ).collect()\n",
      "        [Row(year=2015, month=4, week=15, day=8, minute=8, second=Decimal('15.000000'))]\n",
      "    \n",
      "    day(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the day of the month of a given date/timestamp as integer.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            day of the month for given date/timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(day('dt').alias('day')).collect()\n",
      "        [Row(day=8)]\n",
      "    \n",
      "    dayofmonth(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the day of the month of a given date/timestamp as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            day of the month for given date/timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(dayofmonth('dt').alias('day')).collect()\n",
      "        [Row(day=8)]\n",
      "    \n",
      "    dayofweek(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the day of the week of a given date/timestamp as integer.\n",
      "        Ranges from 1 for a Sunday through to 7 for a Saturday\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            day of the week for given date/timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(dayofweek('dt').alias('day')).collect()\n",
      "        [Row(day=4)]\n",
      "    \n",
      "    dayofyear(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the day of the year of a given date/timestamp as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            day of the year for given date/timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(dayofyear('dt').alias('day')).collect()\n",
      "        [Row(day=98)]\n",
      "    \n",
      "    days(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Partition transform function: A transform for timestamps and dates\n",
      "        to partition data into days.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date or timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            data partitioned by days.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").partitionedBy(  # doctest: +SKIP\n",
      "        ...     days(\"ts\")\n",
      "        ... ).createOrReplace()\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function can be used only in combination with\n",
      "        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n",
      "        method of the `DataFrameWriterV2`.\n",
      "    \n",
      "    decode(col: 'ColumnOrName', charset: str) -> pyspark.sql.column.Column\n",
      "        Computes the first argument into a string from a binary using the provided character set\n",
      "        (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        charset : str\n",
      "            charset to use to decode to.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['a'])\n",
      "        >>> df.select(decode(\"a\", \"UTF-8\")).show()\n",
      "        +----------------+\n",
      "        |decode(a, UTF-8)|\n",
      "        +----------------+\n",
      "        |            abcd|\n",
      "        +----------------+\n",
      "    \n",
      "    degrees(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Converts an angle measured in radians to an approximately equivalent angle\n",
      "        measured in degrees.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            angle in radians\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            angle in degrees, as if computed by `java.lang.Math.toDegrees()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import math\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(degrees(lit(math.pi))).first()\n",
      "        Row(DEGREES(3.14159...)=180.0)\n",
      "    \n",
      "    dense_rank() -> pyspark.sql.column.Column\n",
      "        Window function: returns the rank of rows within a window partition, without any gaps.\n",
      "        \n",
      "        The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking\n",
      "        sequence when there are ties. That is, if you were ranking a competition using dense_rank\n",
      "        and had three people tie for second place, you would say that all three were in second\n",
      "        place and that the next person came in third. Rank would give me sequential numbers, making\n",
      "        the person that came in third place (after the ties) would register as coming in fifth.\n",
      "        \n",
      "        This is equivalent to the DENSE_RANK function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for calculating ranks.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window, types\n",
      "        >>> df = spark.createDataFrame([1, 1, 2, 3, 3, 4], types.IntegerType())\n",
      "        >>> w = Window.orderBy(\"value\")\n",
      "        >>> df.withColumn(\"drank\", dense_rank().over(w)).show()\n",
      "        +-----+-----+\n",
      "        |value|drank|\n",
      "        +-----+-----+\n",
      "        |    1|    1|\n",
      "        |    1|    1|\n",
      "        |    2|    2|\n",
      "        |    3|    3|\n",
      "        |    3|    3|\n",
      "        |    4|    4|\n",
      "        +-----+-----+\n",
      "    \n",
      "    desc(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a sort expression based on the descending order of the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to sort by in the descending order.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column specifying the order.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Sort by the column 'id' in the descending order.\n",
      "        \n",
      "        >>> spark.range(5).orderBy(desc(\"id\")).show()\n",
      "        +---+\n",
      "        | id|\n",
      "        +---+\n",
      "        |  4|\n",
      "        |  3|\n",
      "        |  2|\n",
      "        |  1|\n",
      "        |  0|\n",
      "        +---+\n",
      "    \n",
      "    desc_nulls_first(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a sort expression based on the descending order of the given\n",
      "        column name, and null values appear before non-null values.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to sort by in the descending order.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column specifying the order.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df1 = spark.createDataFrame([(0, None),\n",
      "        ...                              (1, \"Bob\"),\n",
      "        ...                              (2, \"Alice\")], [\"age\", \"name\"])\n",
      "        >>> df1.sort(desc_nulls_first(df1.name)).show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  0| NULL|\n",
      "        |  1|  Bob|\n",
      "        |  2|Alice|\n",
      "        +---+-----+\n",
      "    \n",
      "    desc_nulls_last(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a sort expression based on the descending order of the given\n",
      "        column name, and null values appear after non-null values.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to sort by in the descending order.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column specifying the order.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df1 = spark.createDataFrame([(0, None),\n",
      "        ...                              (1, \"Bob\"),\n",
      "        ...                              (2, \"Alice\")], [\"age\", \"name\"])\n",
      "        >>> df1.sort(desc_nulls_last(df1.name)).show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  1|  Bob|\n",
      "        |  2|Alice|\n",
      "        |  0| NULL|\n",
      "        +---+-----+\n",
      "    \n",
      "    e() -> pyspark.sql.column.Column\n",
      "        Returns Euler's number.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.range(1).select(e()).show()\n",
      "        +-----------------+\n",
      "        |              E()|\n",
      "        +-----------------+\n",
      "        |2.718281828459045|\n",
      "        +-----------------+\n",
      "    \n",
      "    element_at(col: 'ColumnOrName', extraction: Any) -> pyspark.sql.column.Column\n",
      "        Collection function: Returns element of array at given index in `extraction` if col is array.\n",
      "        Returns value for the given key in `extraction` if col is map. If position is negative\n",
      "        then location of the element will start from end, if number is outside the\n",
      "        array boundaries then None will be returned.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array or map\n",
      "        extraction :\n",
      "            index to check for in array or key to check for in map\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value at given position.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not zero based, but 1 based index.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        :meth:`get`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], ['data'])\n",
      "        >>> df.select(element_at(df.data, 1)).collect()\n",
      "        [Row(element_at(data, 1)='a')]\n",
      "        >>> df.select(element_at(df.data, -1)).collect()\n",
      "        [Row(element_at(data, -1)='c')]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([({\"a\": 1.0, \"b\": 2.0},)], ['data'])\n",
      "        >>> df.select(element_at(df.data, lit(\"a\"))).collect()\n",
      "        [Row(element_at(data, a)=1.0)]\n",
      "    \n",
      "    elt(*inputs: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the `n`-th input, e.g., returns `input2` when `n` is 2.\n",
      "        The function returns NULL if the index exceeds the length of the array\n",
      "        and `spark.sql.ansi.enabled` is set to false. If `spark.sql.ansi.enabled` is set to true,\n",
      "        it throws ArrayIndexOutOfBoundsException for invalid indices.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        inputs : :class:`~pyspark.sql.Column` or str\n",
      "            Input columns or strings.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, \"scala\", \"java\")], ['a', 'b', 'c'])\n",
      "        >>> df.select(elt(df.a, df.b, df.c).alias('r')).collect()\n",
      "        [Row(r='scala')]\n",
      "    \n",
      "    encode(col: 'ColumnOrName', charset: str) -> pyspark.sql.column.Column\n",
      "        Computes the first argument into a binary from a string using the provided character set\n",
      "        (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        charset : str\n",
      "            charset to use to encode.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['c'])\n",
      "        >>> df.select(encode(\"c\", \"UTF-8\")).show()\n",
      "        +----------------+\n",
      "        |encode(c, UTF-8)|\n",
      "        +----------------+\n",
      "        |   [61 62 63 64]|\n",
      "        +----------------+\n",
      "    \n",
      "    endswith(str: 'ColumnOrName', suffix: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a boolean. The value is True if str ends with suffix.\n",
      "        Returns NULL if either input expression is NULL. Otherwise, returns False.\n",
      "        Both str or suffix must be of STRING or BINARY type.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string.\n",
      "        suffix : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string, the suffix.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Spark SQL\", \"Spark\",)], [\"a\", \"b\"])\n",
      "        >>> df.select(endswith(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r=False)]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"414243\", \"4243\",)], [\"e\", \"f\"])\n",
      "        >>> df = df.select(to_binary(\"e\").alias(\"e\"), to_binary(\"f\").alias(\"f\"))\n",
      "        >>> df.printSchema()\n",
      "        root\n",
      "         |-- e: binary (nullable = true)\n",
      "         |-- f: binary (nullable = true)\n",
      "        >>> df.select(endswith(\"e\", \"f\"), endswith(\"f\", \"e\")).show()\n",
      "        +--------------+--------------+\n",
      "        |endswith(e, f)|endswith(f, e)|\n",
      "        +--------------+--------------+\n",
      "        |          true|         false|\n",
      "        +--------------+--------------+\n",
      "    \n",
      "    equal_null(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns same result as the EQUAL(=) operator for non-null operands,\n",
      "        but returns true if both are null, false if one of the them is null.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(None, None,), (1, 9,)], [\"a\", \"b\"])\n",
      "        >>> df.select(equal_null(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r=True), Row(r=False)]\n",
      "    \n",
      "    every(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns true if all values of `col` are true.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to check if all values are true.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            true if all values of `col` are true, false otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [[True], [True], [True]], [\"flag\"]\n",
      "        ... ).select(sf.every(\"flag\")).show()\n",
      "        +-----------+\n",
      "        |every(flag)|\n",
      "        +-----------+\n",
      "        |       true|\n",
      "        +-----------+\n",
      "        \n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [[True], [False], [True]], [\"flag\"]\n",
      "        ... ).select(sf.every(\"flag\")).show()\n",
      "        +-----------+\n",
      "        |every(flag)|\n",
      "        +-----------+\n",
      "        |      false|\n",
      "        +-----------+\n",
      "        \n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [[False], [False], [False]], [\"flag\"]\n",
      "        ... ).select(sf.every(\"flag\")).show()\n",
      "        +-----------+\n",
      "        |every(flag)|\n",
      "        +-----------+\n",
      "        |      false|\n",
      "        +-----------+\n",
      "    \n",
      "    exists(col: 'ColumnOrName', f: Callable[[pyspark.sql.column.Column], pyspark.sql.column.Column]) -> pyspark.sql.column.Column\n",
      "        Returns whether a predicate holds for one or more elements in the array.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            ``(x: Column) -> Column: ...``  returning the Boolean expression.\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            True if \"any\" element of an array evaluates to True when passed as an argument to\n",
      "            given function and False otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, [1, 2, 3, 4]), (2, [3, -1, 0])],(\"key\", \"values\"))\n",
      "        >>> df.select(exists(\"values\", lambda x: x < 0).alias(\"any_negative\")).show()\n",
      "        +------------+\n",
      "        |any_negative|\n",
      "        +------------+\n",
      "        |       false|\n",
      "        |        true|\n",
      "        +------------+\n",
      "    \n",
      "    exp(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the exponential of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to calculate exponential for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            exponential of the given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(exp(lit(0))).show()\n",
      "        +------+\n",
      "        |EXP(0)|\n",
      "        +------+\n",
      "        |   1.0|\n",
      "        +------+\n",
      "    \n",
      "    explode(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new row for each element in the given array or map.\n",
      "        Uses the default column name `col` for elements in the array and\n",
      "        `key` and `value` for elements in the map unless specified otherwise.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            one row per array item or map key value.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        :meth:`pyspark.functions.posexplode`\n",
      "        :meth:`pyspark.functions.explode_outer`\n",
      "        :meth:`pyspark.functions.posexplode_outer`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
      "        >>> df.select(explode(df.intlist).alias(\"anInt\")).collect()\n",
      "        [Row(anInt=1), Row(anInt=2), Row(anInt=3)]\n",
      "        \n",
      "        >>> df.select(explode(df.mapfield).alias(\"key\", \"value\")).show()\n",
      "        +---+-----+\n",
      "        |key|value|\n",
      "        +---+-----+\n",
      "        |  a|    b|\n",
      "        +---+-----+\n",
      "    \n",
      "    explode_outer(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new row for each element in the given array or map.\n",
      "        Unlike explode, if the array/map is null or empty then null is produced.\n",
      "        Uses the default column name `col` for elements in the array and\n",
      "        `key` and `value` for elements in the map unless specified otherwise.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            one row per array item or map key value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, [\"foo\", \"bar\"], {\"x\": 1.0}), (2, [], {}), (3, None, None)],\n",
      "        ...     (\"id\", \"an_array\", \"a_map\")\n",
      "        ... )\n",
      "        >>> df.select(\"id\", \"an_array\", explode_outer(\"a_map\")).show()\n",
      "        +---+----------+----+-----+\n",
      "        | id|  an_array| key|value|\n",
      "        +---+----------+----+-----+\n",
      "        |  1|[foo, bar]|   x|  1.0|\n",
      "        |  2|        []|NULL| NULL|\n",
      "        |  3|      NULL|NULL| NULL|\n",
      "        +---+----------+----+-----+\n",
      "        \n",
      "        >>> df.select(\"id\", \"a_map\", explode_outer(\"an_array\")).show()\n",
      "        +---+----------+----+\n",
      "        | id|     a_map| col|\n",
      "        +---+----------+----+\n",
      "        |  1|{x -> 1.0}| foo|\n",
      "        |  1|{x -> 1.0}| bar|\n",
      "        |  2|        {}|NULL|\n",
      "        |  3|      NULL|NULL|\n",
      "        +---+----------+----+\n",
      "    \n",
      "    expm1(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the exponential of the given value minus one.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to calculate exponential for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            exponential less one.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(expm1(lit(1))).first()\n",
      "        Row(EXPM1(1)=1.71828...)\n",
      "    \n",
      "    expr(str: str) -> pyspark.sql.column.Column\n",
      "        Parses the expression string into the column that it represents\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : str\n",
      "            expression defined in string.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            column representing the expression.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[\"Alice\"], [\"Bob\"]], [\"name\"])\n",
      "        >>> df.select(\"name\", expr(\"length(name)\")).show()\n",
      "        +-----+------------+\n",
      "        | name|length(name)|\n",
      "        +-----+------------+\n",
      "        |Alice|           5|\n",
      "        |  Bob|           3|\n",
      "        +-----+------------+\n",
      "    \n",
      "    extract(field: 'ColumnOrName', source: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extracts a part of the date/timestamp or interval source.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        field : :class:`~pyspark.sql.Column` or str\n",
      "            selects which part of the source should be extracted.\n",
      "        source : :class:`~pyspark.sql.Column` or str\n",
      "            a date/timestamp or interval column from where `field` should be extracted.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a part of the date/timestamp or interval source.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import datetime\n",
      "        >>> df = spark.createDataFrame([(datetime.datetime(2015, 4, 8, 13, 8, 15),)], ['ts'])\n",
      "        >>> df.select(\n",
      "        ...     extract(lit('YEAR'), 'ts').alias('year'),\n",
      "        ...     extract(lit('month'), 'ts').alias('month'),\n",
      "        ...     extract(lit('WEEK'), 'ts').alias('week'),\n",
      "        ...     extract(lit('D'), 'ts').alias('day'),\n",
      "        ...     extract(lit('M'), 'ts').alias('minute'),\n",
      "        ...     extract(lit('S'), 'ts').alias('second')\n",
      "        ... ).collect()\n",
      "        [Row(year=2015, month=4, week=15, day=8, minute=8, second=Decimal('15.000000'))]\n",
      "    \n",
      "    factorial(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the factorial of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            a column to calculate factorial for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            factorial of given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(5,)], ['n'])\n",
      "        >>> df.select(factorial(df.n).alias('f')).collect()\n",
      "        [Row(f=120)]\n",
      "    \n",
      "    filter(col: 'ColumnOrName', f: Union[Callable[[pyspark.sql.column.Column], pyspark.sql.column.Column], Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]]) -> pyspark.sql.column.Column\n",
      "        Returns an array of elements for which a predicate holds in a given array.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            A function that returns the Boolean expression.\n",
      "            Can take one of the following forms:\n",
      "        \n",
      "            - Unary ``(x: Column) -> Column: ...``\n",
      "            - Binary ``(x: Column, i: Column) -> Column...``, where the second argument is\n",
      "                a 0-based index of the element.\n",
      "        \n",
      "            and can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            filtered array of elements where given function evaluated to True\n",
      "            when passed as an argument.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, [\"2018-09-20\",  \"2019-02-03\", \"2019-07-01\", \"2020-06-01\"])],\n",
      "        ...     (\"key\", \"values\")\n",
      "        ... )\n",
      "        >>> def after_second_quarter(x):\n",
      "        ...     return month(to_date(x)) > 6\n",
      "        ...\n",
      "        >>> df.select(\n",
      "        ...     filter(\"values\", after_second_quarter).alias(\"after_second_quarter\")\n",
      "        ... ).show(truncate=False)\n",
      "        +------------------------+\n",
      "        |after_second_quarter    |\n",
      "        +------------------------+\n",
      "        |[2018-09-20, 2019-07-01]|\n",
      "        +------------------------+\n",
      "    \n",
      "    find_in_set(str: 'ColumnOrName', str_array: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the index (1-based) of the given string (`str`) in the comma-delimited\n",
      "        list (`strArray`). Returns 0, if the string was not found or if the given string (`str`)\n",
      "        contains a comma.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            The given string to be found.\n",
      "        str_array : :class:`~pyspark.sql.Column` or str\n",
      "            The comma-delimited list.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"ab\", \"abc,b,ab,c,def\")], ['a', 'b'])\n",
      "        >>> df.select(find_in_set(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r=3)]\n",
      "    \n",
      "    first(col: 'ColumnOrName', ignorenulls: bool = False) -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the first value in a group.\n",
      "        \n",
      "        The function by default returns the first values it sees. It will return the first non-null\n",
      "        value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic because its results depends on the order of the\n",
      "        rows which may be non-deterministic after a shuffle.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to fetch first value for.\n",
      "        ignorenulls : :class:`~pyspark.sql.Column` or str\n",
      "            if first value is null then look for first non-null value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            first value of the group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5), (\"Alice\", None)], (\"name\", \"age\"))\n",
      "        >>> df = df.orderBy(df.age)\n",
      "        >>> df.groupby(\"name\").agg(first(\"age\")).orderBy(\"name\").show()\n",
      "        +-----+----------+\n",
      "        | name|first(age)|\n",
      "        +-----+----------+\n",
      "        |Alice|      NULL|\n",
      "        |  Bob|         5|\n",
      "        +-----+----------+\n",
      "        \n",
      "        Now, to ignore any nulls we needs to set ``ignorenulls`` to `True`\n",
      "        \n",
      "        >>> df.groupby(\"name\").agg(first(\"age\", ignorenulls=True)).orderBy(\"name\").show()\n",
      "        +-----+----------+\n",
      "        | name|first(age)|\n",
      "        +-----+----------+\n",
      "        |Alice|         2|\n",
      "        |  Bob|         5|\n",
      "        +-----+----------+\n",
      "    \n",
      "    first_value(col: 'ColumnOrName', ignoreNulls: Union[bool, pyspark.sql.column.Column, NoneType] = None) -> pyspark.sql.column.Column\n",
      "        Returns the first value of `col` for a group of rows. It will return the first non-null\n",
      "        value it sees when `ignoreNulls` is set to true. If all values are null, then null is returned.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        ignorenulls : :class:`~pyspark.sql.Column` or bool\n",
      "            if first value is null then look for first non-null value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            some value of `col` for a group of rows.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(None, 1), (\"a\", 2), (\"a\", 3), (\"b\", 8), (\"b\", 2)], [\"a\", \"b\"]\n",
      "        ... ).select(sf.first_value('a'), sf.first_value('b')).show()\n",
      "        +--------------+--------------+\n",
      "        |first_value(a)|first_value(b)|\n",
      "        +--------------+--------------+\n",
      "        |          NULL|             1|\n",
      "        +--------------+--------------+\n",
      "        \n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(None, 1), (\"a\", 2), (\"a\", 3), (\"b\", 8), (\"b\", 2)], [\"a\", \"b\"]\n",
      "        ... ).select(sf.first_value('a', True), sf.first_value('b', True)).show()\n",
      "        +--------------+--------------+\n",
      "        |first_value(a)|first_value(b)|\n",
      "        +--------------+--------------+\n",
      "        |             a|             1|\n",
      "        +--------------+--------------+\n",
      "    \n",
      "    flatten(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: creates a single array from an array of arrays.\n",
      "        If a structure of nested arrays is deeper than two levels,\n",
      "        only one level of nesting is removed.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            flattened array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([[1, 2, 3], [4, 5], [6]],), ([None, [4, 5]],)], ['data'])\n",
      "        >>> df.show(truncate=False)\n",
      "        +------------------------+\n",
      "        |data                    |\n",
      "        +------------------------+\n",
      "        |[[1, 2, 3], [4, 5], [6]]|\n",
      "        |[NULL, [4, 5]]          |\n",
      "        +------------------------+\n",
      "        >>> df.select(flatten(df.data).alias('r')).show()\n",
      "        +------------------+\n",
      "        |                 r|\n",
      "        +------------------+\n",
      "        |[1, 2, 3, 4, 5, 6]|\n",
      "        |              NULL|\n",
      "        +------------------+\n",
      "    \n",
      "    floor(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the floor of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to find floor for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            nearest integer that is less than or equal to given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(floor(lit(2.5))).show()\n",
      "        +----------+\n",
      "        |FLOOR(2.5)|\n",
      "        +----------+\n",
      "        |         2|\n",
      "        +----------+\n",
      "    \n",
      "    forall(col: 'ColumnOrName', f: Callable[[pyspark.sql.column.Column], pyspark.sql.column.Column]) -> pyspark.sql.column.Column\n",
      "        Returns whether a predicate holds for every element in the array.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            ``(x: Column) -> Column: ...``  returning the Boolean expression.\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            True if \"all\" elements of an array evaluates to True when passed as an argument to\n",
      "            given function and False otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, [\"bar\"]), (2, [\"foo\", \"bar\"]), (3, [\"foobar\", \"foo\"])],\n",
      "        ...     (\"key\", \"values\")\n",
      "        ... )\n",
      "        >>> df.select(forall(\"values\", lambda x: x.rlike(\"foo\")).alias(\"all_foo\")).show()\n",
      "        +-------+\n",
      "        |all_foo|\n",
      "        +-------+\n",
      "        |  false|\n",
      "        |  false|\n",
      "        |   true|\n",
      "        +-------+\n",
      "    \n",
      "    format_number(col: 'ColumnOrName', d: int) -> pyspark.sql.column.Column\n",
      "        Formats the number X to a format like '#,--#,--#.--', rounded to d decimal places\n",
      "        with HALF_EVEN round mode, and returns the result as a string.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            the column name of the numeric value to be formatted\n",
      "        d : int\n",
      "            the N decimal places\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column of formatted results.\n",
      "        \n",
      "        >>> spark.createDataFrame([(5,)], ['a']).select(format_number('a', 4).alias('v')).collect()\n",
      "        [Row(v='5.0000')]\n",
      "    \n",
      "    format_string(format: str, *cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Formats the arguments in printf-style and returns the result as a string column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        format : str\n",
      "            string that can contain embedded format tags and used as result column's value\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s to be used in formatting\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column of formatted results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(5, \"hello\")], ['a', 'b'])\n",
      "        >>> df.select(format_string('%d %s', df.a, df.b).alias('v')).collect()\n",
      "        [Row(v='5 hello')]\n",
      "    \n",
      "    from_csv(col: 'ColumnOrName', schema: Union[pyspark.sql.column.Column, str], options: Optional[Dict[str, str]] = None) -> pyspark.sql.column.Column\n",
      "        Parses a column containing a CSV string to a row with the specified schema.\n",
      "        Returns `null`, in the case of an unparseable string.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            a column or column name in CSV format\n",
      "        schema :class:`~pyspark.sql.Column` or str\n",
      "            a column, or Python string literal with schema in DDL format, to use when parsing the CSV column.\n",
      "        options : dict, optional\n",
      "            options to control parsing. accepts the same options as the CSV datasource.\n",
      "            See `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
      "            for the version you use.\n",
      "        \n",
      "            .. # noqa\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a column of parsed CSV values\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> data = [(\"1,2,3\",)]\n",
      "        >>> df = spark.createDataFrame(data, (\"value\",))\n",
      "        >>> df.select(from_csv(df.value, \"a INT, b INT, c INT\").alias(\"csv\")).collect()\n",
      "        [Row(csv=Row(a=1, b=2, c=3))]\n",
      "        >>> value = data[0][0]\n",
      "        >>> df.select(from_csv(df.value, schema_of_csv(value)).alias(\"csv\")).collect()\n",
      "        [Row(csv=Row(_c0=1, _c1=2, _c2=3))]\n",
      "        >>> data = [(\"   abc\",)]\n",
      "        >>> df = spark.createDataFrame(data, (\"value\",))\n",
      "        >>> options = {'ignoreLeadingWhiteSpace': True}\n",
      "        >>> df.select(from_csv(df.value, \"s string\", options).alias(\"csv\")).collect()\n",
      "        [Row(csv=Row(s='abc'))]\n",
      "    \n",
      "    from_json(col: 'ColumnOrName', schema: Union[pyspark.sql.types.ArrayType, pyspark.sql.types.StructType, pyspark.sql.column.Column, str], options: Optional[Dict[str, str]] = None) -> pyspark.sql.column.Column\n",
      "        Parses a column containing a JSON string into a :class:`MapType` with :class:`StringType`\n",
      "        as keys type, :class:`StructType` or :class:`ArrayType` with\n",
      "        the specified schema. Returns `null`, in the case of an unparseable string.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            a column or column name in JSON format\n",
      "        schema : :class:`DataType` or str\n",
      "            a StructType, ArrayType of StructType or Python string literal with a DDL-formatted string\n",
      "            to use when parsing the json column\n",
      "        options : dict, optional\n",
      "            options to control parsing. accepts the same options as the json datasource.\n",
      "            See `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n",
      "            for the version you use.\n",
      "        \n",
      "            .. # noqa\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a new column of complex type from given JSON object.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.types import *\n",
      "        >>> data = [(1, '''{\"a\": 1}''')]\n",
      "        >>> schema = StructType([StructField(\"a\", IntegerType())])\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=Row(a=1))]\n",
      "        >>> df.select(from_json(df.value, \"a INT\").alias(\"json\")).collect()\n",
      "        [Row(json=Row(a=1))]\n",
      "        >>> df.select(from_json(df.value, \"MAP<STRING,INT>\").alias(\"json\")).collect()\n",
      "        [Row(json={'a': 1})]\n",
      "        >>> data = [(1, '''[{\"a\": 1}]''')]\n",
      "        >>> schema = ArrayType(StructType([StructField(\"a\", IntegerType())]))\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=[Row(a=1)])]\n",
      "        >>> schema = schema_of_json(lit('''{\"a\": 0}'''))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=Row(a=None))]\n",
      "        >>> data = [(1, '''[1, 2, 3]''')]\n",
      "        >>> schema = ArrayType(IntegerType())\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=[1, 2, 3])]\n",
      "    \n",
      "    from_unixtime(timestamp: 'ColumnOrName', format: str = 'yyyy-MM-dd HH:mm:ss') -> pyspark.sql.column.Column\n",
      "        Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string\n",
      "        representing the timestamp of that moment in the current system time zone in the given\n",
      "        format.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str\n",
      "            column of unix time values.\n",
      "        format : str, optional\n",
      "            format to use to convert to (default: yyyy-MM-dd HH:mm:ss)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            formatted timestamp as string.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> time_df = spark.createDataFrame([(1428476400,)], ['unix_time'])\n",
      "        >>> time_df.select(from_unixtime('unix_time').alias('ts')).collect()\n",
      "        [Row(ts='2015-04-08 00:00:00')]\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    from_utc_timestamp(timestamp: 'ColumnOrName', tz: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function\n",
      "        takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in UTC, and\n",
      "        renders that timestamp as a timestamp in the given time zone.\n",
      "        \n",
      "        However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not\n",
      "        timezone-agnostic. So in Spark this function just shift the timestamp value from UTC timezone to\n",
      "        the given timezone.\n",
      "        \n",
      "        This function may return confusing result if the input is a string with timezone, e.g.\n",
      "        '2018-03-13T06:18:23+00:00'. The reason is that, Spark firstly cast the string to timestamp\n",
      "        according to the timezone in the string, and finally display the result by converting the\n",
      "        timestamp to string according to the session local timezone.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str\n",
      "            the column that contains timestamps\n",
      "        tz : :class:`~pyspark.sql.Column` or str\n",
      "            A string detailing the time zone ID that the input should be adjusted to. It should\n",
      "            be in the format of either region-based zone IDs or zone offsets. Region IDs must\n",
      "            have the form 'area/city', such as 'America/Los_Angeles'. Zone offsets must be in\n",
      "            the format '(+|-)HH:mm', for example '-08:00' or '+01:00'. Also 'UTC' and 'Z' are\n",
      "            supported as aliases of '+00:00'. Other short names are not recommended to use\n",
      "            because they can be ambiguous.\n",
      "        \n",
      "            .. versionchanged:: 2.4\n",
      "               `tz` can take a :class:`~pyspark.sql.Column` containing timezone ID strings.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            timestamp value represented in given timezone.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00', 'JST')], ['ts', 'tz'])\n",
      "        >>> df.select(from_utc_timestamp(df.ts, \"PST\").alias('local_time')).collect()\n",
      "        [Row(local_time=datetime.datetime(1997, 2, 28, 2, 30))]\n",
      "        >>> df.select(from_utc_timestamp(df.ts, df.tz).alias('local_time')).collect()\n",
      "        [Row(local_time=datetime.datetime(1997, 2, 28, 19, 30))]\n",
      "    \n",
      "    get(col: 'ColumnOrName', index: Union[ForwardRef('ColumnOrName'), int]) -> pyspark.sql.column.Column\n",
      "        Collection function: Returns element of array at given (0-based) index.\n",
      "        If the index points outside of the array boundaries, then this function\n",
      "        returns NULL.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        index : :class:`~pyspark.sql.Column` or str or int\n",
      "            index to check for in array\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value at given position.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not 1 based, but 0 based index.\n",
      "        Supports Spark Connect.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        :meth:`element_at`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"], 1)], ['data', 'index'])\n",
      "        >>> df.select(get(df.data, 1)).show()\n",
      "        +------------+\n",
      "        |get(data, 1)|\n",
      "        +------------+\n",
      "        |           b|\n",
      "        +------------+\n",
      "        \n",
      "        >>> df.select(get(df.data, -1)).show()\n",
      "        +-------------+\n",
      "        |get(data, -1)|\n",
      "        +-------------+\n",
      "        |         NULL|\n",
      "        +-------------+\n",
      "        \n",
      "        >>> df.select(get(df.data, 3)).show()\n",
      "        +------------+\n",
      "        |get(data, 3)|\n",
      "        +------------+\n",
      "        |        NULL|\n",
      "        +------------+\n",
      "        \n",
      "        >>> df.select(get(df.data, \"index\")).show()\n",
      "        +----------------+\n",
      "        |get(data, index)|\n",
      "        +----------------+\n",
      "        |               b|\n",
      "        +----------------+\n",
      "        \n",
      "        >>> df.select(get(df.data, col(\"index\") - 1)).show()\n",
      "        +----------------------+\n",
      "        |get(data, (index - 1))|\n",
      "        +----------------------+\n",
      "        |                     a|\n",
      "        +----------------------+\n",
      "    \n",
      "    get_json_object(col: 'ColumnOrName', path: str) -> pyspark.sql.column.Column\n",
      "        Extracts json object from a json string based on json `path` specified, and returns json string\n",
      "        of the extracted json object. It will return null if the input json string is invalid.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            string column in json format\n",
      "        path : str\n",
      "            path to the json object to extract\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            string representation of given JSON object value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n",
      "        >>> df.select(df.key, get_json_object(df.jstring, '$.f1').alias(\"c0\"), \\\n",
      "        ...                   get_json_object(df.jstring, '$.f2').alias(\"c1\") ).collect()\n",
      "        [Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]\n",
      "    \n",
      "    getbit(col: 'ColumnOrName', pos: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the value of the bit (0 or 1) at the specified position.\n",
      "        The positions are numbered from right to left, starting at zero.\n",
      "        The position argument cannot be negative.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        pos : :class:`~pyspark.sql.Column` or str\n",
      "            The positions are numbered from right to left, starting at zero.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the value of the bit (0 or 1) at the specified position.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [[1], [1], [2]], [\"c\"]\n",
      "        ... ).select(sf.getbit(\"c\", sf.lit(1))).show()\n",
      "        +------------+\n",
      "        |getbit(c, 1)|\n",
      "        +------------+\n",
      "        |           0|\n",
      "        |           0|\n",
      "        |           1|\n",
      "        +------------+\n",
      "    \n",
      "    greatest(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the greatest value of the list of column names, skipping null values.\n",
      "        This function takes at least 2 parameters. It will return null if all parameters are null.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            columns to check for gratest value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            gratest value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n",
      "        >>> df.select(greatest(df.a, df.b, df.c).alias(\"greatest\")).collect()\n",
      "        [Row(greatest=4)]\n",
      "    \n",
      "    grouping(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated\n",
      "        or not, returns 1 for aggregated or 0 for not aggregated in the result set.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to check if it's aggregated.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            returns 1 for aggregated or 0 for not aggregated in the result set.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5)], (\"name\", \"age\"))\n",
      "        >>> df.cube(\"name\").agg(grouping(\"name\"), sum(\"age\")).orderBy(\"name\").show()\n",
      "        +-----+--------------+--------+\n",
      "        | name|grouping(name)|sum(age)|\n",
      "        +-----+--------------+--------+\n",
      "        | NULL|             1|       7|\n",
      "        |Alice|             0|       2|\n",
      "        |  Bob|             0|       5|\n",
      "        +-----+--------------+--------+\n",
      "    \n",
      "    grouping_id(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the level of grouping, equals to\n",
      "        \n",
      "           (grouping(c1) << (n-1)) + (grouping(c2) << (n-2)) + ... + grouping(cn)\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The list of columns should match with grouping columns exactly, or empty (means all\n",
      "        the grouping columns).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            columns to check for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            returns level of the grouping it relates to.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, \"a\", \"a\"),\n",
      "        ...                             (3, \"a\", \"a\"),\n",
      "        ...                             (4, \"b\", \"c\")], [\"c1\", \"c2\", \"c3\"])\n",
      "        >>> df.cube(\"c2\", \"c3\").agg(grouping_id(), sum(\"c1\")).orderBy(\"c2\", \"c3\").show()\n",
      "        +----+----+-------------+-------+\n",
      "        |  c2|  c3|grouping_id()|sum(c1)|\n",
      "        +----+----+-------------+-------+\n",
      "        |NULL|NULL|            3|      8|\n",
      "        |NULL|   a|            2|      4|\n",
      "        |NULL|   c|            2|      4|\n",
      "        |   a|NULL|            1|      4|\n",
      "        |   a|   a|            0|      4|\n",
      "        |   b|NULL|            1|      4|\n",
      "        |   b|   c|            0|      4|\n",
      "        +----+----+-------------+-------+\n",
      "    \n",
      "    hash(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Calculates the hash code of given columns, and returns the result as an int column.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            one or more columns to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hash value as int column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('ABC', 'DEF')], ['c1', 'c2'])\n",
      "        \n",
      "        Hash for one column\n",
      "        \n",
      "        >>> df.select(hash('c1').alias('hash')).show()\n",
      "        +----------+\n",
      "        |      hash|\n",
      "        +----------+\n",
      "        |-757602832|\n",
      "        +----------+\n",
      "        \n",
      "        Two or more columns\n",
      "        \n",
      "        >>> df.select(hash('c1', 'c2').alias('hash')).show()\n",
      "        +---------+\n",
      "        |     hash|\n",
      "        +---------+\n",
      "        |599895104|\n",
      "        +---------+\n",
      "    \n",
      "    hex(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes hex value of the given column, which could be :class:`pyspark.sql.types.StringType`,\n",
      "        :class:`pyspark.sql.types.BinaryType`, :class:`pyspark.sql.types.IntegerType` or\n",
      "        :class:`pyspark.sql.types.LongType`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hexadecimal representation of given value as string.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC', 3)], ['a', 'b']).select(hex('a'), hex('b')).collect()\n",
      "        [Row(hex(a)='414243', hex(b)='3')]\n",
      "    \n",
      "    histogram_numeric(col: 'ColumnOrName', nBins: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes a histogram on numeric 'col' using nb bins.\n",
      "        The return value is an array of (x,y) pairs representing the centers of the\n",
      "        histogram's bins. As the value of 'nb' is increased, the histogram approximation\n",
      "        gets finer-grained, but may yield artifacts around outliers. In practice, 20-40\n",
      "        histogram bins appear to work well, with more bins being required for skewed or\n",
      "        smaller datasets. Note that this function creates a histogram with non-uniform\n",
      "        bin widths. It offers no guarantees in terms of the mean-squared-error of the\n",
      "        histogram, but in practice is comparable to the histograms produced by the R/S-Plus\n",
      "        statistical computing packages. Note: the output type of the 'x' field in the return value is\n",
      "        propagated from the input value consumed in the aggregate function.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        nBins : :class:`~pyspark.sql.Column` or str\n",
      "            number of Histogram columns.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a histogram on numeric 'col' using nb bins.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"a\", 1),\n",
      "        ...                             (\"a\", 2),\n",
      "        ...                             (\"a\", 3),\n",
      "        ...                             (\"b\", 8),\n",
      "        ...                             (\"b\", 2)], [\"c1\", \"c2\"])\n",
      "        >>> df.select(histogram_numeric('c2', lit(5))).show()\n",
      "        +------------------------+\n",
      "        |histogram_numeric(c2, 5)|\n",
      "        +------------------------+\n",
      "        |    [{1, 1.0}, {2, 1....|\n",
      "        +------------------------+\n",
      "    \n",
      "    hll_sketch_agg(col: 'ColumnOrName', lgConfigK: Union[int, pyspark.sql.column.Column, NoneType] = None) -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the updatable binary representation of the Datasketches\n",
      "        HllSketch configured with lgConfigK arg.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str or int\n",
      "        lgConfigK : int, optional\n",
      "            The log-base-2 of K, where K is the number of buckets or slots for the HllSketch\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            The binary representation of the HllSketch.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([1,2,2,3], \"INT\")\n",
      "        >>> df1 = df.agg(hll_sketch_estimate(hll_sketch_agg(\"value\")).alias(\"distinct_cnt\"))\n",
      "        >>> df1.show()\n",
      "        +------------+\n",
      "        |distinct_cnt|\n",
      "        +------------+\n",
      "        |           3|\n",
      "        +------------+\n",
      "        >>> df2 = df.agg(hll_sketch_estimate(\n",
      "        ...     hll_sketch_agg(\"value\", lit(12))\n",
      "        ... ).alias(\"distinct_cnt\"))\n",
      "        >>> df2.show()\n",
      "        +------------+\n",
      "        |distinct_cnt|\n",
      "        +------------+\n",
      "        |           3|\n",
      "        +------------+\n",
      "        >>> df3 = df.agg(hll_sketch_estimate(\n",
      "        ...     hll_sketch_agg(col(\"value\"), lit(12))).alias(\"distinct_cnt\"))\n",
      "        >>> df3.show()\n",
      "        +------------+\n",
      "        |distinct_cnt|\n",
      "        +------------+\n",
      "        |           3|\n",
      "        +------------+\n",
      "    \n",
      "    hll_sketch_estimate(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the estimated number of unique values given the binary representation\n",
      "        of a Datasketches HllSketch.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            The estimated number of unique values for the HllSketch.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([1,2,2,3], \"INT\")\n",
      "        >>> df = df.agg(hll_sketch_estimate(hll_sketch_agg(\"value\")).alias(\"distinct_cnt\"))\n",
      "        >>> df.show()\n",
      "        +------------+\n",
      "        |distinct_cnt|\n",
      "        +------------+\n",
      "        |           3|\n",
      "        +------------+\n",
      "    \n",
      "    hll_union(col1: 'ColumnOrName', col2: 'ColumnOrName', allowDifferentLgConfigK: Optional[bool] = None) -> pyspark.sql.column.Column\n",
      "        Merges two binary representations of Datasketches HllSketch objects, using a\n",
      "        Datasketches Union object.  Throws an exception if sketches have different\n",
      "        lgConfigK values and allowDifferentLgConfigK is unset or set to false.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "        allowDifferentLgConfigK : bool, optional\n",
      "            Allow sketches with different lgConfigK values to be merged (defaults to false).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            The binary representation of the merged HllSketch.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1,4),(2,5),(2,5),(3,6)], \"struct<v1:int,v2:int>\")\n",
      "        >>> df = df.agg(hll_sketch_agg(\"v1\").alias(\"sketch1\"), hll_sketch_agg(\"v2\").alias(\"sketch2\"))\n",
      "        >>> df = df.withColumn(\"distinct_cnt\", hll_sketch_estimate(hll_union(\"sketch1\", \"sketch2\")))\n",
      "        >>> df.drop(\"sketch1\", \"sketch2\").show()\n",
      "        +------------+\n",
      "        |distinct_cnt|\n",
      "        +------------+\n",
      "        |           6|\n",
      "        +------------+\n",
      "    \n",
      "    hll_union_agg(col: 'ColumnOrName', allowDifferentLgConfigK: Union[bool, pyspark.sql.column.Column, NoneType] = None) -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the updatable binary representation of the Datasketches\n",
      "        HllSketch, generated by merging previously created Datasketches HllSketch instances\n",
      "        via a Datasketches Union instance. Throws an exception if sketches have different\n",
      "        lgConfigK values and allowDifferentLgConfigK is unset or set to false.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str or bool\n",
      "        allowDifferentLgConfigK : bool, optional\n",
      "            Allow sketches with different lgConfigK values to be merged (defaults to false).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            The binary representation of the merged HllSketch.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df1 = spark.createDataFrame([1,2,2,3], \"INT\")\n",
      "        >>> df1 = df1.agg(hll_sketch_agg(\"value\").alias(\"sketch\"))\n",
      "        >>> df2 = spark.createDataFrame([4,5,5,6], \"INT\")\n",
      "        >>> df2 = df2.agg(hll_sketch_agg(\"value\").alias(\"sketch\"))\n",
      "        >>> df3 = df1.union(df2).agg(hll_sketch_estimate(\n",
      "        ...     hll_union_agg(\"sketch\")\n",
      "        ... ).alias(\"distinct_cnt\"))\n",
      "        >>> df3.drop(\"sketch\").show()\n",
      "        +------------+\n",
      "        |distinct_cnt|\n",
      "        +------------+\n",
      "        |           6|\n",
      "        +------------+\n",
      "        >>> df4 = df1.union(df2).agg(hll_sketch_estimate(\n",
      "        ...     hll_union_agg(\"sketch\", lit(False))\n",
      "        ... ).alias(\"distinct_cnt\"))\n",
      "        >>> df4.drop(\"sketch\").show()\n",
      "        +------------+\n",
      "        |distinct_cnt|\n",
      "        +------------+\n",
      "        |           6|\n",
      "        +------------+\n",
      "        >>> df5 = df1.union(df2).agg(hll_sketch_estimate(\n",
      "        ...     hll_union_agg(col(\"sketch\"), lit(False))\n",
      "        ... ).alias(\"distinct_cnt\"))\n",
      "        >>> df5.drop(\"sketch\").show()\n",
      "        +------------+\n",
      "        |distinct_cnt|\n",
      "        +------------+\n",
      "        |           6|\n",
      "        +------------+\n",
      "    \n",
      "    hour(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the hours of a given timestamp as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hour part of the timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import datetime\n",
      "        >>> df = spark.createDataFrame([(datetime.datetime(2015, 4, 8, 13, 8, 15),)], ['ts'])\n",
      "        >>> df.select(hour('ts').alias('hour')).collect()\n",
      "        [Row(hour=13)]\n",
      "    \n",
      "    hours(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Partition transform function: A transform for timestamps\n",
      "        to partition data into hours.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date or timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            data partitioned by hours.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").partitionedBy(   # doctest: +SKIP\n",
      "        ...     hours(\"ts\")\n",
      "        ... ).createOrReplace()\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function can be used only in combination with\n",
      "        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n",
      "        method of the `DataFrameWriterV2`.\n",
      "    \n",
      "    hypot(col1: Union[ForwardRef('ColumnOrName'), float], col2: Union[ForwardRef('ColumnOrName'), float]) -> pyspark.sql.column.Column\n",
      "        Computes ``sqrt(a^2 + b^2)`` without intermediate overflow or underflow.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : str, :class:`~pyspark.sql.Column` or float\n",
      "            a leg.\n",
      "        col2 : str, :class:`~pyspark.sql.Column` or float\n",
      "            b leg.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            length of the hypotenuse.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(hypot(lit(1), lit(2))).first()\n",
      "        Row(HYPOT(1, 2)=2.23606...)\n",
      "    \n",
      "    ifnull(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns `col2` if `col1` is null, or `col1` otherwise.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> df = spark.createDataFrame([(None,), (1,)], [\"e\"])\n",
      "        >>> df.select(sf.ifnull(df.e, sf.lit(8))).show()\n",
      "        +------------+\n",
      "        |ifnull(e, 8)|\n",
      "        +------------+\n",
      "        |           8|\n",
      "        |           1|\n",
      "        +------------+\n",
      "    \n",
      "    ilike(str: 'ColumnOrName', pattern: 'ColumnOrName', escapeChar: Optional[ForwardRef('Column')] = None) -> pyspark.sql.column.Column\n",
      "        Returns true if str matches `pattern` with `escape` case-insensitively,\n",
      "        null if any arguments are null, false otherwise.\n",
      "        The default escape character is the ''.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            A string.\n",
      "        pattern : :class:`~pyspark.sql.Column` or str\n",
      "            A string. The pattern is a string which is matched literally, with\n",
      "            exception to the following special symbols:\n",
      "            _ matches any one character in the input (similar to . in posix regular expressions)\n",
      "            % matches zero or more characters in the input (similar to .* in posix regular\n",
      "            expressions)\n",
      "            Since Spark 2.0, string literals are unescaped in our SQL parser. For example, in order\n",
      "            to match \"\u0007bc\", the pattern should be \"\\abc\".\n",
      "            When SQL config 'spark.sql.parser.escapedStringLiterals' is enabled, it falls back\n",
      "            to Spark 1.6 behavior regarding string literal parsing. For example, if the config is\n",
      "            enabled, the pattern to match \"\u0007bc\" should be \"\u0007bc\".\n",
      "        escape : :class:`~pyspark.sql.Column`\n",
      "            An character added since Spark 3.0. The default escape character is the ''.\n",
      "            If an escape character precedes a special symbol or another escape character, the\n",
      "            following character is matched literally. It is invalid to escape any other character.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Spark\", \"_park\")], ['a', 'b'])\n",
      "        >>> df.select(ilike(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r=True)]\n",
      "        \n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(\"%SystemDrive%/Users/John\", \"/%SystemDrive/%//Users%\")],\n",
      "        ...     ['a', 'b']\n",
      "        ... )\n",
      "        >>> df.select(ilike(df.a, df.b, lit('/')).alias('r')).collect()\n",
      "        [Row(r=True)]\n",
      "    \n",
      "    initcap(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Translate the first letter of each word to upper case in the sentence.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            string with all first letters are uppercase in each word.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ab cd',)], ['a']).select(initcap(\"a\").alias('v')).collect()\n",
      "        [Row(v='Ab Cd')]\n",
      "    \n",
      "    inline(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Explodes an array of structs into a table.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to explode.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            generator expression with the inline exploded result.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        :meth:`explode`\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Supports Spark Connect.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(structlist=[Row(a=1, b=2), Row(a=3, b=4)])])\n",
      "        >>> df.select(inline(df.structlist)).show()\n",
      "        +---+---+\n",
      "        |  a|  b|\n",
      "        +---+---+\n",
      "        |  1|  2|\n",
      "        |  3|  4|\n",
      "        +---+---+\n",
      "    \n",
      "    inline_outer(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Explodes an array of structs into a table.\n",
      "        Unlike inline, if the array is null or empty then null is produced for each nested column.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to explode.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            generator expression with the inline exploded result.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        :meth:`explode_outer`\n",
      "        :meth:`inline`\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Supports Spark Connect.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     Row(id=1, structlist=[Row(a=1, b=2), Row(a=3, b=4)]),\n",
      "        ...     Row(id=2, structlist=[])\n",
      "        ... ])\n",
      "        >>> df.select('id', inline_outer(df.structlist)).show()\n",
      "        +---+----+----+\n",
      "        | id|   a|   b|\n",
      "        +---+----+----+\n",
      "        |  1|   1|   2|\n",
      "        |  1|   3|   4|\n",
      "        |  2|NULL|NULL|\n",
      "        +---+----+----+\n",
      "    \n",
      "    input_file_block_length() -> pyspark.sql.column.Column\n",
      "        Returns the length of the block being read, or -1 if not available.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.read.text(\"python/test_support/sql/ages_newlines.csv\", lineSep=\",\")\n",
      "        >>> df.select(input_file_block_length().alias('r')).first()\n",
      "        Row(r=87)\n",
      "    \n",
      "    input_file_block_start() -> pyspark.sql.column.Column\n",
      "        Returns the start offset of the block being read, or -1 if not available.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.read.text(\"python/test_support/sql/ages_newlines.csv\", lineSep=\",\")\n",
      "        >>> df.select(input_file_block_start().alias('r')).first()\n",
      "        Row(r=0)\n",
      "    \n",
      "    input_file_name() -> pyspark.sql.column.Column\n",
      "        Creates a string column for the file name of the current Spark task.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            file names.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import os\n",
      "        >>> path = os.path.abspath(__file__)\n",
      "        >>> df = spark.read.text(path)\n",
      "        >>> df.select(input_file_name()).first()\n",
      "        Row(input_file_name()='file:///...')\n",
      "    \n",
      "    instr(str: 'ColumnOrName', substr: str) -> pyspark.sql.column.Column\n",
      "        Locate the position of the first occurrence of substr column in the given string.\n",
      "        Returns null if either of the arguments are null.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not zero based, but 1 based index. Returns 0 if substr\n",
      "        could not be found in str.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        substr : str\n",
      "            substring to look for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            location of the first occurrence of the substring as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(instr(df.s, 'b').alias('s')).collect()\n",
      "        [Row(s=2)]\n",
      "    \n",
      "    isnan(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        An expression that returns true if the column is NaN.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            True if value is NaN and False otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n",
      "        >>> df.select(\"a\", \"b\", isnan(\"a\").alias(\"r1\"), isnan(df.b).alias(\"r2\")).show()\n",
      "        +---+---+-----+-----+\n",
      "        |  a|  b|   r1|   r2|\n",
      "        +---+---+-----+-----+\n",
      "        |1.0|NaN|false| true|\n",
      "        |NaN|2.0| true|false|\n",
      "        +---+---+-----+-----+\n",
      "    \n",
      "    isnotnull(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns true if `col` is not null, or false otherwise.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(None,), (1,)], [\"e\"])\n",
      "        >>> df.select(isnotnull(df.e).alias('r')).collect()\n",
      "        [Row(r=False), Row(r=True)]\n",
      "    \n",
      "    isnull(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        An expression that returns true if the column is null.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            True if value is null and False otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, None), (None, 2)], (\"a\", \"b\"))\n",
      "        >>> df.select(\"a\", \"b\", isnull(\"a\").alias(\"r1\"), isnull(df.b).alias(\"r2\")).show()\n",
      "        +----+----+-----+-----+\n",
      "        |   a|   b|   r1|   r2|\n",
      "        +----+----+-----+-----+\n",
      "        |   1|NULL|false| true|\n",
      "        |NULL|   2| true|false|\n",
      "        +----+----+-----+-----+\n",
      "    \n",
      "    java_method(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Calls a method with reflection.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            the first element should be a literal string for the class name,\n",
      "            and the second element should be a literal string for the method name,\n",
      "            and the remaining are input arguments to the Java method.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(1).select(\n",
      "        ...     sf.java_method(\n",
      "        ...         sf.lit(\"java.util.UUID\"),\n",
      "        ...         sf.lit(\"fromString\"),\n",
      "        ...         sf.lit(\"a5cf6c42-0c85-418f-af6c-3e4e5b1328f2\")\n",
      "        ...     )\n",
      "        ... ).show(truncate=False)\n",
      "        +-----------------------------------------------------------------------------+\n",
      "        |java_method(java.util.UUID, fromString, a5cf6c42-0c85-418f-af6c-3e4e5b1328f2)|\n",
      "        +-----------------------------------------------------------------------------+\n",
      "        |a5cf6c42-0c85-418f-af6c-3e4e5b1328f2                                         |\n",
      "        +-----------------------------------------------------------------------------+\n",
      "    \n",
      "    json_array_length(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the number of elements in the outermost JSON array. `NULL` is returned in case of\n",
      "        any other valid JSON string, `NULL` or an invalid JSON.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col: :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            length of json array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(None,), ('[1, 2, 3]',), ('[]',)], ['data'])\n",
      "        >>> df.select(json_array_length(df.data).alias('r')).collect()\n",
      "        [Row(r=None), Row(r=3), Row(r=0)]\n",
      "    \n",
      "    json_object_keys(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns all the keys of the outermost JSON object as an array. If a valid JSON object is\n",
      "        given, all the keys of the outermost object will be returned as an array. If it is any\n",
      "        other valid JSON string, an invalid JSON string or an empty string, the function returns null.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col: :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            all the keys of the outermost JSON object.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(None,), ('{}',), ('{\"key1\":1, \"key2\":2}',)], ['data'])\n",
      "        >>> df.select(json_object_keys(df.data).alias('r')).collect()\n",
      "        [Row(r=None), Row(r=[]), Row(r=['key1', 'key2'])]\n",
      "    \n",
      "    json_tuple(col: 'ColumnOrName', *fields: str) -> pyspark.sql.column.Column\n",
      "        Creates a new row for a json column according to the given field names.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            string column in json format\n",
      "        fields : str\n",
      "            a field or fields to extract\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a new row for each given field value from json object\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n",
      "        >>> df.select(df.key, json_tuple(df.jstring, 'f1', 'f2')).collect()\n",
      "        [Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]\n",
      "    \n",
      "    kurtosis(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the kurtosis of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            kurtosis of given column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n",
      "        >>> df.select(kurtosis(df.c)).show()\n",
      "        +-----------+\n",
      "        |kurtosis(c)|\n",
      "        +-----------+\n",
      "        |       -1.5|\n",
      "        +-----------+\n",
      "    \n",
      "    lag(col: 'ColumnOrName', offset: int = 1, default: Optional[Any] = None) -> pyspark.sql.column.Column\n",
      "        Window function: returns the value that is `offset` rows before the current row, and\n",
      "        `default` if there is less than `offset` rows before the current row. For example,\n",
      "        an `offset` of one will return the previous row at any given point in the window partition.\n",
      "        \n",
      "        This is equivalent to the LAG function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        offset : int, optional default 1\n",
      "            number of row to extend\n",
      "        default : optional\n",
      "            default value\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value before current row based on `offset`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window\n",
      "        >>> df = spark.createDataFrame([(\"a\", 1),\n",
      "        ...                             (\"a\", 2),\n",
      "        ...                             (\"a\", 3),\n",
      "        ...                             (\"b\", 8),\n",
      "        ...                             (\"b\", 2)], [\"c1\", \"c2\"])\n",
      "        >>> df.show()\n",
      "        +---+---+\n",
      "        | c1| c2|\n",
      "        +---+---+\n",
      "        |  a|  1|\n",
      "        |  a|  2|\n",
      "        |  a|  3|\n",
      "        |  b|  8|\n",
      "        |  b|  2|\n",
      "        +---+---+\n",
      "        >>> w = Window.partitionBy(\"c1\").orderBy(\"c2\")\n",
      "        >>> df.withColumn(\"previos_value\", lag(\"c2\").over(w)).show()\n",
      "        +---+---+-------------+\n",
      "        | c1| c2|previos_value|\n",
      "        +---+---+-------------+\n",
      "        |  a|  1|         NULL|\n",
      "        |  a|  2|            1|\n",
      "        |  a|  3|            2|\n",
      "        |  b|  2|         NULL|\n",
      "        |  b|  8|            2|\n",
      "        +---+---+-------------+\n",
      "        >>> df.withColumn(\"previos_value\", lag(\"c2\", 1, 0).over(w)).show()\n",
      "        +---+---+-------------+\n",
      "        | c1| c2|previos_value|\n",
      "        +---+---+-------------+\n",
      "        |  a|  1|            0|\n",
      "        |  a|  2|            1|\n",
      "        |  a|  3|            2|\n",
      "        |  b|  2|            0|\n",
      "        |  b|  8|            2|\n",
      "        +---+---+-------------+\n",
      "        >>> df.withColumn(\"previos_value\", lag(\"c2\", 2, -1).over(w)).show()\n",
      "        +---+---+-------------+\n",
      "        | c1| c2|previos_value|\n",
      "        +---+---+-------------+\n",
      "        |  a|  1|           -1|\n",
      "        |  a|  2|           -1|\n",
      "        |  a|  3|            1|\n",
      "        |  b|  2|           -1|\n",
      "        |  b|  8|           -1|\n",
      "        +---+---+-------------+\n",
      "    \n",
      "    last(col: 'ColumnOrName', ignorenulls: bool = False) -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the last value in a group.\n",
      "        \n",
      "        The function by default returns the last values it sees. It will return the last non-null\n",
      "        value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic because its results depends on the order of the\n",
      "        rows which may be non-deterministic after a shuffle.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to fetch last value for.\n",
      "        ignorenulls : :class:`~pyspark.sql.Column` or str\n",
      "            if last value is null then look for non-null value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            last value of the group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5), (\"Alice\", None)], (\"name\", \"age\"))\n",
      "        >>> df = df.orderBy(df.age.desc())\n",
      "        >>> df.groupby(\"name\").agg(last(\"age\")).orderBy(\"name\").show()\n",
      "        +-----+---------+\n",
      "        | name|last(age)|\n",
      "        +-----+---------+\n",
      "        |Alice|     NULL|\n",
      "        |  Bob|        5|\n",
      "        +-----+---------+\n",
      "        \n",
      "        Now, to ignore any nulls we needs to set ``ignorenulls`` to `True`\n",
      "        \n",
      "        >>> df.groupby(\"name\").agg(last(\"age\", ignorenulls=True)).orderBy(\"name\").show()\n",
      "        +-----+---------+\n",
      "        | name|last(age)|\n",
      "        +-----+---------+\n",
      "        |Alice|        2|\n",
      "        |  Bob|        5|\n",
      "        +-----+---------+\n",
      "    \n",
      "    last_day(date: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the last day of the month which the given date belongs to.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        date : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            last day of the month.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-10',)], ['d'])\n",
      "        >>> df.select(last_day(df.d).alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "    \n",
      "    last_value(col: 'ColumnOrName', ignoreNulls: Union[bool, pyspark.sql.column.Column, NoneType] = None) -> pyspark.sql.column.Column\n",
      "        Returns the last value of `col` for a group of rows. It will return the last non-null\n",
      "        value it sees when `ignoreNulls` is set to true. If all values are null, then null is returned.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        ignorenulls : :class:`~pyspark.sql.Column` or bool\n",
      "            if first value is null then look for first non-null value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            some value of `col` for a group of rows.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(\"a\", 1), (\"a\", 2), (\"a\", 3), (\"b\", 8), (None, 2)], [\"a\", \"b\"]\n",
      "        ... ).select(sf.last_value('a'), sf.last_value('b')).show()\n",
      "        +-------------+-------------+\n",
      "        |last_value(a)|last_value(b)|\n",
      "        +-------------+-------------+\n",
      "        |         NULL|            2|\n",
      "        +-------------+-------------+\n",
      "        \n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(\"a\", 1), (\"a\", 2), (\"a\", 3), (\"b\", 8), (None, 2)], [\"a\", \"b\"]\n",
      "        ... ).select(sf.last_value('a', True), sf.last_value('b', True)).show()\n",
      "        +-------------+-------------+\n",
      "        |last_value(a)|last_value(b)|\n",
      "        +-------------+-------------+\n",
      "        |            b|            2|\n",
      "        +-------------+-------------+\n",
      "    \n",
      "    lcase(str: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns `str` with all characters changed to lowercase.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(1).select(sf.lcase(sf.lit(\"Spark\"))).show()\n",
      "        +------------+\n",
      "        |lcase(Spark)|\n",
      "        +------------+\n",
      "        |       spark|\n",
      "        +------------+\n",
      "    \n",
      "    lead(col: 'ColumnOrName', offset: int = 1, default: Optional[Any] = None) -> pyspark.sql.column.Column\n",
      "        Window function: returns the value that is `offset` rows after the current row, and\n",
      "        `default` if there is less than `offset` rows after the current row. For example,\n",
      "        an `offset` of one will return the next row at any given point in the window partition.\n",
      "        \n",
      "        This is equivalent to the LEAD function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        offset : int, optional default 1\n",
      "            number of row to extend\n",
      "        default : optional\n",
      "            default value\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value after current row based on `offset`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window\n",
      "        >>> df = spark.createDataFrame([(\"a\", 1),\n",
      "        ...                             (\"a\", 2),\n",
      "        ...                             (\"a\", 3),\n",
      "        ...                             (\"b\", 8),\n",
      "        ...                             (\"b\", 2)], [\"c1\", \"c2\"])\n",
      "        >>> df.show()\n",
      "        +---+---+\n",
      "        | c1| c2|\n",
      "        +---+---+\n",
      "        |  a|  1|\n",
      "        |  a|  2|\n",
      "        |  a|  3|\n",
      "        |  b|  8|\n",
      "        |  b|  2|\n",
      "        +---+---+\n",
      "        >>> w = Window.partitionBy(\"c1\").orderBy(\"c2\")\n",
      "        >>> df.withColumn(\"next_value\", lead(\"c2\").over(w)).show()\n",
      "        +---+---+----------+\n",
      "        | c1| c2|next_value|\n",
      "        +---+---+----------+\n",
      "        |  a|  1|         2|\n",
      "        |  a|  2|         3|\n",
      "        |  a|  3|      NULL|\n",
      "        |  b|  2|         8|\n",
      "        |  b|  8|      NULL|\n",
      "        +---+---+----------+\n",
      "        >>> df.withColumn(\"next_value\", lead(\"c2\", 1, 0).over(w)).show()\n",
      "        +---+---+----------+\n",
      "        | c1| c2|next_value|\n",
      "        +---+---+----------+\n",
      "        |  a|  1|         2|\n",
      "        |  a|  2|         3|\n",
      "        |  a|  3|         0|\n",
      "        |  b|  2|         8|\n",
      "        |  b|  8|         0|\n",
      "        +---+---+----------+\n",
      "        >>> df.withColumn(\"next_value\", lead(\"c2\", 2, -1).over(w)).show()\n",
      "        +---+---+----------+\n",
      "        | c1| c2|next_value|\n",
      "        +---+---+----------+\n",
      "        |  a|  1|         3|\n",
      "        |  a|  2|        -1|\n",
      "        |  a|  3|        -1|\n",
      "        |  b|  2|        -1|\n",
      "        |  b|  8|        -1|\n",
      "        +---+---+----------+\n",
      "    \n",
      "    least(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the least value of the list of column names, skipping null values.\n",
      "        This function takes at least 2 parameters. It will return null if all parameters are null.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or columns to be compared\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            least value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n",
      "        >>> df.select(least(df.a, df.b, df.c).alias(\"least\")).collect()\n",
      "        [Row(least=1)]\n",
      "    \n",
      "    left(str: 'ColumnOrName', len: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the leftmost `len`(`len` can be string type) characters from the string `str`,\n",
      "        if `len` is less or equal than 0 the result is an empty string.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        len : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings, the leftmost `len`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Spark SQL\", 3,)], ['a', 'b'])\n",
      "        >>> df.select(left(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r='Spa')]\n",
      "    \n",
      "    length(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the character length of string data or number of bytes of binary data.\n",
      "        The length of character data includes the trailing spaces. The length of binary data\n",
      "        includes binary zeros.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            length of the value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC ',)], ['a']).select(length('a').alias('length')).collect()\n",
      "        [Row(length=4)]\n",
      "    \n",
      "    levenshtein(left: 'ColumnOrName', right: 'ColumnOrName', threshold: Optional[int] = None) -> pyspark.sql.column.Column\n",
      "        Computes the Levenshtein distance of the two given strings.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        left : :class:`~pyspark.sql.Column` or str\n",
      "            first column value.\n",
      "        right : :class:`~pyspark.sql.Column` or str\n",
      "            second column value.\n",
      "        threshold : int, optional\n",
      "            if set when the levenshtein distance of the two given strings\n",
      "            less than or equal to a given threshold then return result distance, or -1\n",
      "        \n",
      "            .. versionchanged: 3.5.0\n",
      "                Added ``threshold`` argument.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            Levenshtein distance as integer value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])\n",
      "        >>> df0.select(levenshtein('l', 'r').alias('d')).collect()\n",
      "        [Row(d=3)]\n",
      "        >>> df0.select(levenshtein('l', 'r', 2).alias('d')).collect()\n",
      "        [Row(d=-1)]\n",
      "    \n",
      "    like(str: 'ColumnOrName', pattern: 'ColumnOrName', escapeChar: Optional[ForwardRef('Column')] = None) -> pyspark.sql.column.Column\n",
      "        Returns true if str matches `pattern` with `escape`,\n",
      "        null if any arguments are null, false otherwise.\n",
      "        The default escape character is the ''.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            A string.\n",
      "        pattern : :class:`~pyspark.sql.Column` or str\n",
      "            A string. The pattern is a string which is matched literally, with\n",
      "            exception to the following special symbols:\n",
      "            _ matches any one character in the input (similar to . in posix regular expressions)\n",
      "            % matches zero or more characters in the input (similar to .* in posix regular\n",
      "            expressions)\n",
      "            Since Spark 2.0, string literals are unescaped in our SQL parser. For example, in order\n",
      "            to match \"\u0007bc\", the pattern should be \"\\abc\".\n",
      "            When SQL config 'spark.sql.parser.escapedStringLiterals' is enabled, it falls back\n",
      "            to Spark 1.6 behavior regarding string literal parsing. For example, if the config is\n",
      "            enabled, the pattern to match \"\u0007bc\" should be \"\u0007bc\".\n",
      "        escape : :class:`~pyspark.sql.Column`\n",
      "            An character added since Spark 3.0. The default escape character is the ''.\n",
      "            If an escape character precedes a special symbol or another escape character, the\n",
      "            following character is matched literally. It is invalid to escape any other character.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Spark\", \"_park\")], ['a', 'b'])\n",
      "        >>> df.select(like(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r=True)]\n",
      "        \n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(\"%SystemDrive%/Users/John\", \"/%SystemDrive/%//Users%\")],\n",
      "        ...     ['a', 'b']\n",
      "        ... )\n",
      "        >>> df.select(like(df.a, df.b, lit('/')).alias('r')).collect()\n",
      "        [Row(r=True)]\n",
      "    \n",
      "    lit(col: Any) -> pyspark.sql.column.Column\n",
      "        Creates a :class:`~pyspark.sql.Column` of literal value.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column`, str, int, float, bool or list, NumPy literals or ndarray.\n",
      "            the value to make it as a PySpark literal. If a column is passed,\n",
      "            it returns the column as is.\n",
      "        \n",
      "            .. versionchanged:: 3.4.0\n",
      "                Since 3.4.0, it supports the list type.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the literal instance.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(lit(5).alias('height'), df.id).show()\n",
      "        +------+---+\n",
      "        |height| id|\n",
      "        +------+---+\n",
      "        |     5|  0|\n",
      "        +------+---+\n",
      "        \n",
      "        Create a literal from a list.\n",
      "        \n",
      "        >>> spark.range(1).select(lit([1, 2, 3])).show()\n",
      "        +--------------+\n",
      "        |array(1, 2, 3)|\n",
      "        +--------------+\n",
      "        |     [1, 2, 3]|\n",
      "        +--------------+\n",
      "    \n",
      "    ln(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the natural logarithm of the argument.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            a column to calculate logariphm for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            natural logarithm of given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(4,)], ['a'])\n",
      "        >>> df.select(ln('a')).show()\n",
      "        +------------------+\n",
      "        |             ln(a)|\n",
      "        +------------------+\n",
      "        |1.3862943611198906|\n",
      "        +------------------+\n",
      "    \n",
      "    localtimestamp() -> pyspark.sql.column.Column\n",
      "        Returns the current timestamp without time zone at the start of query evaluation\n",
      "        as a timestamp without time zone column. All calls of localtimestamp within the\n",
      "        same query return the same value.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            current local date and time.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(localtimestamp()).show(truncate=False) # doctest: +SKIP\n",
      "        +-----------------------+\n",
      "        |localtimestamp()       |\n",
      "        +-----------------------+\n",
      "        |2022-08-26 21:28:34.639|\n",
      "        +-----------------------+\n",
      "    \n",
      "    locate(substr: str, str: 'ColumnOrName', pos: int = 1) -> pyspark.sql.column.Column\n",
      "        Locate the position of the first occurrence of substr in a string column, after position pos.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        substr : str\n",
      "            a string\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            a Column of :class:`pyspark.sql.types.StringType`\n",
      "        pos : int, optional\n",
      "            start position (zero based)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            position of the substring.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not zero based, but 1 based index. Returns 0 if substr\n",
      "        could not be found in str.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(locate('b', df.s, 1).alias('s')).collect()\n",
      "        [Row(s=2)]\n",
      "    \n",
      "    log(arg1: Union[ForwardRef('ColumnOrName'), float], arg2: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Returns the first argument-based logarithm of the second argument.\n",
      "        \n",
      "        If there is only one argument, then this takes the natural logarithm of the argument.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        arg1 : :class:`~pyspark.sql.Column`, str or float\n",
      "            base number or actual number (in this case base is `e`)\n",
      "        arg2 : :class:`~pyspark.sql.Column`, str or float\n",
      "            number to calculate logariphm for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            logariphm of given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import functions as sf\n",
      "        >>> df = spark.sql(\"SELECT * FROM VALUES (1), (2), (4) AS t(value)\")\n",
      "        >>> df.select(sf.log(2.0, df.value).alias('log2_value')).show()\n",
      "        +----------+\n",
      "        |log2_value|\n",
      "        +----------+\n",
      "        |       0.0|\n",
      "        |       1.0|\n",
      "        |       2.0|\n",
      "        +----------+\n",
      "        \n",
      "        And Natural logarithm\n",
      "        \n",
      "        >>> df.select(sf.log(df.value).alias('ln_value')).show()\n",
      "        +------------------+\n",
      "        |          ln_value|\n",
      "        +------------------+\n",
      "        |               0.0|\n",
      "        |0.6931471805599453|\n",
      "        |1.3862943611198906|\n",
      "        +------------------+\n",
      "    \n",
      "    log10(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the logarithm of the given value in Base 10.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to calculate logarithm for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            logarithm of the given value in Base 10.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(log10(lit(100))).show()\n",
      "        +----------+\n",
      "        |LOG10(100)|\n",
      "        +----------+\n",
      "        |       2.0|\n",
      "        +----------+\n",
      "    \n",
      "    log1p(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the natural logarithm of the \"given value plus one\".\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to calculate natural logarithm for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            natural logarithm of the \"given value plus one\".\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import math\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(log1p(lit(math.e))).first()\n",
      "        Row(LOG1P(2.71828...)=1.31326...)\n",
      "        \n",
      "        Same as:\n",
      "        \n",
      "        >>> df.select(log(lit(math.e+1))).first()\n",
      "        Row(ln(3.71828...)=1.31326...)\n",
      "    \n",
      "    log2(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the base-2 logarithm of the argument.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            a column to calculate logariphm for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            logariphm of given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(4,)], ['a'])\n",
      "        >>> df.select(log2('a').alias('log2')).show()\n",
      "        +----+\n",
      "        |log2|\n",
      "        +----+\n",
      "        | 2.0|\n",
      "        +----+\n",
      "    \n",
      "    lower(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Converts a string expression to lower case.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            lower case values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\"Spark\", \"PySpark\", \"Pandas API\"], \"STRING\")\n",
      "        >>> df.select(lower(\"value\")).show()\n",
      "        +------------+\n",
      "        |lower(value)|\n",
      "        +------------+\n",
      "        |       spark|\n",
      "        |     pyspark|\n",
      "        |  pandas api|\n",
      "        +------------+\n",
      "    \n",
      "    lpad(col: 'ColumnOrName', len: int, pad: str) -> pyspark.sql.column.Column\n",
      "        Left-pad the string column to width `len` with `pad`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        len : int\n",
      "            length of the final string.\n",
      "        pad : str\n",
      "            chars to prepend.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            left padded result.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(lpad(df.s, 6, '#').alias('s')).collect()\n",
      "        [Row(s='##abcd')]\n",
      "    \n",
      "    ltrim(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Trim the spaces from left end for the specified string value.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            left trimmed values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\"   Spark\", \"Spark  \", \" Spark\"], \"STRING\")\n",
      "        >>> df.select(ltrim(\"value\").alias(\"r\")).withColumn(\"length\", length(\"r\")).show()\n",
      "        +-------+------+\n",
      "        |      r|length|\n",
      "        +-------+------+\n",
      "        |  Spark|     5|\n",
      "        |Spark  |     7|\n",
      "        |  Spark|     5|\n",
      "        +-------+------+\n",
      "    \n",
      "    make_date(year: 'ColumnOrName', month: 'ColumnOrName', day: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a column with a date built from the year, month and day columns.\n",
      "        \n",
      "        .. versionadded:: 3.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        year : :class:`~pyspark.sql.Column` or str\n",
      "            The year to build the date\n",
      "        month : :class:`~pyspark.sql.Column` or str\n",
      "            The month to build the date\n",
      "        day : :class:`~pyspark.sql.Column` or str\n",
      "            The day to build the date\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a date built from given parts.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(2020, 6, 26)], ['Y', 'M', 'D'])\n",
      "        >>> df.select(make_date(df.Y, df.M, df.D).alias(\"datefield\")).collect()\n",
      "        [Row(datefield=datetime.date(2020, 6, 26))]\n",
      "    \n",
      "    make_dt_interval(days: Optional[ForwardRef('ColumnOrName')] = None, hours: Optional[ForwardRef('ColumnOrName')] = None, mins: Optional[ForwardRef('ColumnOrName')] = None, secs: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Make DayTimeIntervalType duration from days, hours, mins and secs.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        days : :class:`~pyspark.sql.Column` or str\n",
      "            the number of days, positive or negative\n",
      "        hours : :class:`~pyspark.sql.Column` or str\n",
      "            the number of hours, positive or negative\n",
      "        mins : :class:`~pyspark.sql.Column` or str\n",
      "            the number of minutes, positive or negative\n",
      "        secs : :class:`~pyspark.sql.Column` or str\n",
      "            the number of seconds with the fractional part in microsecond precision.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[1, 12, 30, 01.001001]],\n",
      "        ...     [\"day\", \"hour\", \"min\", \"sec\"])\n",
      "        >>> df.select(make_dt_interval(\n",
      "        ...     df.day, df.hour, df.min, df.sec).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +------------------------------------------+\n",
      "        |r                                         |\n",
      "        +------------------------------------------+\n",
      "        |INTERVAL '1 12:30:01.001001' DAY TO SECOND|\n",
      "        +------------------------------------------+\n",
      "        \n",
      "        >>> df.select(make_dt_interval(\n",
      "        ...     df.day, df.hour, df.min).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +-----------------------------------+\n",
      "        |r                                  |\n",
      "        +-----------------------------------+\n",
      "        |INTERVAL '1 12:30:00' DAY TO SECOND|\n",
      "        +-----------------------------------+\n",
      "        \n",
      "        >>> df.select(make_dt_interval(\n",
      "        ...     df.day, df.hour).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +-----------------------------------+\n",
      "        |r                                  |\n",
      "        +-----------------------------------+\n",
      "        |INTERVAL '1 12:00:00' DAY TO SECOND|\n",
      "        +-----------------------------------+\n",
      "        \n",
      "        >>> df.select(make_dt_interval(df.day).alias('r')).show(truncate=False)\n",
      "        +-----------------------------------+\n",
      "        |r                                  |\n",
      "        +-----------------------------------+\n",
      "        |INTERVAL '1 00:00:00' DAY TO SECOND|\n",
      "        +-----------------------------------+\n",
      "        \n",
      "        >>> df.select(make_dt_interval().alias('r')).show(truncate=False)\n",
      "        +-----------------------------------+\n",
      "        |r                                  |\n",
      "        +-----------------------------------+\n",
      "        |INTERVAL '0 00:00:00' DAY TO SECOND|\n",
      "        +-----------------------------------+\n",
      "    \n",
      "    make_interval(years: Optional[ForwardRef('ColumnOrName')] = None, months: Optional[ForwardRef('ColumnOrName')] = None, weeks: Optional[ForwardRef('ColumnOrName')] = None, days: Optional[ForwardRef('ColumnOrName')] = None, hours: Optional[ForwardRef('ColumnOrName')] = None, mins: Optional[ForwardRef('ColumnOrName')] = None, secs: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Make interval from years, months, weeks, days, hours, mins and secs.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        years : :class:`~pyspark.sql.Column` or str\n",
      "            the number of years, positive or negative\n",
      "        months : :class:`~pyspark.sql.Column` or str\n",
      "            the number of months, positive or negative\n",
      "        weeks : :class:`~pyspark.sql.Column` or str\n",
      "            the number of weeks, positive or negative\n",
      "        days : :class:`~pyspark.sql.Column` or str\n",
      "            the number of days, positive or negative\n",
      "        hours : :class:`~pyspark.sql.Column` or str\n",
      "            the number of hours, positive or negative\n",
      "        mins : :class:`~pyspark.sql.Column` or str\n",
      "            the number of minutes, positive or negative\n",
      "        secs : :class:`~pyspark.sql.Column` or str\n",
      "            the number of seconds with the fractional part in microsecond precision.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[100, 11, 1, 1, 12, 30, 01.001001]],\n",
      "        ...     [\"year\", \"month\", \"week\", \"day\", \"hour\", \"min\", \"sec\"])\n",
      "        >>> df.select(make_interval(\n",
      "        ...     df.year, df.month, df.week, df.day, df.hour, df.min, df.sec).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +---------------------------------------------------------------+\n",
      "        |r                                                              |\n",
      "        +---------------------------------------------------------------+\n",
      "        |100 years 11 months 8 days 12 hours 30 minutes 1.001001 seconds|\n",
      "        +---------------------------------------------------------------+\n",
      "        \n",
      "        >>> df.select(make_interval(\n",
      "        ...     df.year, df.month, df.week, df.day, df.hour, df.min).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +----------------------------------------------+\n",
      "        |r                                             |\n",
      "        +----------------------------------------------+\n",
      "        |100 years 11 months 8 days 12 hours 30 minutes|\n",
      "        +----------------------------------------------+\n",
      "        \n",
      "        >>> df.select(make_interval(\n",
      "        ...     df.year, df.month, df.week, df.day, df.hour).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +-----------------------------------+\n",
      "        |r                                  |\n",
      "        +-----------------------------------+\n",
      "        |100 years 11 months 8 days 12 hours|\n",
      "        +-----------------------------------+\n",
      "        \n",
      "        >>> df.select(make_interval(\n",
      "        ...     df.year, df.month, df.week, df.day).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +--------------------------+\n",
      "        |r                         |\n",
      "        +--------------------------+\n",
      "        |100 years 11 months 8 days|\n",
      "        +--------------------------+\n",
      "        \n",
      "        >>> df.select(make_interval(\n",
      "        ...     df.year, df.month, df.week).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +--------------------------+\n",
      "        |r                         |\n",
      "        +--------------------------+\n",
      "        |100 years 11 months 7 days|\n",
      "        +--------------------------+\n",
      "        \n",
      "        >>> df.select(make_interval(df.year, df.month).alias('r')).show(truncate=False)\n",
      "        +-------------------+\n",
      "        |r                  |\n",
      "        +-------------------+\n",
      "        |100 years 11 months|\n",
      "        +-------------------+\n",
      "        \n",
      "        >>> df.select(make_interval(df.year).alias('r')).show(truncate=False)\n",
      "        +---------+\n",
      "        |r        |\n",
      "        +---------+\n",
      "        |100 years|\n",
      "        +---------+\n",
      "    \n",
      "    make_timestamp(years: 'ColumnOrName', months: 'ColumnOrName', days: 'ColumnOrName', hours: 'ColumnOrName', mins: 'ColumnOrName', secs: 'ColumnOrName', timezone: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Create timestamp from years, months, days, hours, mins, secs and timezone fields.\n",
      "        The result data type is consistent with the value of configuration `spark.sql.timestampType`.\n",
      "        If the configuration `spark.sql.ansi.enabled` is false, the function returns NULL\n",
      "        on invalid inputs. Otherwise, it will throw an error instead.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        years : :class:`~pyspark.sql.Column` or str\n",
      "            the year to represent, from 1 to 9999\n",
      "        months : :class:`~pyspark.sql.Column` or str\n",
      "            the month-of-year to represent, from 1 (January) to 12 (December)\n",
      "        days : :class:`~pyspark.sql.Column` or str\n",
      "            the day-of-month to represent, from 1 to 31\n",
      "        hours : :class:`~pyspark.sql.Column` or str\n",
      "            the hour-of-day to represent, from 0 to 23\n",
      "        mins : :class:`~pyspark.sql.Column` or str\n",
      "            the minute-of-hour to represent, from 0 to 59\n",
      "        secs : :class:`~pyspark.sql.Column` or str\n",
      "            the second-of-minute and its micro-fraction to represent, from 0 to 60.\n",
      "            The value can be either an integer like 13 , or a fraction like 13.123.\n",
      "            If the sec argument equals to 60, the seconds field is set\n",
      "            to 0 and 1 minute is added to the final timestamp.\n",
      "        timezone : :class:`~pyspark.sql.Column` or str\n",
      "            the time zone identifier. For example, CET, UTC and etc.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> df = spark.createDataFrame([[2014, 12, 28, 6, 30, 45.887, 'CET']],\n",
      "        ...     [\"year\", \"month\", \"day\", \"hour\", \"min\", \"sec\", \"timezone\"])\n",
      "        >>> df.select(make_timestamp(\n",
      "        ...     df.year, df.month, df.day, df.hour, df.min, df.sec, df.timezone).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +-----------------------+\n",
      "        |r                      |\n",
      "        +-----------------------+\n",
      "        |2014-12-27 21:30:45.887|\n",
      "        +-----------------------+\n",
      "        \n",
      "        >>> df.select(make_timestamp(\n",
      "        ...     df.year, df.month, df.day, df.hour, df.min, df.sec).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +-----------------------+\n",
      "        |r                      |\n",
      "        +-----------------------+\n",
      "        |2014-12-28 06:30:45.887|\n",
      "        +-----------------------+\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    make_timestamp_ltz(years: 'ColumnOrName', months: 'ColumnOrName', days: 'ColumnOrName', hours: 'ColumnOrName', mins: 'ColumnOrName', secs: 'ColumnOrName', timezone: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Create the current timestamp with local time zone from years, months, days, hours, mins,\n",
      "        secs and timezone fields. If the configuration `spark.sql.ansi.enabled` is false,\n",
      "        the function returns NULL on invalid inputs. Otherwise, it will throw an error instead.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        years : :class:`~pyspark.sql.Column` or str\n",
      "            the year to represent, from 1 to 9999\n",
      "        months : :class:`~pyspark.sql.Column` or str\n",
      "            the month-of-year to represent, from 1 (January) to 12 (December)\n",
      "        days : :class:`~pyspark.sql.Column` or str\n",
      "            the day-of-month to represent, from 1 to 31\n",
      "        hours : :class:`~pyspark.sql.Column` or str\n",
      "            the hour-of-day to represent, from 0 to 23\n",
      "        mins : :class:`~pyspark.sql.Column` or str\n",
      "            the minute-of-hour to represent, from 0 to 59\n",
      "        secs : :class:`~pyspark.sql.Column` or str\n",
      "            the second-of-minute and its micro-fraction to represent, from 0 to 60.\n",
      "            The value can be either an integer like 13 , or a fraction like 13.123.\n",
      "            If the sec argument equals to 60, the seconds field is set\n",
      "            to 0 and 1 minute is added to the final timestamp.\n",
      "        timezone : :class:`~pyspark.sql.Column` or str\n",
      "            the time zone identifier. For example, CET, UTC and etc.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> df = spark.createDataFrame([[2014, 12, 28, 6, 30, 45.887, 'CET']],\n",
      "        ...     [\"year\", \"month\", \"day\", \"hour\", \"min\", \"sec\", \"timezone\"])\n",
      "        >>> df.select(sf.make_timestamp_ltz(\n",
      "        ...     df.year, df.month, df.day, df.hour, df.min, df.sec, df.timezone)\n",
      "        ... ).show(truncate=False)\n",
      "        +--------------------------------------------------------------+\n",
      "        |make_timestamp_ltz(year, month, day, hour, min, sec, timezone)|\n",
      "        +--------------------------------------------------------------+\n",
      "        |2014-12-27 21:30:45.887                                       |\n",
      "        +--------------------------------------------------------------+\n",
      "        \n",
      "        >>> df.select(sf.make_timestamp_ltz(\n",
      "        ...     df.year, df.month, df.day, df.hour, df.min, df.sec)\n",
      "        ... ).show(truncate=False)\n",
      "        +----------------------------------------------------+\n",
      "        |make_timestamp_ltz(year, month, day, hour, min, sec)|\n",
      "        +----------------------------------------------------+\n",
      "        |2014-12-28 06:30:45.887                             |\n",
      "        +----------------------------------------------------+\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    make_timestamp_ntz(years: 'ColumnOrName', months: 'ColumnOrName', days: 'ColumnOrName', hours: 'ColumnOrName', mins: 'ColumnOrName', secs: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Create local date-time from years, months, days, hours, mins, secs fields.\n",
      "        If the configuration `spark.sql.ansi.enabled` is false, the function returns NULL\n",
      "        on invalid inputs. Otherwise, it will throw an error instead.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        years : :class:`~pyspark.sql.Column` or str\n",
      "            the year to represent, from 1 to 9999\n",
      "        months : :class:`~pyspark.sql.Column` or str\n",
      "            the month-of-year to represent, from 1 (January) to 12 (December)\n",
      "        days : :class:`~pyspark.sql.Column` or str\n",
      "            the day-of-month to represent, from 1 to 31\n",
      "        hours : :class:`~pyspark.sql.Column` or str\n",
      "            the hour-of-day to represent, from 0 to 23\n",
      "        mins : :class:`~pyspark.sql.Column` or str\n",
      "            the minute-of-hour to represent, from 0 to 59\n",
      "        secs : :class:`~pyspark.sql.Column` or str\n",
      "            the second-of-minute and its micro-fraction to represent, from 0 to 60.\n",
      "            The value can be either an integer like 13 , or a fraction like 13.123.\n",
      "            If the sec argument equals to 60, the seconds field is set\n",
      "            to 0 and 1 minute is added to the final timestamp.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> df = spark.createDataFrame([[2014, 12, 28, 6, 30, 45.887]],\n",
      "        ...     [\"year\", \"month\", \"day\", \"hour\", \"min\", \"sec\"])\n",
      "        >>> df.select(sf.make_timestamp_ntz(\n",
      "        ...     df.year, df.month, df.day, df.hour, df.min, df.sec)\n",
      "        ... ).show(truncate=False)\n",
      "        +----------------------------------------------------+\n",
      "        |make_timestamp_ntz(year, month, day, hour, min, sec)|\n",
      "        +----------------------------------------------------+\n",
      "        |2014-12-28 06:30:45.887                             |\n",
      "        +----------------------------------------------------+\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    make_ym_interval(years: Optional[ForwardRef('ColumnOrName')] = None, months: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Make year-month interval from years, months.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        years : :class:`~pyspark.sql.Column` or str\n",
      "            the number of years, positive or negative\n",
      "        months : :class:`~pyspark.sql.Column` or str\n",
      "            the number of months, positive or negative\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> df = spark.createDataFrame([[2014, 12]], [\"year\", \"month\"])\n",
      "        >>> df.select(make_ym_interval(df.year, df.month).alias('r')).show(truncate=False)\n",
      "        +-------------------------------+\n",
      "        |r                              |\n",
      "        +-------------------------------+\n",
      "        |INTERVAL '2015-0' YEAR TO MONTH|\n",
      "        +-------------------------------+\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    map_concat(*cols: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName_')], Tuple[ForwardRef('ColumnOrName_'), ...]]) -> pyspark.sql.column.Column\n",
      "        Returns the union of all the given maps.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a map of merged entries from other maps.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_concat\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as map1, map(3, 'c') as map2\")\n",
      "        >>> df.select(map_concat(\"map1\", \"map2\").alias(\"map3\")).show(truncate=False)\n",
      "        +------------------------+\n",
      "        |map3                    |\n",
      "        +------------------------+\n",
      "        |{1 -> a, 2 -> b, 3 -> c}|\n",
      "        +------------------------+\n",
      "    \n",
      "    map_contains_key(col: 'ColumnOrName', value: Any) -> pyspark.sql.column.Column\n",
      "        Returns true if the map contains the key.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        value :\n",
      "            a literal value\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            True if key is in the map and False otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_contains_key\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
      "        >>> df.select(map_contains_key(\"data\", 1)).show()\n",
      "        +---------------------------------+\n",
      "        |array_contains(map_keys(data), 1)|\n",
      "        +---------------------------------+\n",
      "        |                             true|\n",
      "        +---------------------------------+\n",
      "        >>> df.select(map_contains_key(\"data\", -1)).show()\n",
      "        +----------------------------------+\n",
      "        |array_contains(map_keys(data), -1)|\n",
      "        +----------------------------------+\n",
      "        |                             false|\n",
      "        +----------------------------------+\n",
      "    \n",
      "    map_entries(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: Returns an unordered array of all entries in the given map.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of key value pairs as a struct type\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_entries\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
      "        >>> df = df.select(map_entries(\"data\").alias(\"entries\"))\n",
      "        >>> df.show()\n",
      "        +----------------+\n",
      "        |         entries|\n",
      "        +----------------+\n",
      "        |[{1, a}, {2, b}]|\n",
      "        +----------------+\n",
      "        >>> df.printSchema()\n",
      "        root\n",
      "         |-- entries: array (nullable = false)\n",
      "         |    |-- element: struct (containsNull = false)\n",
      "         |    |    |-- key: integer (nullable = false)\n",
      "         |    |    |-- value: string (nullable = false)\n",
      "    \n",
      "    map_filter(col: 'ColumnOrName', f: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]) -> pyspark.sql.column.Column\n",
      "        Returns a map whose key-value pairs satisfy a predicate.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            a binary function ``(k: Column, v: Column) -> Column...``\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            filtered map.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, {\"foo\": 42.0, \"bar\": 1.0, \"baz\": 32.0})], (\"id\", \"data\"))\n",
      "        >>> row = df.select(map_filter(\n",
      "        ...     \"data\", lambda _, v: v > 30.0).alias(\"data_filtered\")\n",
      "        ... ).head()\n",
      "        >>> sorted(row[\"data_filtered\"].items())\n",
      "        [('baz', 32.0), ('foo', 42.0)]\n",
      "    \n",
      "    map_from_arrays(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Creates a new map from two arrays.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing a set of keys. All elements should not be null\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing a set of values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a column of map type.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 5], ['a', 'b'])], ['k', 'v'])\n",
      "        >>> df = df.select(map_from_arrays(df.k, df.v).alias(\"col\"))\n",
      "        >>> df.show()\n",
      "        +----------------+\n",
      "        |             col|\n",
      "        +----------------+\n",
      "        |{2 -> a, 5 -> b}|\n",
      "        +----------------+\n",
      "        >>> df.printSchema()\n",
      "        root\n",
      "         |-- col: map (nullable = true)\n",
      "         |    |-- key: long\n",
      "         |    |-- value: string (valueContainsNull = true)\n",
      "    \n",
      "    map_from_entries(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: Converts an array of entries (key value struct types) to a map\n",
      "        of values.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a map created from the given array of entries.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_from_entries\n",
      "        >>> df = spark.sql(\"SELECT array(struct(1, 'a'), struct(2, 'b')) as data\")\n",
      "        >>> df.select(map_from_entries(\"data\").alias(\"map\")).show()\n",
      "        +----------------+\n",
      "        |             map|\n",
      "        +----------------+\n",
      "        |{1 -> a, 2 -> b}|\n",
      "        +----------------+\n",
      "    \n",
      "    map_keys(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: Returns an unordered array containing the keys of the map.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            keys of the map as an array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_keys\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
      "        >>> df.select(map_keys(\"data\").alias(\"keys\")).show()\n",
      "        +------+\n",
      "        |  keys|\n",
      "        +------+\n",
      "        |[1, 2]|\n",
      "        +------+\n",
      "    \n",
      "    map_values(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: Returns an unordered array containing the values of the map.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            values of the map as an array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_values\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
      "        >>> df.select(map_values(\"data\").alias(\"values\")).show()\n",
      "        +------+\n",
      "        |values|\n",
      "        +------+\n",
      "        |[a, b]|\n",
      "        +------+\n",
      "    \n",
      "    map_zip_with(col1: 'ColumnOrName', col2: 'ColumnOrName', f: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]) -> pyspark.sql.column.Column\n",
      "        Merge two given maps, key-wise into a single map using a function.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            name of the first column or expression\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            name of the second column or expression\n",
      "        f : function\n",
      "            a ternary function ``(k: Column, v1: Column, v2: Column) -> Column...``\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            zipped map where entries are calculated by applying given function to each\n",
      "            pair of arguments.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     (1, {\"IT\": 24.0, \"SALES\": 12.00}, {\"IT\": 2.0, \"SALES\": 1.4})],\n",
      "        ...     (\"id\", \"base\", \"ratio\")\n",
      "        ... )\n",
      "        >>> row = df.select(map_zip_with(\n",
      "        ...     \"base\", \"ratio\", lambda k, v1, v2: round(v1 * v2, 2)).alias(\"updated_data\")\n",
      "        ... ).head()\n",
      "        >>> sorted(row[\"updated_data\"].items())\n",
      "        [('IT', 48.0), ('SALES', 16.8)]\n",
      "    \n",
      "    mask(col: 'ColumnOrName', upperChar: Optional[ForwardRef('ColumnOrName')] = None, lowerChar: Optional[ForwardRef('ColumnOrName')] = None, digitChar: Optional[ForwardRef('ColumnOrName')] = None, otherChar: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Masks the given string value. This can be useful for creating copies of tables with sensitive\n",
      "        information removed.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col: :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        upperChar: :class:`~pyspark.sql.Column` or str\n",
      "            character to replace upper-case characters with. Specify NULL to retain original character.\n",
      "        lowerChar: :class:`~pyspark.sql.Column` or str\n",
      "            character to replace lower-case characters with. Specify NULL to retain original character.\n",
      "        digitChar: :class:`~pyspark.sql.Column` or str\n",
      "            character to replace digit characters with. Specify NULL to retain original character.\n",
      "        otherChar: :class:`~pyspark.sql.Column` or str\n",
      "            character to replace all other characters with. Specify NULL to retain original character.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"AbCD123-@$#\",), (\"abcd-EFGH-8765-4321\",)], ['data'])\n",
      "        >>> df.select(mask(df.data).alias('r')).collect()\n",
      "        [Row(r='XxXXnnn-@$#'), Row(r='xxxx-XXXX-nnnn-nnnn')]\n",
      "        >>> df.select(mask(df.data, lit('Y')).alias('r')).collect()\n",
      "        [Row(r='YxYYnnn-@$#'), Row(r='xxxx-YYYY-nnnn-nnnn')]\n",
      "        >>> df.select(mask(df.data, lit('Y'), lit('y')).alias('r')).collect()\n",
      "        [Row(r='YyYYnnn-@$#'), Row(r='yyyy-YYYY-nnnn-nnnn')]\n",
      "        >>> df.select(mask(df.data, lit('Y'), lit('y'), lit('d')).alias('r')).collect()\n",
      "        [Row(r='YyYYddd-@$#'), Row(r='yyyy-YYYY-dddd-dddd')]\n",
      "        >>> df.select(mask(df.data, lit('Y'), lit('y'), lit('d'), lit('*')).alias('r')).collect()\n",
      "        [Row(r='YyYYddd****'), Row(r='yyyy*YYYY*dddd*dddd')]\n",
      "    \n",
      "    max(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the maximum value of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(10)\n",
      "        >>> df.select(max(col(\"id\"))).show()\n",
      "        +-------+\n",
      "        |max(id)|\n",
      "        +-------+\n",
      "        |      9|\n",
      "        +-------+\n",
      "    \n",
      "    max_by(col: 'ColumnOrName', ord: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the value associated with the maximum value of ord.\n",
      "        \n",
      "        .. versionadded:: 3.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        ord : :class:`~pyspark.sql.Column` or str\n",
      "            column to be maximized\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value associated with the maximum value of ord.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     (\"Java\", 2012, 20000), (\"dotNET\", 2012, 5000),\n",
      "        ...     (\"dotNET\", 2013, 48000), (\"Java\", 2013, 30000)],\n",
      "        ...     schema=(\"course\", \"year\", \"earnings\"))\n",
      "        >>> df.groupby(\"course\").agg(max_by(\"year\", \"earnings\")).show()\n",
      "        +------+----------------------+\n",
      "        |course|max_by(year, earnings)|\n",
      "        +------+----------------------+\n",
      "        |  Java|                  2013|\n",
      "        |dotNET|                  2013|\n",
      "        +------+----------------------+\n",
      "    \n",
      "    md5(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Calculates the MD5 digest and returns the value as a 32 character hex string.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(md5('a').alias('hash')).collect()\n",
      "        [Row(hash='902fbdd2b1df0c4f70b4a5d23525e932')]\n",
      "    \n",
      "    mean(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the average of the values in a group.\n",
      "        An alias of :func:`avg`.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(10)\n",
      "        >>> df.select(mean(df.id)).show()\n",
      "        +-------+\n",
      "        |avg(id)|\n",
      "        +-------+\n",
      "        |    4.5|\n",
      "        +-------+\n",
      "    \n",
      "    median(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the median of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the median of the values in a group.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Supports Spark Connect.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     (\"Java\", 2012, 20000), (\"dotNET\", 2012, 5000),\n",
      "        ...     (\"Java\", 2012, 22000), (\"dotNET\", 2012, 10000),\n",
      "        ...     (\"dotNET\", 2013, 48000), (\"Java\", 2013, 30000)],\n",
      "        ...     schema=(\"course\", \"year\", \"earnings\"))\n",
      "        >>> df.groupby(\"course\").agg(median(\"earnings\")).show()\n",
      "        +------+----------------+\n",
      "        |course|median(earnings)|\n",
      "        +------+----------------+\n",
      "        |  Java|         22000.0|\n",
      "        |dotNET|         10000.0|\n",
      "        +------+----------------+\n",
      "    \n",
      "    min(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the minimum value of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(10)\n",
      "        >>> df.select(min(df.id)).show()\n",
      "        +-------+\n",
      "        |min(id)|\n",
      "        +-------+\n",
      "        |      0|\n",
      "        +-------+\n",
      "    \n",
      "    min_by(col: 'ColumnOrName', ord: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the value associated with the minimum value of ord.\n",
      "        \n",
      "        .. versionadded:: 3.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        ord : :class:`~pyspark.sql.Column` or str\n",
      "            column to be minimized\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value associated with the minimum value of ord.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     (\"Java\", 2012, 20000), (\"dotNET\", 2012, 5000),\n",
      "        ...     (\"dotNET\", 2013, 48000), (\"Java\", 2013, 30000)],\n",
      "        ...     schema=(\"course\", \"year\", \"earnings\"))\n",
      "        >>> df.groupby(\"course\").agg(min_by(\"year\", \"earnings\")).show()\n",
      "        +------+----------------------+\n",
      "        |course|min_by(year, earnings)|\n",
      "        +------+----------------------+\n",
      "        |  Java|                  2012|\n",
      "        |dotNET|                  2012|\n",
      "        +------+----------------------+\n",
      "    \n",
      "    minute(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the minutes of a given timestamp as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            minutes part of the timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import datetime\n",
      "        >>> df = spark.createDataFrame([(datetime.datetime(2015, 4, 8, 13, 8, 15),)], ['ts'])\n",
      "        >>> df.select(minute('ts').alias('minute')).collect()\n",
      "        [Row(minute=8)]\n",
      "    \n",
      "    mode(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the most frequent value in a group.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the most frequent value in a group.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Supports Spark Connect.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     (\"Java\", 2012, 20000), (\"dotNET\", 2012, 5000),\n",
      "        ...     (\"Java\", 2012, 20000), (\"dotNET\", 2012, 5000),\n",
      "        ...     (\"dotNET\", 2013, 48000), (\"Java\", 2013, 30000)],\n",
      "        ...     schema=(\"course\", \"year\", \"earnings\"))\n",
      "        >>> df.groupby(\"course\").agg(mode(\"year\")).show()\n",
      "        +------+----------+\n",
      "        |course|mode(year)|\n",
      "        +------+----------+\n",
      "        |  Java|      2012|\n",
      "        |dotNET|      2012|\n",
      "        +------+----------+\n",
      "    \n",
      "    monotonically_increasing_id() -> pyspark.sql.column.Column\n",
      "        A column that generates monotonically increasing 64-bit integers.\n",
      "        \n",
      "        The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.\n",
      "        The current implementation puts the partition ID in the upper 31 bits, and the record number\n",
      "        within each partition in the lower 33 bits. The assumption is that the data frame has\n",
      "        less than 1 billion partitions, and each partition has less than 8 billion records.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic because its result depends on partition IDs.\n",
      "        \n",
      "        As an example, consider a :class:`DataFrame` with two partitions, each with 3 records.\n",
      "        This expression would return the following IDs:\n",
      "        0, 1, 2, 8589934592 (1L << 33), 8589934593, 8589934594.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            last value of the group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import functions as sf\n",
      "        >>> spark.range(0, 10, 1, 2).select(sf.monotonically_increasing_id()).show()\n",
      "        +-----------------------------+\n",
      "        |monotonically_increasing_id()|\n",
      "        +-----------------------------+\n",
      "        |                            0|\n",
      "        |                            1|\n",
      "        |                            2|\n",
      "        |                            3|\n",
      "        |                            4|\n",
      "        |                   8589934592|\n",
      "        |                   8589934593|\n",
      "        |                   8589934594|\n",
      "        |                   8589934595|\n",
      "        |                   8589934596|\n",
      "        +-----------------------------+\n",
      "    \n",
      "    month(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the month of a given date/timestamp as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            month part of the date/timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(month('dt').alias('month')).collect()\n",
      "        [Row(month=4)]\n",
      "    \n",
      "    months(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Partition transform function: A transform for timestamps and dates\n",
      "        to partition data into months.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date or timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            data partitioned by months.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").partitionedBy(\n",
      "        ...     months(\"ts\")\n",
      "        ... ).createOrReplace()  # doctest: +SKIP\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function can be used only in combination with\n",
      "        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n",
      "        method of the `DataFrameWriterV2`.\n",
      "    \n",
      "    months_between(date1: 'ColumnOrName', date2: 'ColumnOrName', roundOff: bool = True) -> pyspark.sql.column.Column\n",
      "        Returns number of months between dates date1 and date2.\n",
      "        If date1 is later than date2, then the result is positive.\n",
      "        A whole number is returned if both inputs have the same day of month or both are the last day\n",
      "        of their respective months. Otherwise, the difference is calculated assuming 31 days per month.\n",
      "        The result is rounded off to 8 digits unless `roundOff` is set to `False`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        date1 : :class:`~pyspark.sql.Column` or str\n",
      "            first date column.\n",
      "        date2 : :class:`~pyspark.sql.Column` or str\n",
      "            second date column.\n",
      "        roundOff : bool, optional\n",
      "            whether to round (to 8 digits) the final value or not (default: True).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            number of months between two dates.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['date1', 'date2'])\n",
      "        >>> df.select(months_between(df.date1, df.date2).alias('months')).collect()\n",
      "        [Row(months=3.94959677)]\n",
      "        >>> df.select(months_between(df.date1, df.date2, False).alias('months')).collect()\n",
      "        [Row(months=3.9495967741935485)]\n",
      "    \n",
      "    named_struct(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Creates a struct with the given field names and values.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            list of columns to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, 2, 3)], ['a', 'b', 'c'])\n",
      "        >>> df.select(named_struct(lit('x'), df.a, lit('y'), df.b).alias('r')).collect()\n",
      "        [Row(r=Row(x=1, y=2))]\n",
      "    \n",
      "    nanvl(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns col1 if it is not NaN, or col2 if col1 is NaN.\n",
      "        \n",
      "        Both inputs should be floating point columns (:class:`DoubleType` or :class:`FloatType`).\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            first column to check.\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            second column to return if first is NaN.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value from first column or second if first is NaN .\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n",
      "        >>> df.select(nanvl(\"a\", \"b\").alias(\"r1\"), nanvl(df.a, df.b).alias(\"r2\")).collect()\n",
      "        [Row(r1=1.0, r2=1.0), Row(r1=2.0, r2=2.0)]\n",
      "    \n",
      "    negate = negative(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the negative value.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to calculate negative value for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            negative value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(3).select(sf.negative(\"id\")).show()\n",
      "        +------------+\n",
      "        |negative(id)|\n",
      "        +------------+\n",
      "        |           0|\n",
      "        |          -1|\n",
      "        |          -2|\n",
      "        +------------+\n",
      "    \n",
      "    negative(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the negative value.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to calculate negative value for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            negative value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(3).select(sf.negative(\"id\")).show()\n",
      "        +------------+\n",
      "        |negative(id)|\n",
      "        +------------+\n",
      "        |           0|\n",
      "        |          -1|\n",
      "        |          -2|\n",
      "        +------------+\n",
      "    \n",
      "    next_day(date: 'ColumnOrName', dayOfWeek: str) -> pyspark.sql.column.Column\n",
      "        Returns the first date which is later than the value of the date column\n",
      "        based on second `week day` argument.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        date : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        dayOfWeek : str\n",
      "            day of the week, case-insensitive, accepts:\n",
      "                \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column of computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-07-27',)], ['d'])\n",
      "        >>> df.select(next_day(df.d, 'Sun').alias('date')).collect()\n",
      "        [Row(date=datetime.date(2015, 8, 2))]\n",
      "    \n",
      "    now() -> pyspark.sql.column.Column\n",
      "        Returns the current timestamp at the start of query evaluation.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            current timestamp at the start of query evaluation.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(now()).show(truncate=False) # doctest: +SKIP\n",
      "        +-----------------------+\n",
      "        |now()    |\n",
      "        +-----------------------+\n",
      "        |2022-08-26 21:23:22.716|\n",
      "        +-----------------------+\n",
      "    \n",
      "    nth_value(col: 'ColumnOrName', offset: int, ignoreNulls: Optional[bool] = False) -> pyspark.sql.column.Column\n",
      "        Window function: returns the value that is the `offset`\\th row of the window frame\n",
      "        (counting from 1), and `null` if the size of window frame is less than `offset` rows.\n",
      "        \n",
      "        It will return the `offset`\\th non-null value it sees when `ignoreNulls` is set to\n",
      "        true. If all values are null, then null is returned.\n",
      "        \n",
      "        This is equivalent to the nth_value function in SQL.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        offset : int\n",
      "            number of row to use as the value\n",
      "        ignoreNulls : bool, optional\n",
      "            indicates the Nth value should skip null in the\n",
      "            determination of which row to use\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value of nth row.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window\n",
      "        >>> df = spark.createDataFrame([(\"a\", 1),\n",
      "        ...                             (\"a\", 2),\n",
      "        ...                             (\"a\", 3),\n",
      "        ...                             (\"b\", 8),\n",
      "        ...                             (\"b\", 2)], [\"c1\", \"c2\"])\n",
      "        >>> df.show()\n",
      "        +---+---+\n",
      "        | c1| c2|\n",
      "        +---+---+\n",
      "        |  a|  1|\n",
      "        |  a|  2|\n",
      "        |  a|  3|\n",
      "        |  b|  8|\n",
      "        |  b|  2|\n",
      "        +---+---+\n",
      "        >>> w = Window.partitionBy(\"c1\").orderBy(\"c2\")\n",
      "        >>> df.withColumn(\"nth_value\", nth_value(\"c2\", 1).over(w)).show()\n",
      "        +---+---+---------+\n",
      "        | c1| c2|nth_value|\n",
      "        +---+---+---------+\n",
      "        |  a|  1|        1|\n",
      "        |  a|  2|        1|\n",
      "        |  a|  3|        1|\n",
      "        |  b|  2|        2|\n",
      "        |  b|  8|        2|\n",
      "        +---+---+---------+\n",
      "        >>> df.withColumn(\"nth_value\", nth_value(\"c2\", 2).over(w)).show()\n",
      "        +---+---+---------+\n",
      "        | c1| c2|nth_value|\n",
      "        +---+---+---------+\n",
      "        |  a|  1|     NULL|\n",
      "        |  a|  2|        2|\n",
      "        |  a|  3|        2|\n",
      "        |  b|  2|     NULL|\n",
      "        |  b|  8|        8|\n",
      "        +---+---+---------+\n",
      "    \n",
      "    ntile(n: int) -> pyspark.sql.column.Column\n",
      "        Window function: returns the ntile group id (from 1 to `n` inclusive)\n",
      "        in an ordered window partition. For example, if `n` is 4, the first\n",
      "        quarter of the rows will get value 1, the second quarter will get 2,\n",
      "        the third quarter will get 3, and the last quarter will get 4.\n",
      "        \n",
      "        This is equivalent to the NTILE function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            an integer\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            portioned group id.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window\n",
      "        >>> df = spark.createDataFrame([(\"a\", 1),\n",
      "        ...                             (\"a\", 2),\n",
      "        ...                             (\"a\", 3),\n",
      "        ...                             (\"b\", 8),\n",
      "        ...                             (\"b\", 2)], [\"c1\", \"c2\"])\n",
      "        >>> df.show()\n",
      "        +---+---+\n",
      "        | c1| c2|\n",
      "        +---+---+\n",
      "        |  a|  1|\n",
      "        |  a|  2|\n",
      "        |  a|  3|\n",
      "        |  b|  8|\n",
      "        |  b|  2|\n",
      "        +---+---+\n",
      "        >>> w = Window.partitionBy(\"c1\").orderBy(\"c2\")\n",
      "        >>> df.withColumn(\"ntile\", ntile(2).over(w)).show()\n",
      "        +---+---+-----+\n",
      "        | c1| c2|ntile|\n",
      "        +---+---+-----+\n",
      "        |  a|  1|    1|\n",
      "        |  a|  2|    1|\n",
      "        |  a|  3|    2|\n",
      "        |  b|  2|    1|\n",
      "        |  b|  8|    2|\n",
      "        +---+---+-----+\n",
      "    \n",
      "    nullif(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns null if `col1` equals to `col2`, or `col1` otherwise.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(None, None,), (1, 9,)], [\"a\", \"b\"])\n",
      "        >>> df.select(nullif(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r=None), Row(r=1)]\n",
      "    \n",
      "    nvl(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns `col2` if `col1` is null, or `col1` otherwise.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(None, 8,), (1, 9,)], [\"a\", \"b\"])\n",
      "        >>> df.select(nvl(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r=8), Row(r=1)]\n",
      "    \n",
      "    nvl2(col1: 'ColumnOrName', col2: 'ColumnOrName', col3: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns `col2` if `col1` is not null, or `col3` otherwise.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "        col3 : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(None, 8, 6,), (1, 9, 9,)], [\"a\", \"b\", \"c\"])\n",
      "        >>> df.select(nvl2(df.a, df.b, df.c).alias('r')).collect()\n",
      "        [Row(r=6), Row(r=9)]\n",
      "    \n",
      "    octet_length(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Calculates the byte length for the specified string column.\n",
      "        \n",
      "        .. versionadded:: 3.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            Source column or strings\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            Byte length of the col\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import octet_length\n",
      "        >>> spark.createDataFrame([('cat',), ( '🐈',)], ['cat']) \\\n",
      "        ...      .select(octet_length('cat')).collect()\n",
      "            [Row(octet_length(cat)=3), Row(octet_length(cat)=4)]\n",
      "    \n",
      "    overlay(src: 'ColumnOrName', replace: 'ColumnOrName', pos: Union[ForwardRef('ColumnOrName'), int], len: Union[ForwardRef('ColumnOrName'), int] = -1) -> pyspark.sql.column.Column\n",
      "        Overlay the specified portion of `src` with `replace`,\n",
      "        starting from byte position `pos` of `src` and proceeding for `len` bytes.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        src : :class:`~pyspark.sql.Column` or str\n",
      "            column name or column containing the string that will be replaced\n",
      "        replace : :class:`~pyspark.sql.Column` or str\n",
      "            column name or column containing the substitution string\n",
      "        pos : :class:`~pyspark.sql.Column` or str or int\n",
      "            column name, column, or int containing the starting position in src\n",
      "        len : :class:`~pyspark.sql.Column` or str or int, optional\n",
      "            column name, column, or int containing the number of bytes to replace in src\n",
      "            string by 'replace' defaults to -1, which represents the length of the 'replace' string\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            string with replaced values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"SPARK_SQL\", \"CORE\")], (\"x\", \"y\"))\n",
      "        >>> df.select(overlay(\"x\", \"y\", 7).alias(\"overlayed\")).collect()\n",
      "        [Row(overlayed='SPARK_CORE')]\n",
      "        >>> df.select(overlay(\"x\", \"y\", 7, 0).alias(\"overlayed\")).collect()\n",
      "        [Row(overlayed='SPARK_CORESQL')]\n",
      "        >>> df.select(overlay(\"x\", \"y\", 7, 2).alias(\"overlayed\")).collect()\n",
      "        [Row(overlayed='SPARK_COREL')]\n",
      "    \n",
      "    parse_url(url: 'ColumnOrName', partToExtract: 'ColumnOrName', key: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Extracts a part from a URL.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        url : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string.\n",
      "        partToExtract : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string, the path.\n",
      "        key : :class:`~pyspark.sql.Column` or str, optional\n",
      "            A column of string, the key.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(\"http://spark.apache.org/path?query=1\", \"QUERY\", \"query\",)],\n",
      "        ...     [\"a\", \"b\", \"c\"]\n",
      "        ... )\n",
      "        >>> df.select(parse_url(df.a, df.b, df.c).alias('r')).collect()\n",
      "        [Row(r='1')]\n",
      "        \n",
      "        >>> df.select(parse_url(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r='query=1')]\n",
      "    \n",
      "    percent_rank() -> pyspark.sql.column.Column\n",
      "        Window function: returns the relative rank (i.e. percentile) of rows within a window partition.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for calculating relative rank.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window, types\n",
      "        >>> df = spark.createDataFrame([1, 1, 2, 3, 3, 4], types.IntegerType())\n",
      "        >>> w = Window.orderBy(\"value\")\n",
      "        >>> df.withColumn(\"pr\", percent_rank().over(w)).show()\n",
      "        +-----+---+\n",
      "        |value| pr|\n",
      "        +-----+---+\n",
      "        |    1|0.0|\n",
      "        |    1|0.0|\n",
      "        |    2|0.4|\n",
      "        |    3|0.6|\n",
      "        |    3|0.6|\n",
      "        |    4|1.0|\n",
      "        +-----+---+\n",
      "    \n",
      "    percentile(col: 'ColumnOrName', percentage: Union[pyspark.sql.column.Column, float, List[float], Tuple[float]], frequency: Union[pyspark.sql.column.Column, int] = 1) -> pyspark.sql.column.Column\n",
      "        Returns the exact percentile(s) of numeric column `expr` at the given percentage(s)\n",
      "        with value range in [0.0, 1.0].\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str input column.\n",
      "        percentage : :class:`~pyspark.sql.Column`, float, list of floats or tuple of floats\n",
      "            percentage in decimal (must be between 0.0 and 1.0).\n",
      "        frequency : :class:`~pyspark.sql.Column` or int is a positive numeric literal which\n",
      "            controls frequency.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the exact `percentile` of the numeric column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> key = (col(\"id\") % 3).alias(\"key\")\n",
      "        >>> value = (randn(42) + key * 10).alias(\"value\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(key, value)\n",
      "        >>> df.select(\n",
      "        ...     percentile(\"value\", [0.25, 0.5, 0.75], lit(1)).alias(\"quantiles\")\n",
      "        ... ).show()\n",
      "        +--------------------+\n",
      "        |           quantiles|\n",
      "        +--------------------+\n",
      "        |[0.74419914941216...|\n",
      "        +--------------------+\n",
      "        \n",
      "        >>> df.groupBy(\"key\").agg(\n",
      "        ...     percentile(\"value\", 0.5, lit(1)).alias(\"median\")\n",
      "        ... ).show()\n",
      "        +---+--------------------+\n",
      "        |key|              median|\n",
      "        +---+--------------------+\n",
      "        |  0|-0.03449962216667901|\n",
      "        |  1|   9.990389751837329|\n",
      "        |  2|  19.967859769284075|\n",
      "        +---+--------------------+\n",
      "    \n",
      "    percentile_approx(col: 'ColumnOrName', percentage: Union[pyspark.sql.column.Column, float, List[float], Tuple[float]], accuracy: Union[pyspark.sql.column.Column, float] = 10000) -> pyspark.sql.column.Column\n",
      "        Returns the approximate `percentile` of the numeric column `col` which is the smallest value\n",
      "        in the ordered `col` values (sorted from least to greatest) such that no more than `percentage`\n",
      "        of `col` values is less than the value or equal to that value.\n",
      "        \n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column.\n",
      "        percentage : :class:`~pyspark.sql.Column`, float, list of floats or tuple of floats\n",
      "            percentage in decimal (must be between 0.0 and 1.0).\n",
      "            When percentage is an array, each value of the percentage array must be between 0.0 and 1.0.\n",
      "            In this case, returns the approximate percentile array of column col\n",
      "            at the given percentage array.\n",
      "        accuracy : :class:`~pyspark.sql.Column` or float\n",
      "            is a positive numeric literal which controls approximation accuracy\n",
      "            at the cost of memory. Higher value of accuracy yields better accuracy,\n",
      "            1.0/accuracy is the relative error of the approximation. (default: 10000).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            approximate `percentile` of the numeric column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> key = (col(\"id\") % 3).alias(\"key\")\n",
      "        >>> value = (randn(42) + key * 10).alias(\"value\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(key, value)\n",
      "        >>> df.select(\n",
      "        ...     percentile_approx(\"value\", [0.25, 0.5, 0.75], 1000000).alias(\"quantiles\")\n",
      "        ... ).printSchema()\n",
      "        root\n",
      "         |-- quantiles: array (nullable = true)\n",
      "         |    |-- element: double (containsNull = false)\n",
      "        \n",
      "        >>> df.groupBy(\"key\").agg(\n",
      "        ...     percentile_approx(\"value\", 0.5, lit(1000000)).alias(\"median\")\n",
      "        ... ).printSchema()\n",
      "        root\n",
      "         |-- key: long (nullable = true)\n",
      "         |-- median: double (nullable = true)\n",
      "    \n",
      "    pi() -> pyspark.sql.column.Column\n",
      "        Returns Pi.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.range(1).select(pi()).show()\n",
      "        +-----------------+\n",
      "        |             PI()|\n",
      "        +-----------------+\n",
      "        |3.141592653589793|\n",
      "        +-----------------+\n",
      "    \n",
      "    pmod(dividend: Union[ForwardRef('ColumnOrName'), float], divisor: Union[ForwardRef('ColumnOrName'), float]) -> pyspark.sql.column.Column\n",
      "        Returns the positive value of dividend mod divisor.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        dividend : str, :class:`~pyspark.sql.Column` or float\n",
      "            the column that contains dividend, or the specified dividend value\n",
      "        divisor : str, :class:`~pyspark.sql.Column` or float\n",
      "            the column that contains divisor, or the specified divisor value\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            positive value of dividend mod divisor.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Supports Spark Connect.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import pmod\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     (1.0, float('nan')), (float('nan'), 2.0), (10.0, 3.0),\n",
      "        ...     (float('nan'), float('nan')), (-3.0, 4.0), (-10.0, 3.0),\n",
      "        ...     (-5.0, -6.0), (7.0, -8.0), (1.0, 2.0)],\n",
      "        ...     (\"a\", \"b\"))\n",
      "        >>> df.select(pmod(\"a\", \"b\")).show()\n",
      "        +----------+\n",
      "        |pmod(a, b)|\n",
      "        +----------+\n",
      "        |       NaN|\n",
      "        |       NaN|\n",
      "        |       1.0|\n",
      "        |       NaN|\n",
      "        |       1.0|\n",
      "        |       2.0|\n",
      "        |      -5.0|\n",
      "        |       7.0|\n",
      "        |       1.0|\n",
      "        +----------+\n",
      "    \n",
      "    posexplode(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new row for each element with position in the given array or map.\n",
      "        Uses the default column name `pos` for position, and `col` for elements in the\n",
      "        array and `key` and `value` for elements in the map unless specified otherwise.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            one row per array item or map key value including positions as a separate column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
      "        >>> df.select(posexplode(df.intlist)).collect()\n",
      "        [Row(pos=0, col=1), Row(pos=1, col=2), Row(pos=2, col=3)]\n",
      "        \n",
      "        >>> df.select(posexplode(df.mapfield)).show()\n",
      "        +---+---+-----+\n",
      "        |pos|key|value|\n",
      "        +---+---+-----+\n",
      "        |  0|  a|    b|\n",
      "        +---+---+-----+\n",
      "    \n",
      "    posexplode_outer(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new row for each element with position in the given array or map.\n",
      "        Unlike posexplode, if the array/map is null or empty then the row (null, null) is produced.\n",
      "        Uses the default column name `pos` for position, and `col` for elements in the\n",
      "        array and `key` and `value` for elements in the map unless specified otherwise.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            one row per array item or map key value including positions as a separate column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, [\"foo\", \"bar\"], {\"x\": 1.0}), (2, [], {}), (3, None, None)],\n",
      "        ...     (\"id\", \"an_array\", \"a_map\")\n",
      "        ... )\n",
      "        >>> df.select(\"id\", \"an_array\", posexplode_outer(\"a_map\")).show()\n",
      "        +---+----------+----+----+-----+\n",
      "        | id|  an_array| pos| key|value|\n",
      "        +---+----------+----+----+-----+\n",
      "        |  1|[foo, bar]|   0|   x|  1.0|\n",
      "        |  2|        []|NULL|NULL| NULL|\n",
      "        |  3|      NULL|NULL|NULL| NULL|\n",
      "        +---+----------+----+----+-----+\n",
      "        >>> df.select(\"id\", \"a_map\", posexplode_outer(\"an_array\")).show()\n",
      "        +---+----------+----+----+\n",
      "        | id|     a_map| pos| col|\n",
      "        +---+----------+----+----+\n",
      "        |  1|{x -> 1.0}|   0| foo|\n",
      "        |  1|{x -> 1.0}|   1| bar|\n",
      "        |  2|        {}|NULL|NULL|\n",
      "        |  3|      NULL|NULL|NULL|\n",
      "        +---+----------+----+----+\n",
      "    \n",
      "    position(substr: 'ColumnOrName', str: 'ColumnOrName', start: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Returns the position of the first occurrence of `substr` in `str` after position `start`.\n",
      "        The given `start` and return value are 1-based.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        substr : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string, substring.\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string.\n",
      "        start : :class:`~pyspark.sql.Column` or str, optional\n",
      "            A column of string, start position.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(\"bar\", \"foobarbar\", 5,)], [\"a\", \"b\", \"c\"]\n",
      "        ... ).select(sf.position(\"a\", \"b\", \"c\")).show()\n",
      "        +-----------------+\n",
      "        |position(a, b, c)|\n",
      "        +-----------------+\n",
      "        |                7|\n",
      "        +-----------------+\n",
      "        \n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(\"bar\", \"foobarbar\", 5,)], [\"a\", \"b\", \"c\"]\n",
      "        ... ).select(sf.position(\"a\", \"b\")).show()\n",
      "        +-----------------+\n",
      "        |position(a, b, 1)|\n",
      "        +-----------------+\n",
      "        |                4|\n",
      "        +-----------------+\n",
      "    \n",
      "    positive(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the value.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input value column.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(-1,), (0,), (1,)], ['v'])\n",
      "        >>> df.select(positive(\"v\").alias(\"p\")).show()\n",
      "        +---+\n",
      "        |  p|\n",
      "        +---+\n",
      "        | -1|\n",
      "        |  0|\n",
      "        |  1|\n",
      "        +---+\n",
      "    \n",
      "    pow(col1: Union[ForwardRef('ColumnOrName'), float], col2: Union[ForwardRef('ColumnOrName'), float]) -> pyspark.sql.column.Column\n",
      "        Returns the value of the first argument raised to the power of the second argument.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : str, :class:`~pyspark.sql.Column` or float\n",
      "            the base number.\n",
      "        col2 : str, :class:`~pyspark.sql.Column` or float\n",
      "            the exponent number.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the base rased to the power the argument.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(pow(lit(3), lit(2))).first()\n",
      "        Row(POWER(3, 2)=9.0)\n",
      "    \n",
      "    power = pow(col1: Union[ForwardRef('ColumnOrName'), float], col2: Union[ForwardRef('ColumnOrName'), float]) -> pyspark.sql.column.Column\n",
      "        Returns the value of the first argument raised to the power of the second argument.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : str, :class:`~pyspark.sql.Column` or float\n",
      "            the base number.\n",
      "        col2 : str, :class:`~pyspark.sql.Column` or float\n",
      "            the exponent number.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the base rased to the power the argument.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(pow(lit(3), lit(2))).first()\n",
      "        Row(POWER(3, 2)=9.0)\n",
      "    \n",
      "    printf(format: 'ColumnOrName', *cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Formats the arguments in printf-style and returns the result as a string column.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        format : :class:`~pyspark.sql.Column` or str\n",
      "            string that can contain embedded format tags and used as result column's value\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s to be used in formatting\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(\"aa%d%s\", 123, \"cc\",)], [\"a\", \"b\", \"c\"]\n",
      "        ... ).select(sf.printf(\"a\", \"b\", \"c\")).show()\n",
      "        +---------------+\n",
      "        |printf(a, b, c)|\n",
      "        +---------------+\n",
      "        |        aa123cc|\n",
      "        +---------------+\n",
      "    \n",
      "    product(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the product of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : str, :class:`Column`\n",
      "            column containing values to be multiplied together\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1, 10).toDF('x').withColumn('mod3', col('x') % 3)\n",
      "        >>> prods = df.groupBy('mod3').agg(product('x').alias('product'))\n",
      "        >>> prods.orderBy('mod3').show()\n",
      "        +----+-------+\n",
      "        |mod3|product|\n",
      "        +----+-------+\n",
      "        |   0|  162.0|\n",
      "        |   1|   28.0|\n",
      "        |   2|   80.0|\n",
      "        +----+-------+\n",
      "    \n",
      "    quarter(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the quarter of a given date/timestamp as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            quarter of the date/timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(quarter('dt').alias('quarter')).collect()\n",
      "        [Row(quarter=2)]\n",
      "    \n",
      "    radians(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Converts an angle measured in degrees to an approximately equivalent angle\n",
      "        measured in radians.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            angle in degrees\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            angle in radians, as if computed by `java.lang.Math.toRadians()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(radians(lit(180))).first()\n",
      "        Row(RADIANS(180)=3.14159...)\n",
      "    \n",
      "    raise_error(errMsg: Union[pyspark.sql.column.Column, str]) -> pyspark.sql.column.Column\n",
      "        Throws an exception with the provided error message.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        errMsg : :class:`~pyspark.sql.Column` or str\n",
      "            A Python string literal or column containing the error message\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            throws an error with specified message.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(raise_error(\"My error message\")).show() # doctest: +SKIP\n",
      "        ...\n",
      "        java.lang.RuntimeException: My error message\n",
      "        ...\n",
      "    \n",
      "    rand(seed: Optional[int] = None) -> pyspark.sql.column.Column\n",
      "        Generates a random column with independent and identically distributed (i.i.d.) samples\n",
      "        uniformly distributed in [0.0, 1.0).\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic in general case.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        seed : int (default: None)\n",
      "            seed value for random generator.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            random values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import functions as sf\n",
      "        >>> spark.range(0, 2, 1, 1).withColumn('rand', sf.rand(seed=42) * 3).show()\n",
      "        +---+------------------+\n",
      "        | id|              rand|\n",
      "        +---+------------------+\n",
      "        |  0|1.8575681106759028|\n",
      "        |  1|1.5288056527339444|\n",
      "        +---+------------------+\n",
      "    \n",
      "    randn(seed: Optional[int] = None) -> pyspark.sql.column.Column\n",
      "        Generates a column with independent and identically distributed (i.i.d.) samples from\n",
      "        the standard normal distribution.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic in general case.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        seed : int (default: None)\n",
      "            seed value for random generator.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            random values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import functions as sf\n",
      "        >>> spark.range(0, 2, 1, 1).withColumn('randn', sf.randn(seed=42)).show()\n",
      "        +---+------------------+\n",
      "        | id|             randn|\n",
      "        +---+------------------+\n",
      "        |  0| 2.384479054241165|\n",
      "        |  1|0.1920934041293524|\n",
      "        +---+------------------+\n",
      "    \n",
      "    rank() -> pyspark.sql.column.Column\n",
      "        Window function: returns the rank of rows within a window partition.\n",
      "        \n",
      "        The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking\n",
      "        sequence when there are ties. That is, if you were ranking a competition using dense_rank\n",
      "        and had three people tie for second place, you would say that all three were in second\n",
      "        place and that the next person came in third. Rank would give me sequential numbers, making\n",
      "        the person that came in third place (after the ties) would register as coming in fifth.\n",
      "        \n",
      "        This is equivalent to the RANK function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for calculating ranks.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window, types\n",
      "        >>> df = spark.createDataFrame([1, 1, 2, 3, 3, 4], types.IntegerType())\n",
      "        >>> w = Window.orderBy(\"value\")\n",
      "        >>> df.withColumn(\"drank\", rank().over(w)).show()\n",
      "        +-----+-----+\n",
      "        |value|drank|\n",
      "        +-----+-----+\n",
      "        |    1|    1|\n",
      "        |    1|    1|\n",
      "        |    2|    3|\n",
      "        |    3|    4|\n",
      "        |    3|    4|\n",
      "        |    4|    6|\n",
      "        +-----+-----+\n",
      "    \n",
      "    reduce(col: 'ColumnOrName', initialValue: 'ColumnOrName', merge: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column], finish: Optional[Callable[[pyspark.sql.column.Column], pyspark.sql.column.Column]] = None) -> pyspark.sql.column.Column\n",
      "        Applies a binary operator to an initial state and all elements in the array,\n",
      "        and reduces this to a single state. The final state is converted into the final result\n",
      "        by applying a finish function.\n",
      "        \n",
      "        Both functions can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "        :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "        Python ``UserDefinedFunctions`` are not supported\n",
      "        (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        initialValue : :class:`~pyspark.sql.Column` or str\n",
      "            initial value. Name of column or expression\n",
      "        merge : function\n",
      "            a binary function ``(acc: Column, x: Column) -> Column...`` returning expression\n",
      "            of the same type as ``zero``\n",
      "        finish : function\n",
      "            an optional unary function ``(x: Column) -> Column: ...``\n",
      "            used to convert accumulated value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            final value after aggregate function is applied.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, [20.0, 4.0, 2.0, 6.0, 10.0])], (\"id\", \"values\"))\n",
      "        >>> df.select(reduce(\"values\", lit(0.0), lambda acc, x: acc + x).alias(\"sum\")).show()\n",
      "        +----+\n",
      "        | sum|\n",
      "        +----+\n",
      "        |42.0|\n",
      "        +----+\n",
      "        \n",
      "        >>> def merge(acc, x):\n",
      "        ...     count = acc.count + 1\n",
      "        ...     sum = acc.sum + x\n",
      "        ...     return struct(count.alias(\"count\"), sum.alias(\"sum\"))\n",
      "        ...\n",
      "        >>> df.select(\n",
      "        ...     reduce(\n",
      "        ...         \"values\",\n",
      "        ...         struct(lit(0).alias(\"count\"), lit(0.0).alias(\"sum\")),\n",
      "        ...         merge,\n",
      "        ...         lambda acc: acc.sum / acc.count,\n",
      "        ...     ).alias(\"mean\")\n",
      "        ... ).show()\n",
      "        +----+\n",
      "        |mean|\n",
      "        +----+\n",
      "        | 8.4|\n",
      "        +----+\n",
      "    \n",
      "    reflect(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Calls a method with reflection.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            the first element should be a literal string for the class name,\n",
      "            and the second element should be a literal string for the method name,\n",
      "            and the remaining are input arguments to the Java method.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"a5cf6c42-0c85-418f-af6c-3e4e5b1328f2\",)], [\"a\"])\n",
      "        >>> df.select(\n",
      "        ...     reflect(lit(\"java.util.UUID\"), lit(\"fromString\"), df.a).alias('r')\n",
      "        ... ).collect()\n",
      "        [Row(r='a5cf6c42-0c85-418f-af6c-3e4e5b1328f2')]\n",
      "    \n",
      "    regexp(str: 'ColumnOrName', regexp: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns true if `str` matches the Java regex `regexp`, or false otherwise.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        regexp : :class:`~pyspark.sql.Column` or str\n",
      "            regex pattern to apply.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            true if `str` matches a Java regex, or false otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n",
      "        ... ).select(sf.regexp('str', sf.lit(r'(\\d+)'))).show()\n",
      "        +------------------+\n",
      "        |REGEXP(str, (\\d+))|\n",
      "        +------------------+\n",
      "        |              true|\n",
      "        +------------------+\n",
      "        \n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n",
      "        ... ).select(sf.regexp('str', sf.lit(r'\\d{2}b'))).show()\n",
      "        +-------------------+\n",
      "        |REGEXP(str, \\d{2}b)|\n",
      "        +-------------------+\n",
      "        |              false|\n",
      "        +-------------------+\n",
      "        \n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n",
      "        ... ).select(sf.regexp('str', sf.col(\"regexp\"))).show()\n",
      "        +-------------------+\n",
      "        |REGEXP(str, regexp)|\n",
      "        +-------------------+\n",
      "        |               true|\n",
      "        +-------------------+\n",
      "    \n",
      "    regexp_count(str: 'ColumnOrName', regexp: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a count of the number of times that the Java regex pattern `regexp` is matched\n",
      "        in the string `str`.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        regexp : :class:`~pyspark.sql.Column` or str\n",
      "            regex pattern to apply.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the number of times that a Java regex pattern is matched in the string.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"1a 2b 14m\", r\"\\d+\")], [\"str\", \"regexp\"])\n",
      "        >>> df.select(regexp_count('str', lit(r'\\d+')).alias('d')).collect()\n",
      "        [Row(d=3)]\n",
      "        >>> df.select(regexp_count('str', lit(r'mmm')).alias('d')).collect()\n",
      "        [Row(d=0)]\n",
      "        >>> df.select(regexp_count(\"str\", col(\"regexp\")).alias('d')).collect()\n",
      "        [Row(d=3)]\n",
      "    \n",
      "    regexp_extract(str: 'ColumnOrName', pattern: str, idx: int) -> pyspark.sql.column.Column\n",
      "        Extract a specific group matched by the Java regex `regexp`, from the specified string column.\n",
      "        If the regex did not match, or the specified group did not match, an empty string is returned.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        pattern : str\n",
      "            regex pattern to apply.\n",
      "        idx : int\n",
      "            matched group id.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            matched value specified by `idx` group id.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('100-200',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', r'(\\d+)-(\\d+)', 1).alias('d')).collect()\n",
      "        [Row(d='100')]\n",
      "        >>> df = spark.createDataFrame([('foo',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', r'(\\d+)', 1).alias('d')).collect()\n",
      "        [Row(d='')]\n",
      "        >>> df = spark.createDataFrame([('aaaac',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', '(a+)(b)?(c)', 2).alias('d')).collect()\n",
      "        [Row(d='')]\n",
      "    \n",
      "    regexp_extract_all(str: 'ColumnOrName', regexp: 'ColumnOrName', idx: Union[int, pyspark.sql.column.Column, NoneType] = None) -> pyspark.sql.column.Column\n",
      "        Extract all strings in the `str` that match the Java regex `regexp`\n",
      "        and corresponding to the regex group index.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        regexp : :class:`~pyspark.sql.Column` or str\n",
      "            regex pattern to apply.\n",
      "        idx : int\n",
      "            matched group id.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            all strings in the `str` that match a Java regex and corresponding to the regex group index.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"100-200, 300-400\", r\"(\\d+)-(\\d+)\")], [\"str\", \"regexp\"])\n",
      "        >>> df.select(regexp_extract_all('str', lit(r'(\\d+)-(\\d+)')).alias('d')).collect()\n",
      "        [Row(d=['100', '300'])]\n",
      "        >>> df.select(regexp_extract_all('str', lit(r'(\\d+)-(\\d+)'), 1).alias('d')).collect()\n",
      "        [Row(d=['100', '300'])]\n",
      "        >>> df.select(regexp_extract_all('str', lit(r'(\\d+)-(\\d+)'), 2).alias('d')).collect()\n",
      "        [Row(d=['200', '400'])]\n",
      "        >>> df.select(regexp_extract_all('str', col(\"regexp\")).alias('d')).collect()\n",
      "        [Row(d=['100', '300'])]\n",
      "    \n",
      "    regexp_instr(str: 'ColumnOrName', regexp: 'ColumnOrName', idx: Union[int, pyspark.sql.column.Column, NoneType] = None) -> pyspark.sql.column.Column\n",
      "        Extract all strings in the `str` that match the Java regex `regexp`\n",
      "        and corresponding to the regex group index.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        regexp : :class:`~pyspark.sql.Column` or str\n",
      "            regex pattern to apply.\n",
      "        idx : int\n",
      "            matched group id.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            all strings in the `str` that match a Java regex and corresponding to the regex group index.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"1a 2b 14m\", r\"\\d+(a|b|m)\")], [\"str\", \"regexp\"])\n",
      "        >>> df.select(regexp_instr('str', lit(r'\\d+(a|b|m)')).alias('d')).collect()\n",
      "        [Row(d=1)]\n",
      "        >>> df.select(regexp_instr('str', lit(r'\\d+(a|b|m)'), 1).alias('d')).collect()\n",
      "        [Row(d=1)]\n",
      "        >>> df.select(regexp_instr('str', lit(r'\\d+(a|b|m)'), 2).alias('d')).collect()\n",
      "        [Row(d=1)]\n",
      "        >>> df.select(regexp_instr('str', col(\"regexp\")).alias('d')).collect()\n",
      "        [Row(d=1)]\n",
      "    \n",
      "    regexp_like(str: 'ColumnOrName', regexp: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns true if `str` matches the Java regex `regexp`, or false otherwise.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        regexp : :class:`~pyspark.sql.Column` or str\n",
      "            regex pattern to apply.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            true if `str` matches a Java regex, or false otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n",
      "        ... ).select(sf.regexp_like('str', sf.lit(r'(\\d+)'))).show()\n",
      "        +-----------------------+\n",
      "        |REGEXP_LIKE(str, (\\d+))|\n",
      "        +-----------------------+\n",
      "        |                   true|\n",
      "        +-----------------------+\n",
      "        \n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n",
      "        ... ).select(sf.regexp_like('str', sf.lit(r'\\d{2}b'))).show()\n",
      "        +------------------------+\n",
      "        |REGEXP_LIKE(str, \\d{2}b)|\n",
      "        +------------------------+\n",
      "        |                   false|\n",
      "        +------------------------+\n",
      "        \n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"]\n",
      "        ... ).select(sf.regexp_like('str', sf.col(\"regexp\"))).show()\n",
      "        +------------------------+\n",
      "        |REGEXP_LIKE(str, regexp)|\n",
      "        +------------------------+\n",
      "        |                    true|\n",
      "        +------------------------+\n",
      "    \n",
      "    regexp_replace(string: 'ColumnOrName', pattern: Union[str, pyspark.sql.column.Column], replacement: Union[str, pyspark.sql.column.Column]) -> pyspark.sql.column.Column\n",
      "        Replace all substrings of the specified string value that match regexp with replacement.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        string : :class:`~pyspark.sql.Column` or str\n",
      "            column name or column containing the string value\n",
      "        pattern : :class:`~pyspark.sql.Column` or str\n",
      "            column object or str containing the regexp pattern\n",
      "        replacement : :class:`~pyspark.sql.Column` or str\n",
      "            column object or str containing the replacement\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            string with all substrings replaced.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"100-200\", r\"(\\d+)\", \"--\")], [\"str\", \"pattern\", \"replacement\"])\n",
      "        >>> df.select(regexp_replace('str', r'(\\d+)', '--').alias('d')).collect()\n",
      "        [Row(d='-----')]\n",
      "        >>> df.select(regexp_replace(\"str\", col(\"pattern\"), col(\"replacement\")).alias('d')).collect()\n",
      "        [Row(d='-----')]\n",
      "    \n",
      "    regexp_substr(str: 'ColumnOrName', regexp: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the substring that matches the Java regex `regexp` within the string `str`.\n",
      "        If the regular expression is not found, the result is null.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        regexp : :class:`~pyspark.sql.Column` or str\n",
      "            regex pattern to apply.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the substring that matches a Java regex within the string `str`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"1a 2b 14m\", r\"\\d+\")], [\"str\", \"regexp\"])\n",
      "        >>> df.select(regexp_substr('str', lit(r'\\d+')).alias('d')).collect()\n",
      "        [Row(d='1')]\n",
      "        >>> df.select(regexp_substr('str', lit(r'mmm')).alias('d')).collect()\n",
      "        [Row(d=None)]\n",
      "        >>> df.select(regexp_substr(\"str\", col(\"regexp\")).alias('d')).collect()\n",
      "        [Row(d='1')]\n",
      "    \n",
      "    regr_avgx(y: 'ColumnOrName', x: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the average of the independent variable for non-null pairs\n",
      "        in a group, where `y` is the dependent variable and `x` is the independent variable.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : :class:`~pyspark.sql.Column` or str\n",
      "            the dependent variable.\n",
      "        x : :class:`~pyspark.sql.Column` or str\n",
      "            the independent variable.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the average of the independent variable for non-null pairs in a group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> x = (col(\"id\") % 3).alias(\"x\")\n",
      "        >>> y = (randn(42) + x * 10).alias(\"y\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(x, y)\n",
      "        >>> df.select(regr_avgx(\"y\", \"x\")).first()\n",
      "        Row(regr_avgx(y, x)=0.999)\n",
      "    \n",
      "    regr_avgy(y: 'ColumnOrName', x: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the average of the dependent variable for non-null pairs\n",
      "        in a group, where `y` is the dependent variable and `x` is the independent variable.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : :class:`~pyspark.sql.Column` or str\n",
      "            the dependent variable.\n",
      "        x : :class:`~pyspark.sql.Column` or str\n",
      "            the independent variable.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the average of the dependent variable for non-null pairs in a group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> x = (col(\"id\") % 3).alias(\"x\")\n",
      "        >>> y = (randn(42) + x * 10).alias(\"y\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(x, y)\n",
      "        >>> df.select(regr_avgy(\"y\", \"x\")).first()\n",
      "        Row(regr_avgy(y, x)=9.980732994136464)\n",
      "    \n",
      "    regr_count(y: 'ColumnOrName', x: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the number of non-null number pairs\n",
      "        in a group, where `y` is the dependent variable and `x` is the independent variable.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : :class:`~pyspark.sql.Column` or str\n",
      "            the dependent variable.\n",
      "        x : :class:`~pyspark.sql.Column` or str\n",
      "            the independent variable.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the number of non-null number pairs in a group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> x = (col(\"id\") % 3).alias(\"x\")\n",
      "        >>> y = (randn(42) + x * 10).alias(\"y\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(x, y)\n",
      "        >>> df.select(regr_count(\"y\", \"x\")).first()\n",
      "        Row(regr_count(y, x)=1000)\n",
      "    \n",
      "    regr_intercept(y: 'ColumnOrName', x: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the intercept of the univariate linear regression line\n",
      "        for non-null pairs in a group, where `y` is the dependent variable and\n",
      "        `x` is the independent variable.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : :class:`~pyspark.sql.Column` or str\n",
      "            the dependent variable.\n",
      "        x : :class:`~pyspark.sql.Column` or str\n",
      "            the independent variable.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the intercept of the univariate linear regression line for non-null pairs in a group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> x = (col(\"id\") % 3).alias(\"x\")\n",
      "        >>> y = (randn(42) + x * 10).alias(\"y\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(x, y)\n",
      "        >>> df.select(regr_intercept(\"y\", \"x\")).first()\n",
      "        Row(regr_intercept(y, x)=-0.04961745990969568)\n",
      "    \n",
      "    regr_r2(y: 'ColumnOrName', x: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the coefficient of determination for non-null pairs\n",
      "        in a group, where `y` is the dependent variable and `x` is the independent variable.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : :class:`~pyspark.sql.Column` or str\n",
      "            the dependent variable.\n",
      "        x : :class:`~pyspark.sql.Column` or str\n",
      "            the independent variable.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the coefficient of determination for non-null pairs in a group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> x = (col(\"id\") % 3).alias(\"x\")\n",
      "        >>> y = (randn(42) + x * 10).alias(\"y\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(x, y)\n",
      "        >>> df.select(regr_r2(\"y\", \"x\")).first()\n",
      "        Row(regr_r2(y, x)=0.9851908293645436)\n",
      "    \n",
      "    regr_slope(y: 'ColumnOrName', x: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the slope of the linear regression line for non-null pairs\n",
      "        in a group, where `y` is the dependent variable and `x` is the independent variable.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : :class:`~pyspark.sql.Column` or str\n",
      "            the dependent variable.\n",
      "        x : :class:`~pyspark.sql.Column` or str\n",
      "            the independent variable.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the slope of the linear regression line for non-null pairs in a group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> x = (col(\"id\") % 3).alias(\"x\")\n",
      "        >>> y = (randn(42) + x * 10).alias(\"y\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(x, y)\n",
      "        >>> df.select(regr_slope(\"y\", \"x\")).first()\n",
      "        Row(regr_slope(y, x)=10.040390844891048)\n",
      "    \n",
      "    regr_sxx(y: 'ColumnOrName', x: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns REGR_COUNT(y, x) * VAR_POP(x) for non-null pairs\n",
      "        in a group, where `y` is the dependent variable and `x` is the independent variable.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : :class:`~pyspark.sql.Column` or str\n",
      "            the dependent variable.\n",
      "        x : :class:`~pyspark.sql.Column` or str\n",
      "            the independent variable.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            REGR_COUNT(y, x) * VAR_POP(x) for non-null pairs in a group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> x = (col(\"id\") % 3).alias(\"x\")\n",
      "        >>> y = (randn(42) + x * 10).alias(\"y\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(x, y)\n",
      "        >>> df.select(regr_sxx(\"y\", \"x\")).first()\n",
      "        Row(regr_sxx(y, x)=666.9989999999996)\n",
      "    \n",
      "    regr_sxy(y: 'ColumnOrName', x: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns REGR_COUNT(y, x) * COVAR_POP(y, x) for non-null pairs\n",
      "        in a group, where `y` is the dependent variable and `x` is the independent variable.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : :class:`~pyspark.sql.Column` or str\n",
      "            the dependent variable.\n",
      "        x : :class:`~pyspark.sql.Column` or str\n",
      "            the independent variable.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            REGR_COUNT(y, x) * COVAR_POP(y, x) for non-null pairs in a group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> x = (col(\"id\") % 3).alias(\"x\")\n",
      "        >>> y = (randn(42) + x * 10).alias(\"y\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(x, y)\n",
      "        >>> df.select(regr_sxy(\"y\", \"x\")).first()\n",
      "        Row(regr_sxy(y, x)=6696.93065315148)\n",
      "    \n",
      "    regr_syy(y: 'ColumnOrName', x: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns REGR_COUNT(y, x) * VAR_POP(y) for non-null pairs\n",
      "        in a group, where `y` is the dependent variable and `x` is the independent variable.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : :class:`~pyspark.sql.Column` or str\n",
      "            the dependent variable.\n",
      "        x : :class:`~pyspark.sql.Column` or str\n",
      "            the independent variable.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            REGR_COUNT(y, x) * VAR_POP(y) for non-null pairs in a group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> x = (col(\"id\") % 3).alias(\"x\")\n",
      "        >>> y = (randn(42) + x * 10).alias(\"y\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(x, y)\n",
      "        >>> df.select(regr_syy(\"y\", \"x\")).first()\n",
      "        Row(regr_syy(y, x)=68250.53503811295)\n",
      "    \n",
      "    repeat(col: 'ColumnOrName', n: int) -> pyspark.sql.column.Column\n",
      "        Repeats a string column n times, and returns it as a new string column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        n : int\n",
      "            number of times to repeat value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            string with repeated values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('ab',)], ['s',])\n",
      "        >>> df.select(repeat(df.s, 3).alias('s')).collect()\n",
      "        [Row(s='ababab')]\n",
      "    \n",
      "    replace(src: 'ColumnOrName', search: 'ColumnOrName', replace: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Replaces all occurrences of `search` with `replace`.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        src : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string to be replaced.\n",
      "        search : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string, If `search` is not found in `str`, `str` is returned unchanged.\n",
      "        replace : :class:`~pyspark.sql.Column` or str, optional\n",
      "            A column of string, If `replace` is not specified or is an empty string,\n",
      "            nothing replaces the string that is removed from `str`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"ABCabc\", \"abc\", \"DEF\",)], [\"a\", \"b\", \"c\"])\n",
      "        >>> df.select(replace(df.a, df.b, df.c).alias('r')).collect()\n",
      "        [Row(r='ABCDEF')]\n",
      "        \n",
      "        >>> df.select(replace(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r='ABC')]\n",
      "    \n",
      "    reverse(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: returns a reversed string or an array with reverse order of elements.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            array of elements in reverse order.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('Spark SQL',)], ['data'])\n",
      "        >>> df.select(reverse(df.data).alias('s')).collect()\n",
      "        [Row(s='LQS krapS')]\n",
      "        >>> df = spark.createDataFrame([([2, 1, 3],) ,([1],) ,([],)], ['data'])\n",
      "        >>> df.select(reverse(df.data).alias('r')).collect()\n",
      "        [Row(r=[3, 1, 2]), Row(r=[1]), Row(r=[])]\n",
      "    \n",
      "    right(str: 'ColumnOrName', len: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the rightmost `len`(`len` can be string type) characters from the string `str`,\n",
      "        if `len` is less or equal than 0 the result is an empty string.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        len : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings, the rightmost `len`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Spark SQL\", 3,)], ['a', 'b'])\n",
      "        >>> df.select(right(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r='SQL')]\n",
      "    \n",
      "    rint(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the double value that is closest in value to the argument and\n",
      "        is equal to a mathematical integer.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(rint(lit(10.6))).show()\n",
      "        +----------+\n",
      "        |rint(10.6)|\n",
      "        +----------+\n",
      "        |      11.0|\n",
      "        +----------+\n",
      "        \n",
      "        >>> df.select(rint(lit(10.3))).show()\n",
      "        +----------+\n",
      "        |rint(10.3)|\n",
      "        +----------+\n",
      "        |      10.0|\n",
      "        +----------+\n",
      "    \n",
      "    rlike(str: 'ColumnOrName', regexp: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns true if `str` matches the Java regex `regexp`, or false otherwise.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        regexp : :class:`~pyspark.sql.Column` or str\n",
      "            regex pattern to apply.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            true if `str` matches a Java regex, or false otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"1a 2b 14m\", r\"(\\d+)\")], [\"str\", \"regexp\"])\n",
      "        >>> df.select(rlike('str', lit(r'(\\d+)')).alias('d')).collect()\n",
      "        [Row(d=True)]\n",
      "        >>> df.select(rlike('str', lit(r'\\d{2}b')).alias('d')).collect()\n",
      "        [Row(d=False)]\n",
      "        >>> df.select(rlike(\"str\", col(\"regexp\")).alias('d')).collect()\n",
      "        [Row(d=True)]\n",
      "    \n",
      "    round(col: 'ColumnOrName', scale: int = 0) -> pyspark.sql.column.Column\n",
      "        Round the given value to `scale` decimal places using HALF_UP rounding mode if `scale` >= 0\n",
      "        or at integral part when `scale` < 0.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column to round.\n",
      "        scale : int optional default 0\n",
      "            scale value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            rounded values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([(2.5,)], ['a']).select(round('a', 0).alias('r')).collect()\n",
      "        [Row(r=3.0)]\n",
      "    \n",
      "    row_number() -> pyspark.sql.column.Column\n",
      "        Window function: returns a sequential number starting at 1 within a window partition.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for calculating row numbers.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window\n",
      "        >>> df = spark.range(3)\n",
      "        >>> w = Window.orderBy(df.id.desc())\n",
      "        >>> df.withColumn(\"desc_order\", row_number().over(w)).show()\n",
      "        +---+----------+\n",
      "        | id|desc_order|\n",
      "        +---+----------+\n",
      "        |  2|         1|\n",
      "        |  1|         2|\n",
      "        |  0|         3|\n",
      "        +---+----------+\n",
      "    \n",
      "    rpad(col: 'ColumnOrName', len: int, pad: str) -> pyspark.sql.column.Column\n",
      "        Right-pad the string column to width `len` with `pad`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        len : int\n",
      "            length of the final string.\n",
      "        pad : str\n",
      "            chars to append.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            right padded result.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(rpad(df.s, 6, '#').alias('s')).collect()\n",
      "        [Row(s='abcd##')]\n",
      "    \n",
      "    rtrim(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Trim the spaces from right end for the specified string value.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            right trimmed values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\"   Spark\", \"Spark  \", \" Spark\"], \"STRING\")\n",
      "        >>> df.select(rtrim(\"value\").alias(\"r\")).withColumn(\"length\", length(\"r\")).show()\n",
      "        +--------+------+\n",
      "        |       r|length|\n",
      "        +--------+------+\n",
      "        |   Spark|     8|\n",
      "        |   Spark|     5|\n",
      "        |   Spark|     6|\n",
      "        +--------+------+\n",
      "    \n",
      "    schema_of_csv(csv: 'ColumnOrName', options: Optional[Dict[str, str]] = None) -> pyspark.sql.column.Column\n",
      "        Parses a CSV string and infers its schema in DDL format.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        csv : :class:`~pyspark.sql.Column` or str\n",
      "            a CSV string or a foldable string column containing a CSV string.\n",
      "        options : dict, optional\n",
      "            options to control parsing. accepts the same options as the CSV datasource.\n",
      "            See `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
      "            for the version you use.\n",
      "        \n",
      "            .. # noqa\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a string representation of a :class:`StructType` parsed from given CSV.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(schema_of_csv(lit('1|a'), {'sep':'|'}).alias(\"csv\")).collect()\n",
      "        [Row(csv='STRUCT<_c0: INT, _c1: STRING>')]\n",
      "        >>> df.select(schema_of_csv('1|a', {'sep':'|'}).alias(\"csv\")).collect()\n",
      "        [Row(csv='STRUCT<_c0: INT, _c1: STRING>')]\n",
      "    \n",
      "    schema_of_json(json: 'ColumnOrName', options: Optional[Dict[str, str]] = None) -> pyspark.sql.column.Column\n",
      "        Parses a JSON string and infers its schema in DDL format.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        json : :class:`~pyspark.sql.Column` or str\n",
      "            a JSON string or a foldable string column containing a JSON string.\n",
      "        options : dict, optional\n",
      "            options to control parsing. accepts the same options as the JSON datasource.\n",
      "            See `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n",
      "            for the version you use.\n",
      "        \n",
      "            .. # noqa\n",
      "        \n",
      "            .. versionchanged:: 3.0.0\n",
      "               It accepts `options` parameter to control schema inferring.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a string representation of a :class:`StructType` parsed from given JSON.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(schema_of_json(lit('{\"a\": 0}')).alias(\"json\")).collect()\n",
      "        [Row(json='STRUCT<a: BIGINT>')]\n",
      "        >>> schema = schema_of_json('{a: 1}', {'allowUnquotedFieldNames':'true'})\n",
      "        >>> df.select(schema.alias(\"json\")).collect()\n",
      "        [Row(json='STRUCT<a: BIGINT>')]\n",
      "    \n",
      "    sec(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes secant of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            Angle in radians\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            Secant of the angle.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(sec(lit(1.5))).first()\n",
      "        Row(SEC(1.5)=14.13683...)\n",
      "    \n",
      "    second(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the seconds of a given date as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            `seconds` part of the timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import datetime\n",
      "        >>> df = spark.createDataFrame([(datetime.datetime(2015, 4, 8, 13, 8, 15),)], ['ts'])\n",
      "        >>> df.select(second('ts').alias('second')).collect()\n",
      "        [Row(second=15)]\n",
      "    \n",
      "    sentences(string: 'ColumnOrName', language: Optional[ForwardRef('ColumnOrName')] = None, country: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Splits a string into arrays of sentences, where each sentence is an array of words.\n",
      "        The 'language' and 'country' arguments are optional, and if omitted, the default locale is used.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        string : :class:`~pyspark.sql.Column` or str\n",
      "            a string to be split\n",
      "        language : :class:`~pyspark.sql.Column` or str, optional\n",
      "            a language of the locale\n",
      "        country : :class:`~pyspark.sql.Column` or str, optional\n",
      "            a country of the locale\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            arrays of split sentences.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[\"This is an example sentence.\"]], [\"string\"])\n",
      "        >>> df.select(sentences(df.string, lit(\"en\"), lit(\"US\"))).show(truncate=False)\n",
      "        +-----------------------------------+\n",
      "        |sentences(string, en, US)          |\n",
      "        +-----------------------------------+\n",
      "        |[[This, is, an, example, sentence]]|\n",
      "        +-----------------------------------+\n",
      "        >>> df = spark.createDataFrame([[\"Hello world. How are you?\"]], [\"s\"])\n",
      "        >>> df.select(sentences(\"s\")).show(truncate=False)\n",
      "        +---------------------------------+\n",
      "        |sentences(s, , )                 |\n",
      "        +---------------------------------+\n",
      "        |[[Hello, world], [How, are, you]]|\n",
      "        +---------------------------------+\n",
      "    \n",
      "    sequence(start: 'ColumnOrName', stop: 'ColumnOrName', step: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Generate a sequence of integers from `start` to `stop`, incrementing by `step`.\n",
      "        If `step` is not set, incrementing by 1 if `start` is less than or equal to `stop`,\n",
      "        otherwise -1.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        start : :class:`~pyspark.sql.Column` or str\n",
      "            starting value (inclusive)\n",
      "        stop : :class:`~pyspark.sql.Column` or str\n",
      "            last values (inclusive)\n",
      "        step : :class:`~pyspark.sql.Column` or str, optional\n",
      "            value to add to current to get next element (default is 1)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of sequence values\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df1 = spark.createDataFrame([(-2, 2)], ('C1', 'C2'))\n",
      "        >>> df1.select(sequence('C1', 'C2').alias('r')).collect()\n",
      "        [Row(r=[-2, -1, 0, 1, 2])]\n",
      "        >>> df2 = spark.createDataFrame([(4, -4, -2)], ('C1', 'C2', 'C3'))\n",
      "        >>> df2.select(sequence('C1', 'C2', 'C3').alias('r')).collect()\n",
      "        [Row(r=[4, 2, 0, -2, -4])]\n",
      "    \n",
      "    session_window(timeColumn: 'ColumnOrName', gapDuration: Union[pyspark.sql.column.Column, str]) -> pyspark.sql.column.Column\n",
      "        Generates session window given a timestamp specifying column.\n",
      "        Session window is one of dynamic windows, which means the length of window is varying\n",
      "        according to the given inputs. The length of session window is defined as \"the timestamp\n",
      "        of latest input of the session + gap duration\", so when the new inputs are bound to the\n",
      "        current session window, the end time of session window can be expanded according to the new\n",
      "        inputs.\n",
      "        Windows can support microsecond precision. Windows in the order of months are not supported.\n",
      "        For a streaming query, you may use the function `current_timestamp` to generate windows on\n",
      "        processing time.\n",
      "        gapDuration is provided as strings, e.g. '1 second', '1 day 12 hours', '2 minutes'. Valid\n",
      "        interval strings are 'week', 'day', 'hour', 'minute', 'second', 'millisecond', 'microsecond'.\n",
      "        It could also be a Column which can be evaluated to gap duration dynamically based on the\n",
      "        input row.\n",
      "        The output column will be a struct called 'session_window' by default with the nested columns\n",
      "        'start' and 'end', where 'start' and 'end' will be of :class:`pyspark.sql.types.TimestampType`.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timeColumn : :class:`~pyspark.sql.Column` or str\n",
      "            The column name or column to use as the timestamp for windowing by time.\n",
      "            The time column must be of TimestampType or TimestampNTZType.\n",
      "        gapDuration : :class:`~pyspark.sql.Column` or str\n",
      "            A Python string literal or column specifying the timeout of the session. It could be\n",
      "            static value, e.g. `10 minutes`, `1 second`, or an expression/UDF that specifies gap\n",
      "            duration dynamically based on the input row.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"2016-03-11 09:00:07\", 1)]).toDF(\"date\", \"val\")\n",
      "        >>> w = df.groupBy(session_window(\"date\", \"5 seconds\")).agg(sum(\"val\").alias(\"sum\"))\n",
      "        >>> w.select(w.session_window.start.cast(\"string\").alias(\"start\"),\n",
      "        ...          w.session_window.end.cast(\"string\").alias(\"end\"), \"sum\").collect()\n",
      "        [Row(start='2016-03-11 09:00:07', end='2016-03-11 09:00:12', sum=1)]\n",
      "        >>> w = df.groupBy(session_window(\"date\", lit(\"5 seconds\"))).agg(sum(\"val\").alias(\"sum\"))\n",
      "        >>> w.select(w.session_window.start.cast(\"string\").alias(\"start\"),\n",
      "        ...          w.session_window.end.cast(\"string\").alias(\"end\"), \"sum\").collect()\n",
      "        [Row(start='2016-03-11 09:00:07', end='2016-03-11 09:00:12', sum=1)]\n",
      "    \n",
      "    sha(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a sha1 hash value as a hex string of the `col`.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(1).select(sf.sha(sf.lit(\"Spark\"))).show()\n",
      "        +--------------------+\n",
      "        |          sha(Spark)|\n",
      "        +--------------------+\n",
      "        |85f5955f4b27a9a4c...|\n",
      "        +--------------------+\n",
      "    \n",
      "    sha1(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the hex string result of SHA-1.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(sha1('a').alias('hash')).collect()\n",
      "        [Row(hash='3c01bdbb26f358bab27f267924aa2c9a03fcfdb8')]\n",
      "    \n",
      "    sha2(col: 'ColumnOrName', numBits: int) -> pyspark.sql.column.Column\n",
      "        Returns the hex string result of SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384,\n",
      "        and SHA-512). The numBits indicates the desired bit length of the result, which must have a\n",
      "        value of 224, 256, 384, 512, or 0 (which is equivalent to 256).\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        numBits : int\n",
      "            the desired bit length of the result, which must have a\n",
      "            value of 224, 256, 384, 512, or 0 (which is equivalent to 256).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[\"Alice\"], [\"Bob\"]], [\"name\"])\n",
      "        >>> df.withColumn(\"sha2\", sha2(df.name, 256)).show(truncate=False)\n",
      "        +-----+----------------------------------------------------------------+\n",
      "        |name |sha2                                                            |\n",
      "        +-----+----------------------------------------------------------------+\n",
      "        |Alice|3bc51062973c458d5a6f2d8d64a023246354ad7e064b1e4e009ec8a0699a3043|\n",
      "        |Bob  |cd9fb1e148ccd8442e5aa74904cc73bf6fb54d1d54d333bd596aa9bb4bb4e961|\n",
      "        +-----+----------------------------------------------------------------+\n",
      "    \n",
      "    shiftLeft(col: 'ColumnOrName', numBits: int) -> pyspark.sql.column.Column\n",
      "        Shift the given value numBits left.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        .. deprecated:: 3.2.0\n",
      "            Use :func:`shiftleft` instead.\n",
      "    \n",
      "    shiftRight(col: 'ColumnOrName', numBits: int) -> pyspark.sql.column.Column\n",
      "        (Signed) shift the given value numBits right.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        .. deprecated:: 3.2.0\n",
      "            Use :func:`shiftright` instead.\n",
      "    \n",
      "    shiftRightUnsigned(col: 'ColumnOrName', numBits: int) -> pyspark.sql.column.Column\n",
      "        Unsigned shift the given value numBits right.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        .. deprecated:: 3.2.0\n",
      "            Use :func:`shiftrightunsigned` instead.\n",
      "    \n",
      "    shiftleft(col: 'ColumnOrName', numBits: int) -> pyspark.sql.column.Column\n",
      "        Shift the given value numBits left.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to shift.\n",
      "        numBits : int\n",
      "            number of bits to shift.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            shifted value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([(21,)], ['a']).select(shiftleft('a', 1).alias('r')).collect()\n",
      "        [Row(r=42)]\n",
      "    \n",
      "    shiftright(col: 'ColumnOrName', numBits: int) -> pyspark.sql.column.Column\n",
      "        (Signed) shift the given value numBits right.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to shift.\n",
      "        numBits : int\n",
      "            number of bits to shift.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            shifted values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([(42,)], ['a']).select(shiftright('a', 1).alias('r')).collect()\n",
      "        [Row(r=21)]\n",
      "    \n",
      "    shiftrightunsigned(col: 'ColumnOrName', numBits: int) -> pyspark.sql.column.Column\n",
      "        Unsigned shift the given value numBits right.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to shift.\n",
      "        numBits : int\n",
      "            number of bits to shift.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            shifted value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(-42,)], ['a'])\n",
      "        >>> df.select(shiftrightunsigned('a', 1).alias('r')).collect()\n",
      "        [Row(r=9223372036854775787)]\n",
      "    \n",
      "    shuffle(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: Generates a random permutation of the given array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of elements in random order.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 20, 3, 5],), ([1, 20, None, 3],)], ['data'])\n",
      "        >>> df.select(shuffle(df.data).alias('s')).collect()  # doctest: +SKIP\n",
      "        [Row(s=[3, 1, 5, 20]), Row(s=[20, None, 3, 1])]\n",
      "    \n",
      "    sign(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the signum of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(1).select(\n",
      "        ...     sf.sign(sf.lit(-5)),\n",
      "        ...     sf.sign(sf.lit(6))\n",
      "        ... ).show()\n",
      "        +--------+-------+\n",
      "        |sign(-5)|sign(6)|\n",
      "        +--------+-------+\n",
      "        |    -1.0|    1.0|\n",
      "        +--------+-------+\n",
      "    \n",
      "    signum(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the signum of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(1).select(\n",
      "        ...     sf.signum(sf.lit(-5)),\n",
      "        ...     sf.signum(sf.lit(6))\n",
      "        ... ).show()\n",
      "        +----------+---------+\n",
      "        |SIGNUM(-5)|SIGNUM(6)|\n",
      "        +----------+---------+\n",
      "        |      -1.0|      1.0|\n",
      "        +----------+---------+\n",
      "    \n",
      "    sin(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes sine of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            sine of the angle, as if computed by `java.lang.Math.sin()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import math\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(sin(lit(math.radians(90)))).first()\n",
      "        Row(SIN(1.57079...)=1.0)\n",
      "    \n",
      "    sinh(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes hyperbolic sine of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            hyperbolic angle.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hyperbolic sine of the given value,\n",
      "            as if computed by `java.lang.Math.sinh()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(sinh(lit(1.1))).first()\n",
      "        Row(SINH(1.1)=1.33564...)\n",
      "    \n",
      "    size(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: returns the length of the array or map stored in the column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            length of the array/map.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 2, 3],),([1],),([],)], ['data'])\n",
      "        >>> df.select(size(df.data)).collect()\n",
      "        [Row(size(data)=3), Row(size(data)=1), Row(size(data)=0)]\n",
      "    \n",
      "    skewness(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the skewness of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            skewness of given column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n",
      "        >>> df.select(skewness(df.c)).first()\n",
      "        Row(skewness(c)=0.70710...)\n",
      "    \n",
      "    slice(x: 'ColumnOrName', start: Union[ForwardRef('ColumnOrName'), int], length: Union[ForwardRef('ColumnOrName'), int]) -> pyspark.sql.column.Column\n",
      "        Collection function: returns an array containing all the elements in `x` from index `start`\n",
      "        (array indices start at 1, or from the end if `start` is negative) with the specified `length`.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : :class:`~pyspark.sql.Column` or str\n",
      "            column name or column containing the array to be sliced\n",
      "        start : :class:`~pyspark.sql.Column` or str or int\n",
      "            column name, column, or int containing the starting index\n",
      "        length : :class:`~pyspark.sql.Column` or str or int\n",
      "            column name, column, or int containing the length of the slice\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a column of array type. Subset of array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 2, 3],), ([4, 5],)], ['x'])\n",
      "        >>> df.select(slice(df.x, 2, 2).alias(\"sliced\")).collect()\n",
      "        [Row(sliced=[2, 3]), Row(sliced=[5])]\n",
      "    \n",
      "    some(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns true if at least one value of `col` is true.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to check if at least one value is true.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            true if at least one value of `col` is true, false otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [[True], [True], [True]], [\"flag\"]\n",
      "        ... ).select(sf.some(\"flag\")).show()\n",
      "        +----------+\n",
      "        |some(flag)|\n",
      "        +----------+\n",
      "        |      true|\n",
      "        +----------+\n",
      "        \n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [[True], [False], [True]], [\"flag\"]\n",
      "        ... ).select(sf.some(\"flag\")).show()\n",
      "        +----------+\n",
      "        |some(flag)|\n",
      "        +----------+\n",
      "        |      true|\n",
      "        +----------+\n",
      "        \n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [[False], [False], [False]], [\"flag\"]\n",
      "        ... ).select(sf.some(\"flag\")).show()\n",
      "        +----------+\n",
      "        |some(flag)|\n",
      "        +----------+\n",
      "        |     false|\n",
      "        +----------+\n",
      "    \n",
      "    sort_array(col: 'ColumnOrName', asc: bool = True) -> pyspark.sql.column.Column\n",
      "        Collection function: sorts the input array in ascending or descending order according\n",
      "        to the natural ordering of the array elements. Null elements will be placed at the beginning\n",
      "        of the returned array in ascending order or at the end of the returned array in descending\n",
      "        order.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        asc : bool, optional\n",
      "            whether to sort in ascending or descending order. If `asc` is True (default)\n",
      "            then ascending and if False then descending.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            sorted array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], ['data'])\n",
      "        >>> df.select(sort_array(df.data).alias('r')).collect()\n",
      "        [Row(r=[None, 1, 2, 3]), Row(r=[1]), Row(r=[])]\n",
      "        >>> df.select(sort_array(df.data, asc=False).alias('r')).collect()\n",
      "        [Row(r=[3, 2, 1, None]), Row(r=[1]), Row(r=[])]\n",
      "    \n",
      "    soundex(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the SoundEx encoding for a string\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            SoundEx encoded string.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Peters\",),(\"Uhrbach\",)], ['name'])\n",
      "        >>> df.select(soundex(df.name).alias(\"soundex\")).collect()\n",
      "        [Row(soundex='P362'), Row(soundex='U612')]\n",
      "    \n",
      "    spark_partition_id() -> pyspark.sql.column.Column\n",
      "        A column for partition ID.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is non deterministic because it depends on data partitioning and task scheduling.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            partition id the record belongs to.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(2)\n",
      "        >>> df.repartition(1).select(spark_partition_id().alias(\"pid\")).collect()\n",
      "        [Row(pid=0), Row(pid=0)]\n",
      "    \n",
      "    split(str: 'ColumnOrName', pattern: str, limit: int = -1) -> pyspark.sql.column.Column\n",
      "        Splits str around matches of the given pattern.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            a string expression to split\n",
      "        pattern : str\n",
      "            a string representing a regular expression. The regex string should be\n",
      "            a Java regular expression.\n",
      "        limit : int, optional\n",
      "            an integer which controls the number of times `pattern` is applied.\n",
      "        \n",
      "            * ``limit > 0``: The resulting array's length will not be more than `limit`, and the\n",
      "                             resulting array's last entry will contain all input beyond the last\n",
      "                             matched pattern.\n",
      "            * ``limit <= 0``: `pattern` will be applied as many times as possible, and the resulting\n",
      "                              array can be of any size.\n",
      "        \n",
      "            .. versionchanged:: 3.0\n",
      "               `split` now takes an optional `limit` field. If not provided, default limit value is -1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            array of separated strings.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('oneAtwoBthreeC',)], ['s',])\n",
      "        >>> df.select(split(df.s, '[ABC]', 2).alias('s')).collect()\n",
      "        [Row(s=['one', 'twoBthreeC'])]\n",
      "        >>> df.select(split(df.s, '[ABC]', -1).alias('s')).collect()\n",
      "        [Row(s=['one', 'two', 'three', ''])]\n",
      "    \n",
      "    split_part(src: 'ColumnOrName', delimiter: 'ColumnOrName', partNum: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Splits `str` by delimiter and return requested part of the split (1-based).\n",
      "        If any input is null, returns null. if `partNum` is out of range of split parts,\n",
      "        returns empty string. If `partNum` is 0, throws an error. If `partNum` is negative,\n",
      "        the parts are counted backward from the end of the string.\n",
      "        If the `delimiter` is an empty string, the `str` is not split.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        src : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string to be splited.\n",
      "        delimiter : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string, the delimiter used for split.\n",
      "        partNum : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string, requested part of the split (1-based).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"11.12.13\", \".\", 3,)], [\"a\", \"b\", \"c\"])\n",
      "        >>> df.select(split_part(df.a, df.b, df.c).alias('r')).collect()\n",
      "        [Row(r='13')]\n",
      "    \n",
      "    sqrt(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the square root of the specified float value.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(sqrt(lit(4))).show()\n",
      "        +-------+\n",
      "        |SQRT(4)|\n",
      "        +-------+\n",
      "        |    2.0|\n",
      "        +-------+\n",
      "    \n",
      "    stack(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Separates `col1`, ..., `colk` into `n` rows. Uses column names col0, col1, etc. by default\n",
      "        unless specified otherwise.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            the first element should be a literal int for the number of rows to be separated,\n",
      "            and the remaining are input elements to be separated.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, 2, 3)], [\"a\", \"b\", \"c\"])\n",
      "        >>> df.select(stack(lit(2), df.a, df.b, df.c)).show(truncate=False)\n",
      "        +----+----+\n",
      "        |col0|col1|\n",
      "        +----+----+\n",
      "        |1   |2   |\n",
      "        |3   |NULL|\n",
      "        +----+----+\n",
      "    \n",
      "    startswith(str: 'ColumnOrName', prefix: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a boolean. The value is True if str starts with prefix.\n",
      "        Returns NULL if either input expression is NULL. Otherwise, returns False.\n",
      "        Both str or prefix must be of STRING or BINARY type.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string.\n",
      "        prefix : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string, the prefix.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Spark SQL\", \"Spark\",)], [\"a\", \"b\"])\n",
      "        >>> df.select(startswith(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r=True)]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"414243\", \"4142\",)], [\"e\", \"f\"])\n",
      "        >>> df = df.select(to_binary(\"e\").alias(\"e\"), to_binary(\"f\").alias(\"f\"))\n",
      "        >>> df.printSchema()\n",
      "        root\n",
      "         |-- e: binary (nullable = true)\n",
      "         |-- f: binary (nullable = true)\n",
      "        >>> df.select(startswith(\"e\", \"f\"), startswith(\"f\", \"e\")).show()\n",
      "        +----------------+----------------+\n",
      "        |startswith(e, f)|startswith(f, e)|\n",
      "        +----------------+----------------+\n",
      "        |            true|           false|\n",
      "        +----------------+----------------+\n",
      "    \n",
      "    std(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: alias for stddev_samp.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            standard deviation of given column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(6).select(sf.std(\"id\")).show()\n",
      "        +------------------+\n",
      "        |           std(id)|\n",
      "        +------------------+\n",
      "        |1.8708286933869...|\n",
      "        +------------------+\n",
      "    \n",
      "    stddev(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: alias for stddev_samp.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            standard deviation of given column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(6).select(sf.stddev(\"id\")).show()\n",
      "        +------------------+\n",
      "        |        stddev(id)|\n",
      "        +------------------+\n",
      "        |1.8708286933869...|\n",
      "        +------------------+\n",
      "    \n",
      "    stddev_pop(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns population standard deviation of\n",
      "        the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            standard deviation of given column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(6).select(sf.stddev_pop(\"id\")).show()\n",
      "        +-----------------+\n",
      "        |   stddev_pop(id)|\n",
      "        +-----------------+\n",
      "        |1.707825127659...|\n",
      "        +-----------------+\n",
      "    \n",
      "    stddev_samp(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the unbiased sample standard deviation of\n",
      "        the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            standard deviation of given column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(6).select(sf.stddev_samp(\"id\")).show()\n",
      "        +------------------+\n",
      "        |   stddev_samp(id)|\n",
      "        +------------------+\n",
      "        |1.8708286933869...|\n",
      "        +------------------+\n",
      "    \n",
      "    str_to_map(text: 'ColumnOrName', pairDelim: Optional[ForwardRef('ColumnOrName')] = None, keyValueDelim: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Creates a map after splitting the text into key/value pairs using delimiters.\n",
      "        Both `pairDelim` and `keyValueDelim` are treated as regular expressions.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        text : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        pairDelim : :class:`~pyspark.sql.Column` or str, optional\n",
      "            delimiter to use to split pair.\n",
      "        keyValueDelim : :class:`~pyspark.sql.Column` or str, optional\n",
      "            delimiter to use to split key/value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"a:1,b:2,c:3\",)], [\"e\"])\n",
      "        >>> df.select(str_to_map(df.e, lit(\",\"), lit(\":\")).alias('r')).collect()\n",
      "        [Row(r={'a': '1', 'b': '2', 'c': '3'})]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"a:1,b:2,c:3\",)], [\"e\"])\n",
      "        >>> df.select(str_to_map(df.e, lit(\",\")).alias('r')).collect()\n",
      "        [Row(r={'a': '1', 'b': '2', 'c': '3'})]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"a:1,b:2,c:3\",)], [\"e\"])\n",
      "        >>> df.select(str_to_map(df.e).alias('r')).collect()\n",
      "        [Row(r={'a': '1', 'b': '2', 'c': '3'})]\n",
      "    \n",
      "    struct(*cols: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName_')], Tuple[ForwardRef('ColumnOrName_'), ...]]) -> pyspark.sql.column.Column\n",
      "        Creates a new struct column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : list, set, str or :class:`~pyspark.sql.Column`\n",
      "            column names or :class:`~pyspark.sql.Column`\\s to contain in the output struct.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a struct type column of given columns.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5)], (\"name\", \"age\"))\n",
      "        >>> df.select(struct('age', 'name').alias(\"struct\")).collect()\n",
      "        [Row(struct=Row(age=2, name='Alice')), Row(struct=Row(age=5, name='Bob'))]\n",
      "        >>> df.select(struct([df.age, df.name]).alias(\"struct\")).collect()\n",
      "        [Row(struct=Row(age=2, name='Alice')), Row(struct=Row(age=5, name='Bob'))]\n",
      "    \n",
      "    substr(str: 'ColumnOrName', pos: 'ColumnOrName', len: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Returns the substring of `str` that starts at `pos` and is of length `len`,\n",
      "        or the slice of byte array that starts at `pos` and is of length `len`.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        src : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string.\n",
      "        pos : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string, the substring of `str` that starts at `pos`.\n",
      "        len : :class:`~pyspark.sql.Column` or str, optional\n",
      "            A column of string, the substring of `str` is of length `len`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(\"Spark SQL\", 5, 1,)], [\"a\", \"b\", \"c\"]\n",
      "        ... ).select(sf.substr(\"a\", \"b\", \"c\")).show()\n",
      "        +---------------+\n",
      "        |substr(a, b, c)|\n",
      "        +---------------+\n",
      "        |              k|\n",
      "        +---------------+\n",
      "        \n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(\"Spark SQL\", 5, 1,)], [\"a\", \"b\", \"c\"]\n",
      "        ... ).select(sf.substr(\"a\", \"b\")).show()\n",
      "        +------------------------+\n",
      "        |substr(a, b, 2147483647)|\n",
      "        +------------------------+\n",
      "        |                   k SQL|\n",
      "        +------------------------+\n",
      "    \n",
      "    substring(str: 'ColumnOrName', pos: int, len: int) -> pyspark.sql.column.Column\n",
      "        Substring starts at `pos` and is of length `len` when str is String type or\n",
      "        returns the slice of byte array that starts at `pos` in byte and is of length `len`\n",
      "        when str is Binary type.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not zero based, but 1 based index.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        pos : int\n",
      "            starting position in str.\n",
      "        len : int\n",
      "            length of chars.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            substring of given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(substring(df.s, 1, 2).alias('s')).collect()\n",
      "        [Row(s='ab')]\n",
      "    \n",
      "    substring_index(str: 'ColumnOrName', delim: str, count: int) -> pyspark.sql.column.Column\n",
      "        Returns the substring from string str before count occurrences of the delimiter delim.\n",
      "        If count is positive, everything the left of the final delimiter (counting from left) is\n",
      "        returned. If count is negative, every to the right of the final delimiter (counting from the\n",
      "        right) is returned. substring_index performs a case-sensitive match when searching for delim.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        delim : str\n",
      "            delimiter of values.\n",
      "        count : int\n",
      "            number of occurrences.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            substring of given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('a.b.c.d',)], ['s'])\n",
      "        >>> df.select(substring_index(df.s, '.', 2).alias('s')).collect()\n",
      "        [Row(s='a.b')]\n",
      "        >>> df.select(substring_index(df.s, '.', -3).alias('s')).collect()\n",
      "        [Row(s='b.c.d')]\n",
      "    \n",
      "    sum(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the sum of all values in the expression.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(10)\n",
      "        >>> df.select(sum(df[\"id\"])).show()\n",
      "        +-------+\n",
      "        |sum(id)|\n",
      "        +-------+\n",
      "        |     45|\n",
      "        +-------+\n",
      "    \n",
      "    sumDistinct(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the sum of distinct values in the expression.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        .. deprecated:: 3.2.0\n",
      "            Use :func:`sum_distinct` instead.\n",
      "    \n",
      "    sum_distinct(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the sum of distinct values in the expression.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(None,), (1,), (1,), (2,)], schema=[\"numbers\"])\n",
      "        >>> df.select(sum_distinct(col(\"numbers\"))).show()\n",
      "        +---------------------+\n",
      "        |sum(DISTINCT numbers)|\n",
      "        +---------------------+\n",
      "        |                    3|\n",
      "        +---------------------+\n",
      "    \n",
      "    tan(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes tangent of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            angle in radians\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            tangent of the given value, as if computed by `java.lang.Math.tan()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import math\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(tan(lit(math.radians(45)))).first()\n",
      "        Row(TAN(0.78539...)=0.99999...)\n",
      "    \n",
      "    tanh(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes hyperbolic tangent of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            hyperbolic angle\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hyperbolic tangent of the given value\n",
      "            as if computed by `java.lang.Math.tanh()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import math\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(tanh(lit(math.radians(90)))).first()\n",
      "        Row(TANH(1.57079...)=0.91715...)\n",
      "    \n",
      "    timestamp_micros(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Creates timestamp from the number of microseconds since UTC epoch.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            unix time values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            converted timestamp value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
      "        >>> time_df = spark.createDataFrame([(1230219000,)], ['unix_time'])\n",
      "        >>> time_df.select(timestamp_micros(time_df.unix_time).alias('ts')).show()\n",
      "        +--------------------+\n",
      "        |                  ts|\n",
      "        +--------------------+\n",
      "        |1970-01-01 00:20:...|\n",
      "        +--------------------+\n",
      "        >>> time_df.select(timestamp_micros('unix_time').alias('ts')).printSchema()\n",
      "        root\n",
      "         |-- ts: timestamp (nullable = true)\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    timestamp_millis(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Creates timestamp from the number of milliseconds since UTC epoch.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            unix time values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            converted timestamp value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
      "        >>> time_df = spark.createDataFrame([(1230219000,)], ['unix_time'])\n",
      "        >>> time_df.select(timestamp_millis(time_df.unix_time).alias('ts')).show()\n",
      "        +-------------------+\n",
      "        |                 ts|\n",
      "        +-------------------+\n",
      "        |1970-01-15 05:43:39|\n",
      "        +-------------------+\n",
      "        >>> time_df.select(timestamp_millis('unix_time').alias('ts')).printSchema()\n",
      "        root\n",
      "         |-- ts: timestamp (nullable = true)\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    timestamp_seconds(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Converts the number of seconds from the Unix epoch (1970-01-01T00:00:00Z)\n",
      "        to a timestamp.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            unix time values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            converted timestamp value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import timestamp_seconds\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
      "        >>> time_df = spark.createDataFrame([(1230219000,)], ['unix_time'])\n",
      "        >>> time_df.select(timestamp_seconds(time_df.unix_time).alias('ts')).show()\n",
      "        +-------------------+\n",
      "        |                 ts|\n",
      "        +-------------------+\n",
      "        |2008-12-25 15:30:00|\n",
      "        +-------------------+\n",
      "        >>> time_df.select(timestamp_seconds('unix_time').alias('ts')).printSchema()\n",
      "        root\n",
      "         |-- ts: timestamp (nullable = true)\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    toDegrees(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        .. deprecated:: 2.1.0\n",
      "            Use :func:`degrees` instead.\n",
      "    \n",
      "    toRadians(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        .. deprecated:: 2.1.0\n",
      "            Use :func:`radians` instead.\n",
      "    \n",
      "    to_binary(col: 'ColumnOrName', format: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Converts the input `col` to a binary value based on the supplied `format`.\n",
      "        The `format` can be a case-insensitive string literal of \"hex\", \"utf-8\", \"utf8\",\n",
      "        or \"base64\". By default, the binary format for conversion is \"hex\" if\n",
      "        `format` is omitted. The function returns NULL if at least one of the\n",
      "        input parameters is NULL.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        format : :class:`~pyspark.sql.Column` or str, optional\n",
      "            format to use to convert binary values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"abc\",)], [\"e\"])\n",
      "        >>> df.select(to_binary(df.e, lit(\"utf-8\")).alias('r')).collect()\n",
      "        [Row(r=bytearray(b'abc'))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"414243\",)], [\"e\"])\n",
      "        >>> df.select(to_binary(df.e).alias('r')).collect()\n",
      "        [Row(r=bytearray(b'ABC'))]\n",
      "    \n",
      "    to_char(col: 'ColumnOrName', format: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Convert `col` to a string based on the `format`.\n",
      "        Throws an exception if the conversion fails. The format can consist of the following\n",
      "        characters, case insensitive:\n",
      "        '0' or '9': Specifies an expected digit between 0 and 9. A sequence of 0 or 9 in the\n",
      "        format string matches a sequence of digits in the input value, generating a result\n",
      "        string of the same length as the corresponding sequence in the format string.\n",
      "        The result string is left-padded with zeros if the 0/9 sequence comprises more digits\n",
      "        than the matching part of the decimal value, starts with 0, and is before the decimal\n",
      "        point. Otherwise, it is padded with spaces.\n",
      "        '.' or 'D': Specifies the position of the decimal point (optional, only allowed once).\n",
      "        ',' or 'G': Specifies the position of the grouping (thousands) separator (,).\n",
      "        There must be a 0 or 9 to the left and right of each grouping separator.\n",
      "        '$': Specifies the location of the $ currency sign. This character may only be specified once.\n",
      "        'S' or 'MI': Specifies the position of a '-' or '+' sign (optional, only allowed once at\n",
      "        the beginning or end of the format string). Note that 'S' prints '+' for positive\n",
      "        values but 'MI' prints a space.\n",
      "        'PR': Only allowed at the end of the format string; specifies that the result string\n",
      "        will be wrapped by angle brackets if the input value is negative.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        format : :class:`~pyspark.sql.Column` or str, optional\n",
      "            format to use to convert char values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(78.12,)], [\"e\"])\n",
      "        >>> df.select(to_char(df.e, lit(\"$99.99\")).alias('r')).collect()\n",
      "        [Row(r='$78.12')]\n",
      "    \n",
      "    to_csv(col: 'ColumnOrName', options: Optional[Dict[str, str]] = None) -> pyspark.sql.column.Column\n",
      "        Converts a column containing a :class:`StructType` into a CSV string.\n",
      "        Throws an exception, in the case of an unsupported type.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing a struct.\n",
      "        options: dict, optional\n",
      "            options to control converting. accepts the same options as the CSV datasource.\n",
      "            See `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
      "            for the version you use.\n",
      "        \n",
      "            .. # noqa\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a CSV string converted from given :class:`StructType`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> data = [(1, Row(age=2, name='Alice'))]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_csv(df.value).alias(\"csv\")).collect()\n",
      "        [Row(csv='2,Alice')]\n",
      "    \n",
      "    to_date(col: 'ColumnOrName', format: Optional[str] = None) -> pyspark.sql.column.Column\n",
      "        Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.DateType`\n",
      "        using the optionally specified format. Specify formats according to `datetime pattern`_.\n",
      "        By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format\n",
      "        is omitted. Equivalent to ``col.cast(\"date\")``.\n",
      "        \n",
      "        .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "        \n",
      "        .. versionadded:: 2.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to convert.\n",
      "        format: str, optional\n",
      "            format to use to convert date values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            date value as :class:`pyspark.sql.types.DateType` type.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_date(df.t).alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_date(df.t, 'yyyy-MM-dd HH:mm:ss').alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "    \n",
      "    to_json(col: 'ColumnOrName', options: Optional[Dict[str, str]] = None) -> pyspark.sql.column.Column\n",
      "        Converts a column containing a :class:`StructType`, :class:`ArrayType` or a :class:`MapType`\n",
      "        into a JSON string. Throws an exception, in the case of an unsupported type.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing a struct, an array or a map.\n",
      "        options : dict, optional\n",
      "            options to control converting. accepts the same options as the JSON datasource.\n",
      "            See `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n",
      "            for the version you use.\n",
      "            Additionally the function supports the `pretty` option which enables\n",
      "            pretty JSON generation.\n",
      "        \n",
      "            .. # noqa\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            JSON object as string column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> from pyspark.sql.types import *\n",
      "        >>> data = [(1, Row(age=2, name='Alice'))]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='{\"age\":2,\"name\":\"Alice\"}')]\n",
      "        >>> data = [(1, [Row(age=2, name='Alice'), Row(age=3, name='Bob')])]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='[{\"age\":2,\"name\":\"Alice\"},{\"age\":3,\"name\":\"Bob\"}]')]\n",
      "        >>> data = [(1, {\"name\": \"Alice\"})]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='{\"name\":\"Alice\"}')]\n",
      "        >>> data = [(1, [{\"name\": \"Alice\"}, {\"name\": \"Bob\"}])]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='[{\"name\":\"Alice\"},{\"name\":\"Bob\"}]')]\n",
      "        >>> data = [(1, [\"Alice\", \"Bob\"])]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='[\"Alice\",\"Bob\"]')]\n",
      "    \n",
      "    to_number(col: 'ColumnOrName', format: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Convert string 'col' to a number based on the string format 'format'.\n",
      "        Throws an exception if the conversion fails. The format can consist of the following\n",
      "        characters, case insensitive:\n",
      "        '0' or '9': Specifies an expected digit between 0 and 9. A sequence of 0 or 9 in the\n",
      "        format string matches a sequence of digits in the input string. If the 0/9\n",
      "        sequence starts with 0 and is before the decimal point, it can only match a digit\n",
      "        sequence of the same size. Otherwise, if the sequence starts with 9 or is after\n",
      "        the decimal point, it can match a digit sequence that has the same or smaller size.\n",
      "        '.' or 'D': Specifies the position of the decimal point (optional, only allowed once).\n",
      "        ',' or 'G': Specifies the position of the grouping (thousands) separator (,).\n",
      "        There must be a 0 or 9 to the left and right of each grouping separator.\n",
      "        'col' must match the grouping separator relevant for the size of the number.\n",
      "        '$': Specifies the location of the $ currency sign. This character may only be\n",
      "        specified once.\n",
      "        'S' or 'MI': Specifies the position of a '-' or '+' sign (optional, only allowed\n",
      "        once at the beginning or end of the format string). Note that 'S' allows '-'\n",
      "        but 'MI' does not.\n",
      "        'PR': Only allowed at the end of the format string; specifies that 'col' indicates a\n",
      "        negative number with wrapping angled brackets.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        format : :class:`~pyspark.sql.Column` or str, optional\n",
      "            format to use to convert number values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"$78.12\",)], [\"e\"])\n",
      "        >>> df.select(to_number(df.e, lit(\"$99.99\")).alias('r')).collect()\n",
      "        [Row(r=Decimal('78.12'))]\n",
      "    \n",
      "    to_timestamp(col: 'ColumnOrName', format: Optional[str] = None) -> pyspark.sql.column.Column\n",
      "        Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.TimestampType`\n",
      "        using the optionally specified format. Specify formats according to `datetime pattern`_.\n",
      "        By default, it follows casting rules to :class:`pyspark.sql.types.TimestampType` if the format\n",
      "        is omitted. Equivalent to ``col.cast(\"timestamp\")``.\n",
      "        \n",
      "        .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "        \n",
      "        .. versionadded:: 2.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column values to convert.\n",
      "        format: str, optional\n",
      "            format to use to convert timestamp values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            timestamp value as :class:`pyspark.sql.types.TimestampType` type.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_timestamp(df.t).alias('dt')).collect()\n",
      "        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_timestamp(df.t, 'yyyy-MM-dd HH:mm:ss').alias('dt')).collect()\n",
      "        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "    \n",
      "    to_timestamp_ltz(timestamp: 'ColumnOrName', format: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Parses the `timestamp` with the `format` to a timestamp without time zone.\n",
      "        Returns null with invalid input.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        format : :class:`~pyspark.sql.Column` or str, optional\n",
      "            format to use to convert type `TimestampType` timestamp values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"2016-12-31\",)], [\"e\"])\n",
      "        >>> df.select(to_timestamp_ltz(df.e, lit(\"yyyy-MM-dd\")).alias('r')).collect()\n",
      "        ... # doctest: +SKIP\n",
      "        [Row(r=datetime.datetime(2016, 12, 31, 0, 0))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"2016-12-31\",)], [\"e\"])\n",
      "        >>> df.select(to_timestamp_ltz(df.e).alias('r')).collect()\n",
      "        ... # doctest: +SKIP\n",
      "        [Row(r=datetime.datetime(2016, 12, 31, 0, 0))]\n",
      "    \n",
      "    to_timestamp_ntz(timestamp: 'ColumnOrName', format: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Parses the `timestamp` with the `format` to a timestamp without time zone.\n",
      "        Returns null with invalid input.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        format : :class:`~pyspark.sql.Column` or str, optional\n",
      "            format to use to convert type `TimestampNTZType` timestamp values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"2016-04-08\",)], [\"e\"])\n",
      "        >>> df.select(to_timestamp_ntz(df.e, lit(\"yyyy-MM-dd\")).alias('r')).collect()\n",
      "        ... # doctest: +SKIP\n",
      "        [Row(r=datetime.datetime(2016, 4, 8, 0, 0))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"2016-04-08\",)], [\"e\"])\n",
      "        >>> df.select(to_timestamp_ntz(df.e).alias('r')).collect()\n",
      "        ... # doctest: +SKIP\n",
      "        [Row(r=datetime.datetime(2016, 4, 8, 0, 0))]\n",
      "    \n",
      "    to_unix_timestamp(timestamp: 'ColumnOrName', format: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Returns the UNIX timestamp of the given time.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        format : :class:`~pyspark.sql.Column` or str, optional\n",
      "            format to use to convert UNIX timestamp values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> df = spark.createDataFrame([(\"2016-04-08\",)], [\"e\"])\n",
      "        >>> df.select(to_unix_timestamp(df.e, lit(\"yyyy-MM-dd\")).alias('r')).collect()\n",
      "        [Row(r=1460098800)]\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "        \n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> df = spark.createDataFrame([(\"2016-04-08\",)], [\"e\"])\n",
      "        >>> df.select(to_unix_timestamp(df.e).alias('r')).collect()\n",
      "        [Row(r=None)]\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    to_utc_timestamp(timestamp: 'ColumnOrName', tz: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function\n",
      "        takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in the given\n",
      "        timezone, and renders that timestamp as a timestamp in UTC.\n",
      "        \n",
      "        However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not\n",
      "        timezone-agnostic. So in Spark this function just shift the timestamp value from the given\n",
      "        timezone to UTC timezone.\n",
      "        \n",
      "        This function may return confusing result if the input is a string with timezone, e.g.\n",
      "        '2018-03-13T06:18:23+00:00'. The reason is that, Spark firstly cast the string to timestamp\n",
      "        according to the timezone in the string, and finally display the result by converting the\n",
      "        timestamp to string according to the session local timezone.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str\n",
      "            the column that contains timestamps\n",
      "        tz : :class:`~pyspark.sql.Column` or str\n",
      "            A string detailing the time zone ID that the input should be adjusted to. It should\n",
      "            be in the format of either region-based zone IDs or zone offsets. Region IDs must\n",
      "            have the form 'area/city', such as 'America/Los_Angeles'. Zone offsets must be in\n",
      "            the format '(+|-)HH:mm', for example '-08:00' or '+01:00'. Also 'UTC' and 'Z' are\n",
      "            supported as aliases of '+00:00'. Other short names are not recommended to use\n",
      "            because they can be ambiguous.\n",
      "        \n",
      "            .. versionchanged:: 2.4.0\n",
      "               `tz` can take a :class:`~pyspark.sql.Column` containing timezone ID strings.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            timestamp value represented in UTC timezone.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00', 'JST')], ['ts', 'tz'])\n",
      "        >>> df.select(to_utc_timestamp(df.ts, \"PST\").alias('utc_time')).collect()\n",
      "        [Row(utc_time=datetime.datetime(1997, 2, 28, 18, 30))]\n",
      "        >>> df.select(to_utc_timestamp(df.ts, df.tz).alias('utc_time')).collect()\n",
      "        [Row(utc_time=datetime.datetime(1997, 2, 28, 1, 30))]\n",
      "    \n",
      "    to_varchar(col: 'ColumnOrName', format: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Convert `col` to a string based on the `format`.\n",
      "        Throws an exception if the conversion fails. The format can consist of the following\n",
      "        characters, case insensitive:\n",
      "        '0' or '9': Specifies an expected digit between 0 and 9. A sequence of 0 or 9 in the\n",
      "        format string matches a sequence of digits in the input value, generating a result\n",
      "        string of the same length as the corresponding sequence in the format string.\n",
      "        The result string is left-padded with zeros if the 0/9 sequence comprises more digits\n",
      "        than the matching part of the decimal value, starts with 0, and is before the decimal\n",
      "        point. Otherwise, it is padded with spaces.\n",
      "        '.' or 'D': Specifies the position of the decimal point (optional, only allowed once).\n",
      "        ',' or 'G': Specifies the position of the grouping (thousands) separator (,).\n",
      "        There must be a 0 or 9 to the left and right of each grouping separator.\n",
      "        '$': Specifies the location of the $ currency sign. This character may only be specified once.\n",
      "        'S' or 'MI': Specifies the position of a '-' or '+' sign (optional, only allowed once at\n",
      "        the beginning or end of the format string). Note that 'S' prints '+' for positive\n",
      "        values but 'MI' prints a space.\n",
      "        'PR': Only allowed at the end of the format string; specifies that the result string\n",
      "        will be wrapped by angle brackets if the input value is negative.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        format : :class:`~pyspark.sql.Column` or str, optional\n",
      "            format to use to convert char values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(78.12,)], [\"e\"])\n",
      "        >>> df.select(to_varchar(df.e, lit(\"$99.99\")).alias('r')).collect()\n",
      "        [Row(r='$78.12')]\n",
      "    \n",
      "    transform(col: 'ColumnOrName', f: Union[Callable[[pyspark.sql.column.Column], pyspark.sql.column.Column], Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]]) -> pyspark.sql.column.Column\n",
      "        Returns an array of elements after applying a transformation to each element in the input array.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            a function that is applied to each element of the input array.\n",
      "            Can take one of the following forms:\n",
      "        \n",
      "            - Unary ``(x: Column) -> Column: ...``\n",
      "            - Binary ``(x: Column, i: Column) -> Column...``, where the second argument is\n",
      "                a 0-based index of the element.\n",
      "        \n",
      "            and can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a new array of transformed elements.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, [1, 2, 3, 4])], (\"key\", \"values\"))\n",
      "        >>> df.select(transform(\"values\", lambda x: x * 2).alias(\"doubled\")).show()\n",
      "        +------------+\n",
      "        |     doubled|\n",
      "        +------------+\n",
      "        |[2, 4, 6, 8]|\n",
      "        +------------+\n",
      "        \n",
      "        >>> def alternate(x, i):\n",
      "        ...     return when(i % 2 == 0, x).otherwise(-x)\n",
      "        ...\n",
      "        >>> df.select(transform(\"values\", alternate).alias(\"alternated\")).show()\n",
      "        +--------------+\n",
      "        |    alternated|\n",
      "        +--------------+\n",
      "        |[1, -2, 3, -4]|\n",
      "        +--------------+\n",
      "    \n",
      "    transform_keys(col: 'ColumnOrName', f: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]) -> pyspark.sql.column.Column\n",
      "        Applies a function to every key-value pair in a map and returns\n",
      "        a map with the results of those applications as the new keys for the pairs.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            a binary function ``(k: Column, v: Column) -> Column...``\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a new map of enties where new keys were calculated by applying given function to\n",
      "            each key value argument.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, {\"foo\": -2.0, \"bar\": 2.0})], (\"id\", \"data\"))\n",
      "        >>> row = df.select(transform_keys(\n",
      "        ...     \"data\", lambda k, _: upper(k)).alias(\"data_upper\")\n",
      "        ... ).head()\n",
      "        >>> sorted(row[\"data_upper\"].items())\n",
      "        [('BAR', 2.0), ('FOO', -2.0)]\n",
      "    \n",
      "    transform_values(col: 'ColumnOrName', f: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]) -> pyspark.sql.column.Column\n",
      "        Applies a function to every key-value pair in a map and returns\n",
      "        a map with the results of those applications as the new values for the pairs.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            a binary function ``(k: Column, v: Column) -> Column...``\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a new map of enties where new values were calculated by applying given function to\n",
      "            each key value argument.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, {\"IT\": 10.0, \"SALES\": 2.0, \"OPS\": 24.0})], (\"id\", \"data\"))\n",
      "        >>> row = df.select(transform_values(\n",
      "        ...     \"data\", lambda k, v: when(k.isin(\"IT\", \"OPS\"), v + 10.0).otherwise(v)\n",
      "        ... ).alias(\"new_data\")).head()\n",
      "        >>> sorted(row[\"new_data\"].items())\n",
      "        [('IT', 20.0), ('OPS', 34.0), ('SALES', 2.0)]\n",
      "    \n",
      "    translate(srcCol: 'ColumnOrName', matching: str, replace: str) -> pyspark.sql.column.Column\n",
      "        A function translate any character in the `srcCol` by a character in `matching`.\n",
      "        The characters in `replace` is corresponding to the characters in `matching`.\n",
      "        Translation will happen whenever any character in the string is matching with the character\n",
      "        in the `matching`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        srcCol : :class:`~pyspark.sql.Column` or str\n",
      "            Source column or strings\n",
      "        matching : str\n",
      "            matching characters.\n",
      "        replace : str\n",
      "            characters for replacement. If this is shorter than `matching` string then\n",
      "            those chars that don't have replacement will be dropped.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            replaced value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('translate',)], ['a']).select(translate('a', \"rnlt\", \"123\") \\\n",
      "        ...     .alias('r')).collect()\n",
      "        [Row(r='1a2s3ae')]\n",
      "    \n",
      "    trim(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Trim the spaces from both ends for the specified string column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            trimmed values from both sides.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\"   Spark\", \"Spark  \", \" Spark\"], \"STRING\")\n",
      "        >>> df.select(trim(\"value\").alias(\"r\")).withColumn(\"length\", length(\"r\")).show()\n",
      "        +-----+------+\n",
      "        |    r|length|\n",
      "        +-----+------+\n",
      "        |Spark|     5|\n",
      "        |Spark|     5|\n",
      "        |Spark|     5|\n",
      "        +-----+------+\n",
      "    \n",
      "    trunc(date: 'ColumnOrName', format: str) -> pyspark.sql.column.Column\n",
      "        Returns date truncated to the unit specified by the format.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        date : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to truncate.\n",
      "        format : str\n",
      "            'year', 'yyyy', 'yy' to truncate by year,\n",
      "            or 'month', 'mon', 'mm' to truncate by month\n",
      "            Other options are: 'week', 'quarter'\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            truncated date.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28',)], ['d'])\n",
      "        >>> df.select(trunc(df.d, 'year').alias('year')).collect()\n",
      "        [Row(year=datetime.date(1997, 1, 1))]\n",
      "        >>> df.select(trunc(df.d, 'mon').alias('month')).collect()\n",
      "        [Row(month=datetime.date(1997, 2, 1))]\n",
      "    \n",
      "    try_add(left: 'ColumnOrName', right: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the sum of `left`and `right` and the result is null on overflow.\n",
      "        The acceptable input types are the same with the `+` operator.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        left : :class:`~pyspark.sql.Column` or str\n",
      "        right : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1982, 15), (1990, 2)], [\"birth\", \"age\"])\n",
      "        >>> df.select(try_add(df.birth, df.age).alias('r')).collect()\n",
      "        [Row(r=1997), Row(r=1992)]\n",
      "        \n",
      "        >>> from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
      "        >>> schema = StructType([\n",
      "        ...     StructField(\"i\", IntegerType(), True),\n",
      "        ...     StructField(\"d\", StringType(), True),\n",
      "        ... ])\n",
      "        >>> df = spark.createDataFrame([(1, '2015-09-30')], schema)\n",
      "        >>> df = df.select(df.i, to_date(df.d).alias('d'))\n",
      "        >>> df.select(try_add(df.d, df.i).alias('r')).collect()\n",
      "        [Row(r=datetime.date(2015, 10, 1))]\n",
      "        \n",
      "        >>> df.select(try_add(df.d, make_interval(df.i)).alias('r')).collect()\n",
      "        [Row(r=datetime.date(2016, 9, 30))]\n",
      "        \n",
      "        >>> df.select(\n",
      "        ...     try_add(df.d, make_interval(lit(0), lit(0), lit(0), df.i)).alias('r')\n",
      "        ... ).collect()\n",
      "        [Row(r=datetime.date(2015, 10, 1))]\n",
      "        \n",
      "        >>> df.select(\n",
      "        ...     try_add(make_interval(df.i), make_interval(df.i)).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +-------+\n",
      "        |r      |\n",
      "        +-------+\n",
      "        |2 years|\n",
      "        +-------+\n",
      "    \n",
      "    try_aes_decrypt(input: 'ColumnOrName', key: 'ColumnOrName', mode: Optional[ForwardRef('ColumnOrName')] = None, padding: Optional[ForwardRef('ColumnOrName')] = None, aad: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        This is a special version of `aes_decrypt` that performs the same operation,\n",
      "        but returns a NULL value instead of raising an error if the decryption cannot be performed.\n",
      "        Returns a decrypted value of `input` using AES in `mode` with `padding`. Key lengths of 16,\n",
      "        24 and 32 bits are supported. Supported combinations of (`mode`, `padding`) are ('ECB',\n",
      "        'PKCS'), ('GCM', 'NONE') and ('CBC', 'PKCS'). Optional additional authenticated data (AAD) is\n",
      "        only supported for GCM. If provided for encryption, the identical AAD value must be provided\n",
      "        for decryption. The default mode is GCM.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        input : :class:`~pyspark.sql.Column` or str\n",
      "            The binary value to decrypt.\n",
      "        key : :class:`~pyspark.sql.Column` or str\n",
      "            The passphrase to use to decrypt the data.\n",
      "        mode : :class:`~pyspark.sql.Column` or str, optional\n",
      "            Specifies which block cipher mode should be used to decrypt messages. Valid modes: ECB,\n",
      "            GCM, CBC.\n",
      "        padding : :class:`~pyspark.sql.Column` or str, optional\n",
      "            Specifies how to pad messages whose length is not a multiple of the block size. Valid\n",
      "            values: PKCS, NONE, DEFAULT. The DEFAULT padding means PKCS for ECB, NONE for GCM and PKCS\n",
      "            for CBC.\n",
      "        aad : :class:`~pyspark.sql.Column` or str, optional\n",
      "            Optional additional authenticated data. Only supported for GCM mode. This can be any\n",
      "            free-form input and must be provided for both encryption and decryption.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\n",
      "        ...     \"AAAAAAAAAAAAAAAAQiYi+sTLm7KD9UcZ2nlRdYDe/PX4\",\n",
      "        ...     \"abcdefghijklmnop12345678ABCDEFGH\", \"GCM\", \"DEFAULT\",\n",
      "        ...     \"This is an AAD mixed into the input\",)],\n",
      "        ...     [\"input\", \"key\", \"mode\", \"padding\", \"aad\"]\n",
      "        ... )\n",
      "        >>> df.select(try_aes_decrypt(\n",
      "        ...     unbase64(df.input), df.key, df.mode, df.padding, df.aad).alias('r')\n",
      "        ... ).collect()\n",
      "        [Row(r=bytearray(b'Spark'))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\n",
      "        ...     \"AAAAAAAAAAAAAAAAAAAAAPSd4mWyMZ5mhvjiAPQJnfg=\",\n",
      "        ...     \"abcdefghijklmnop12345678ABCDEFGH\", \"CBC\", \"DEFAULT\",)],\n",
      "        ...     [\"input\", \"key\", \"mode\", \"padding\"]\n",
      "        ... )\n",
      "        >>> df.select(try_aes_decrypt(\n",
      "        ...     unbase64(df.input), df.key, df.mode, df.padding).alias('r')\n",
      "        ... ).collect()\n",
      "        [Row(r=bytearray(b'Spark'))]\n",
      "        \n",
      "        >>> df.select(try_aes_decrypt(unbase64(df.input), df.key, df.mode).alias('r')).collect()\n",
      "        [Row(r=bytearray(b'Spark'))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\n",
      "        ...     \"83F16B2AA704794132802D248E6BFD4E380078182D1544813898AC97E709B28A94\",\n",
      "        ...     \"0000111122223333\",)],\n",
      "        ...     [\"input\", \"key\"]\n",
      "        ... )\n",
      "        >>> df.select(try_aes_decrypt(unhex(df.input), df.key).alias('r')).collect()\n",
      "        [Row(r=bytearray(b'Spark'))]\n",
      "    \n",
      "    try_avg(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the mean calculated from values of a group and the result is null on overflow.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [(1982, 15), (1990, 2)], [\"birth\", \"age\"]\n",
      "        ... ).select(sf.try_avg(\"age\")).show()\n",
      "        +------------+\n",
      "        |try_avg(age)|\n",
      "        +------------+\n",
      "        |         8.5|\n",
      "        +------------+\n",
      "    \n",
      "    try_divide(left: 'ColumnOrName', right: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns `dividend`/`divisor`. It always performs floating point division. Its result is\n",
      "        always null if `divisor` is 0.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        left : :class:`~pyspark.sql.Column` or str\n",
      "            dividend\n",
      "        right : :class:`~pyspark.sql.Column` or str\n",
      "            divisor\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(6000, 15), (1990, 2)], [\"a\", \"b\"])\n",
      "        >>> df.select(try_divide(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r=400.0), Row(r=995.0)]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1, 2)], [\"year\", \"month\"])\n",
      "        >>> df.select(\n",
      "        ...     try_divide(make_interval(df.year), df.month).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +--------+\n",
      "        |r       |\n",
      "        +--------+\n",
      "        |6 months|\n",
      "        +--------+\n",
      "        \n",
      "        >>> df.select(\n",
      "        ...     try_divide(make_interval(df.year, df.month), lit(2)).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +--------+\n",
      "        |r       |\n",
      "        +--------+\n",
      "        |7 months|\n",
      "        +--------+\n",
      "        \n",
      "        >>> df.select(\n",
      "        ...     try_divide(make_interval(df.year, df.month), lit(0)).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +----+\n",
      "        |r   |\n",
      "        +----+\n",
      "        |NULL|\n",
      "        +----+\n",
      "    \n",
      "    try_element_at(col: 'ColumnOrName', extraction: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        (array, index) - Returns element of array at given (1-based) index. If Index is 0, Spark will\n",
      "        throw an error. If index < 0, accesses elements from the last to the first. The function\n",
      "        always returns NULL if the index exceeds the length of the array.\n",
      "        \n",
      "        (map, key) - Returns value for given key. The function always returns NULL if the key is not\n",
      "        contained in the map.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array or map\n",
      "        extraction :\n",
      "            index to check for in array or key to check for in map\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], ['data'])\n",
      "        >>> df.select(try_element_at(df.data, lit(1)).alias('r')).collect()\n",
      "        [Row(r='a')]\n",
      "        >>> df.select(try_element_at(df.data, lit(-1)).alias('r')).collect()\n",
      "        [Row(r='c')]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([({\"a\": 1.0, \"b\": 2.0},)], ['data'])\n",
      "        >>> df.select(try_element_at(df.data, lit(\"a\")).alias('r')).collect()\n",
      "        [Row(r=1.0)]\n",
      "    \n",
      "    try_multiply(left: 'ColumnOrName', right: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns `left`*`right` and the result is null on overflow. The acceptable input types are the\n",
      "        same with the `*` operator.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        left : :class:`~pyspark.sql.Column` or str\n",
      "            multiplicand\n",
      "        right : :class:`~pyspark.sql.Column` or str\n",
      "            multiplier\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(6000, 15), (1990, 2)], [\"a\", \"b\"])\n",
      "        >>> df.select(try_multiply(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r=90000), Row(r=3980)]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(2, 3),], [\"a\", \"b\"])\n",
      "        >>> df.select(try_multiply(make_interval(df.a), df.b).alias('r')).show(truncate=False)\n",
      "        +-------+\n",
      "        |r      |\n",
      "        +-------+\n",
      "        |6 years|\n",
      "        +-------+\n",
      "    \n",
      "    try_subtract(left: 'ColumnOrName', right: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns `left`-`right` and the result is null on overflow. The acceptable input types are the\n",
      "        same with the `-` operator.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        left : :class:`~pyspark.sql.Column` or str\n",
      "        right : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(6000, 15), (1990, 2)], [\"a\", \"b\"])\n",
      "        >>> df.select(try_subtract(df.a, df.b).alias('r')).collect()\n",
      "        [Row(r=5985), Row(r=1988)]\n",
      "        \n",
      "        >>> from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
      "        >>> schema = StructType([\n",
      "        ...     StructField(\"i\", IntegerType(), True),\n",
      "        ...     StructField(\"d\", StringType(), True),\n",
      "        ... ])\n",
      "        >>> df = spark.createDataFrame([(1, '2015-09-30')], schema)\n",
      "        >>> df = df.select(df.i, to_date(df.d).alias('d'))\n",
      "        >>> df.select(try_subtract(df.d, df.i).alias('r')).collect()\n",
      "        [Row(r=datetime.date(2015, 9, 29))]\n",
      "        \n",
      "        >>> df.select(try_subtract(df.d, make_interval(df.i)).alias('r')).collect()\n",
      "        [Row(r=datetime.date(2014, 9, 30))]\n",
      "        \n",
      "        >>> df.select(\n",
      "        ...     try_subtract(df.d, make_interval(lit(0), lit(0), lit(0), df.i)).alias('r')\n",
      "        ... ).collect()\n",
      "        [Row(r=datetime.date(2015, 9, 29))]\n",
      "        \n",
      "        >>> df.select(\n",
      "        ...     try_subtract(make_interval(df.i), make_interval(df.i)).alias('r')\n",
      "        ... ).show(truncate=False)\n",
      "        +---------+\n",
      "        |r        |\n",
      "        +---------+\n",
      "        |0 seconds|\n",
      "        +---------+\n",
      "    \n",
      "    try_sum(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the sum calculated from values of a group and the result is null on overflow.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(10).select(sf.try_sum(\"id\")).show()\n",
      "        +-----------+\n",
      "        |try_sum(id)|\n",
      "        +-----------+\n",
      "        |         45|\n",
      "        +-----------+\n",
      "    \n",
      "    try_to_binary(col: 'ColumnOrName', format: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        This is a special version of `to_binary` that performs the same operation, but returns a NULL\n",
      "        value instead of raising an error if the conversion cannot be performed.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        format : :class:`~pyspark.sql.Column` or str, optional\n",
      "            format to use to convert binary values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"abc\",)], [\"e\"])\n",
      "        >>> df.select(try_to_binary(df.e, lit(\"utf-8\")).alias('r')).collect()\n",
      "        [Row(r=bytearray(b'abc'))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"414243\",)], [\"e\"])\n",
      "        >>> df.select(try_to_binary(df.e).alias('r')).collect()\n",
      "        [Row(r=bytearray(b'ABC'))]\n",
      "    \n",
      "    try_to_number(col: 'ColumnOrName', format: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Convert string 'col' to a number based on the string format `format`. Returns NULL if the\n",
      "        string 'col' does not match the expected format. The format follows the same semantics as the\n",
      "        to_number function.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        format : :class:`~pyspark.sql.Column` or str, optional\n",
      "            format to use to convert number values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"$78.12\",)], [\"e\"])\n",
      "        >>> df.select(try_to_number(df.e, lit(\"$99.99\")).alias('r')).collect()\n",
      "        [Row(r=Decimal('78.12'))]\n",
      "    \n",
      "    try_to_timestamp(col: 'ColumnOrName', format: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Parses the `col` with the `format` to a timestamp. The function always\n",
      "        returns null on an invalid input with/without ANSI SQL mode enabled. The result data type is\n",
      "        consistent with the value of configuration `spark.sql.timestampType`.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column values to convert.\n",
      "        format: str, optional\n",
      "            format to use to convert timestamp values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(try_to_timestamp(df.t).alias('dt')).collect()\n",
      "        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "        \n",
      "        >>> df.select(try_to_timestamp(df.t, lit('yyyy-MM-dd HH:mm:ss')).alias('dt')).collect()\n",
      "        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "    \n",
      "    typeof(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Return DDL-formatted type string for the data type of the input.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1,)], [\"a\"])\n",
      "        >>> df.select(typeof(df.a).alias('r')).collect()\n",
      "        [Row(r='bigint')]\n",
      "    \n",
      "    ucase(str: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns `str` with all characters changed to uppercase.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            Input column or strings.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(1).select(sf.ucase(sf.lit(\"Spark\"))).show()\n",
      "        +------------+\n",
      "        |ucase(Spark)|\n",
      "        +------------+\n",
      "        |       SPARK|\n",
      "        +------------+\n",
      "    \n",
      "    udf(f: Union[Callable[..., Any], ForwardRef('DataTypeOrString'), NoneType] = None, returnType: 'DataTypeOrString' = StringType(), *, useArrow: Optional[bool] = None) -> Union[ForwardRef('UserDefinedFunctionLike'), Callable[[Callable[..., Any]], ForwardRef('UserDefinedFunctionLike')]]\n",
      "        Creates a user defined function (UDF).\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            python function if used as a standalone function\n",
      "        returnType : :class:`pyspark.sql.types.DataType` or str\n",
      "            the return type of the user-defined function. The value can be either a\n",
      "            :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "        useArrow : bool or None\n",
      "            whether to use Arrow to optimize the (de)serialization. When it is None, the\n",
      "            Spark config \"spark.sql.execution.pythonUDF.arrow.enabled\" takes effect.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.types import IntegerType\n",
      "        >>> slen = udf(lambda s: len(s), IntegerType())\n",
      "        >>> @udf\n",
      "        ... def to_upper(s):\n",
      "        ...     if s is not None:\n",
      "        ...         return s.upper()\n",
      "        ...\n",
      "        >>> @udf(returnType=IntegerType())\n",
      "        ... def add_one(x):\n",
      "        ...     if x is not None:\n",
      "        ...         return x + 1\n",
      "        ...\n",
      "        >>> df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n",
      "        >>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")).show()\n",
      "        +----------+--------------+------------+\n",
      "        |slen(name)|to_upper(name)|add_one(age)|\n",
      "        +----------+--------------+------------+\n",
      "        |         8|      JOHN DOE|          22|\n",
      "        +----------+--------------+------------+\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The user-defined functions are considered deterministic by default. Due to\n",
      "        optimization, duplicate invocations may be eliminated or the function may even be invoked\n",
      "        more times than it is present in the query. If your function is not deterministic, call\n",
      "        `asNondeterministic` on the user defined function. E.g.:\n",
      "        \n",
      "        >>> from pyspark.sql.types import IntegerType\n",
      "        >>> import random\n",
      "        >>> random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()\n",
      "        \n",
      "        The user-defined functions do not support conditional expressions or short circuiting\n",
      "        in boolean expressions and it ends up with being executed all internally. If the functions\n",
      "        can fail on special rows, the workaround is to incorporate the condition into the functions.\n",
      "        \n",
      "        The user-defined functions do not take keyword arguments on the calling side.\n",
      "    \n",
      "    udtf(cls: Optional[Type] = None, *, returnType: Union[pyspark.sql.types.StructType, str], useArrow: Optional[bool] = None) -> Union[ForwardRef('UserDefinedTableFunction'), Callable[[Type], ForwardRef('UserDefinedTableFunction')]]\n",
      "        Creates a user defined table function (UDTF).\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cls : class\n",
      "            the Python user-defined table function handler class.\n",
      "        returnType : :class:`pyspark.sql.types.StructType` or str\n",
      "            the return type of the user-defined table function. The value can be either a\n",
      "            :class:`pyspark.sql.types.StructType` object or a DDL-formatted struct type string.\n",
      "        useArrow : bool or None, optional\n",
      "            whether to use Arrow to optimize the (de)serializations. When it's set to None, the\n",
      "            Spark config \"spark.sql.execution.pythonUDTF.arrow.enabled\" is used.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Implement the UDTF class and create a UDTF:\n",
      "        \n",
      "        >>> class TestUDTF:\n",
      "        ...     def eval(self, *args: Any):\n",
      "        ...         yield \"hello\", \"world\"\n",
      "        ...\n",
      "        >>> from pyspark.sql.functions import udtf\n",
      "        >>> test_udtf = udtf(TestUDTF, returnType=\"c1: string, c2: string\")\n",
      "        >>> test_udtf().show()\n",
      "        +-----+-----+\n",
      "        |   c1|   c2|\n",
      "        +-----+-----+\n",
      "        |hello|world|\n",
      "        +-----+-----+\n",
      "        \n",
      "        UDTF can also be created using the decorator syntax:\n",
      "        \n",
      "        >>> @udtf(returnType=\"c1: int, c2: int\")\n",
      "        ... class PlusOne:\n",
      "        ...     def eval(self, x: int):\n",
      "        ...         yield x, x + 1\n",
      "        ...\n",
      "        >>> from pyspark.sql.functions import lit\n",
      "        >>> PlusOne(lit(1)).show()\n",
      "        +---+---+\n",
      "        | c1| c2|\n",
      "        +---+---+\n",
      "        |  1|  2|\n",
      "        +---+---+\n",
      "        \n",
      "        Arrow optimization can be explicitly enabled when creating UDTFs:\n",
      "        \n",
      "        >>> @udtf(returnType=\"c1: int, c2: int\", useArrow=True)\n",
      "        ... class ArrowPlusOne:\n",
      "        ...     def eval(self, x: int):\n",
      "        ...         yield x, x + 1\n",
      "        ...\n",
      "        >>> ArrowPlusOne(lit(1)).show()\n",
      "        +---+---+\n",
      "        | c1| c2|\n",
      "        +---+---+\n",
      "        |  1|  2|\n",
      "        +---+---+\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        User-defined table functions (UDTFs) are considered non-deterministic by default.\n",
      "        Use `asDeterministic()` to mark a function as deterministic. E.g.:\n",
      "        \n",
      "        >>> class PlusOne:\n",
      "        ...     def eval(self, a: int):\n",
      "        ...         yield a + 1,\n",
      "        >>> plus_one = udtf(PlusOne, returnType=\"r: int\").asDeterministic()\n",
      "        \n",
      "        Use \"yield\" to produce one row for the UDTF result relation as many times\n",
      "        as needed. In the context of a lateral join, each such result row will be\n",
      "        associated with the most recent input row consumed from the \"eval\" method.\n",
      "        \n",
      "        User-defined table functions are considered opaque to the optimizer by default.\n",
      "        As a result, operations like filters from WHERE clauses or limits from\n",
      "        LIMIT/OFFSET clauses that appear after the UDTF call will execute on the\n",
      "        UDTF's result relation. By the same token, any relations forwarded as input\n",
      "        to UDTFs will plan as full table scans in the absence of any explicit such\n",
      "        filtering or other logic explicitly written in a table subquery surrounding the\n",
      "        provided input relation.\n",
      "        \n",
      "        User-defined table functions do not accept keyword arguments on the calling side.\n",
      "    \n",
      "    unbase64(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Decodes a BASE64 encoded string column and returns it as a binary column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            encoded string value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\"U3Bhcms=\",\n",
      "        ...                             \"UHlTcGFyaw==\",\n",
      "        ...                             \"UGFuZGFzIEFQSQ==\"], \"STRING\")\n",
      "        >>> df.select(unbase64(\"value\")).show()\n",
      "        +--------------------+\n",
      "        |     unbase64(value)|\n",
      "        +--------------------+\n",
      "        |    [53 70 61 72 6B]|\n",
      "        |[50 79 53 70 61 7...|\n",
      "        |[50 61 6E 64 61 7...|\n",
      "        +--------------------+\n",
      "    \n",
      "    unhex(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Inverse of hex. Interprets each pair of characters as a hexadecimal number\n",
      "        and converts to the byte representation of number.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            string representation of given hexadecimal value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('414243',)], ['a']).select(unhex('a')).collect()\n",
      "        [Row(unhex(a)=bytearray(b'ABC'))]\n",
      "    \n",
      "    unix_date(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the number of days since 1970-01-01.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> df = spark.createDataFrame([('1970-01-02',)], ['t'])\n",
      "        >>> df.select(unix_date(to_date(df.t)).alias('n')).collect()\n",
      "        [Row(n=1)]\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    unix_micros(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the number of microseconds since 1970-01-01 00:00:00 UTC.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> df = spark.createDataFrame([('2015-07-22 10:00:00',)], ['t'])\n",
      "        >>> df.select(unix_micros(to_timestamp(df.t)).alias('n')).collect()\n",
      "        [Row(n=1437584400000000)]\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    unix_millis(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the number of milliseconds since 1970-01-01 00:00:00 UTC.\n",
      "        Truncates higher levels of precision.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> df = spark.createDataFrame([('2015-07-22 10:00:00',)], ['t'])\n",
      "        >>> df.select(unix_millis(to_timestamp(df.t)).alias('n')).collect()\n",
      "        [Row(n=1437584400000)]\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    unix_seconds(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the number of seconds since 1970-01-01 00:00:00 UTC.\n",
      "        Truncates higher levels of precision.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> df = spark.createDataFrame([('2015-07-22 10:00:00',)], ['t'])\n",
      "        >>> df.select(unix_seconds(to_timestamp(df.t)).alias('n')).collect()\n",
      "        [Row(n=1437584400)]\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    unix_timestamp(timestamp: Optional[ForwardRef('ColumnOrName')] = None, format: str = 'yyyy-MM-dd HH:mm:ss') -> pyspark.sql.column.Column\n",
      "        Convert time string with given pattern ('yyyy-MM-dd HH:mm:ss', by default)\n",
      "        to Unix time stamp (in seconds), using the default timezone and the default\n",
      "        locale, returns null if failed.\n",
      "        \n",
      "        if `timestamp` is None, then it returns current timestamp.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str, optional\n",
      "            timestamps of string values.\n",
      "        format : str, optional\n",
      "            alternative format to use for converting (default: yyyy-MM-dd HH:mm:ss).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            unix time as long integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> time_df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> time_df.select(unix_timestamp('dt', 'yyyy-MM-dd').alias('unix_time')).collect()\n",
      "        [Row(unix_time=1428476400)]\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    unwrap_udt(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Unwrap UDT data type column into its underlying type.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    upper(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Converts a string expression to upper case.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            upper case values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\"Spark\", \"PySpark\", \"Pandas API\"], \"STRING\")\n",
      "        >>> df.select(upper(\"value\")).show()\n",
      "        +------------+\n",
      "        |upper(value)|\n",
      "        +------------+\n",
      "        |       SPARK|\n",
      "        |     PYSPARK|\n",
      "        |  PANDAS API|\n",
      "        +------------+\n",
      "    \n",
      "    url_decode(str: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Decodes a `str` in 'application/x-www-form-urlencoded' format\n",
      "        using a specific encoding scheme.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string to decode.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"https%3A%2F%2Fspark.apache.org\",)], [\"a\"])\n",
      "        >>> df.select(url_decode(df.a).alias('r')).collect()\n",
      "        [Row(r='https://spark.apache.org')]\n",
      "    \n",
      "    url_encode(str: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Translates a string into 'application/x-www-form-urlencoded' format\n",
      "        using a specific encoding scheme.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            A column of string to encode.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"https://spark.apache.org\",)], [\"a\"])\n",
      "        >>> df.select(url_encode(df.a).alias('r')).collect()\n",
      "        [Row(r='https%3A%2F%2Fspark.apache.org')]\n",
      "    \n",
      "    user() -> pyspark.sql.column.Column\n",
      "        Returns the current database.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.range(1).select(sf.user()).show() # doctest: +SKIP\n",
      "        +--------------+\n",
      "        |current_user()|\n",
      "        +--------------+\n",
      "        | ruifeng.zheng|\n",
      "        +--------------+\n",
      "    \n",
      "    var_pop(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the population variance of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            variance of given column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(6)\n",
      "        >>> df.select(var_pop(df.id)).first()\n",
      "        Row(var_pop(id)=2.91666...)\n",
      "    \n",
      "    var_samp(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the unbiased sample variance of\n",
      "        the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            variance of given column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(6)\n",
      "        >>> df.select(var_samp(df.id)).show()\n",
      "        +------------+\n",
      "        |var_samp(id)|\n",
      "        +------------+\n",
      "        |         3.5|\n",
      "        +------------+\n",
      "    \n",
      "    variance(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: alias for var_samp\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            variance of given column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(6)\n",
      "        >>> df.select(variance(df.id)).show()\n",
      "        +------------+\n",
      "        |var_samp(id)|\n",
      "        +------------+\n",
      "        |         3.5|\n",
      "        +------------+\n",
      "    \n",
      "    version() -> pyspark.sql.column.Column\n",
      "        Returns the Spark version. The string contains 2 fields, the first being a release version\n",
      "        and the second being a git revision.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(version()).show(truncate=False) # doctest: +SKIP\n",
      "        +----------------------------------------------+\n",
      "        |version()                                     |\n",
      "        +----------------------------------------------+\n",
      "        |3.5.0 cafbea5b13623276517a9d716f75745eff91f616|\n",
      "        +----------------------------------------------+\n",
      "    \n",
      "    weekday(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the day of the week for date/timestamp (0 = Monday, 1 = Tuesday, ..., 6 = Sunday).\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the day of the week for date/timestamp (0 = Monday, 1 = Tuesday, ..., 6 = Sunday).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(weekday('dt').alias('day')).show()\n",
      "        +---+\n",
      "        |day|\n",
      "        +---+\n",
      "        |  2|\n",
      "        +---+\n",
      "    \n",
      "    weekofyear(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the week number of a given date as integer.\n",
      "        A week is considered to start on a Monday and week 1 is the first week with more than 3 days,\n",
      "        as defined by ISO 8601\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            `week` of the year for given date as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(weekofyear(df.dt).alias('week')).collect()\n",
      "        [Row(week=15)]\n",
      "    \n",
      "    when(condition: pyspark.sql.column.Column, value: Any) -> pyspark.sql.column.Column\n",
      "        Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      "        If :func:`pyspark.sql.Column.otherwise` is not invoked, None is returned for unmatched\n",
      "        conditions.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        condition : :class:`~pyspark.sql.Column`\n",
      "            a boolean :class:`~pyspark.sql.Column` expression.\n",
      "        value :\n",
      "            a literal value, or a :class:`~pyspark.sql.Column` expression.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            column representing when expression.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(3)\n",
      "        >>> df.select(when(df['id'] == 2, 3).otherwise(4).alias(\"age\")).show()\n",
      "        +---+\n",
      "        |age|\n",
      "        +---+\n",
      "        |  4|\n",
      "        |  4|\n",
      "        |  3|\n",
      "        +---+\n",
      "        \n",
      "        >>> df.select(when(df.id == 2, df.id + 1).alias(\"age\")).show()\n",
      "        +----+\n",
      "        | age|\n",
      "        +----+\n",
      "        |NULL|\n",
      "        |NULL|\n",
      "        |   3|\n",
      "        +----+\n",
      "    \n",
      "    width_bucket(v: 'ColumnOrName', min: 'ColumnOrName', max: 'ColumnOrName', numBucket: Union[ForwardRef('ColumnOrName'), int]) -> pyspark.sql.column.Column\n",
      "        Returns the bucket number into which the value of this expression would fall\n",
      "        after being evaluated. Note that input arguments must follow conditions listed below;\n",
      "        otherwise, the method will return null.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : str or :class:`~pyspark.sql.Column`\n",
      "            value to compute a bucket number in the histogram\n",
      "        min : str or :class:`~pyspark.sql.Column`\n",
      "            minimum value of the histogram\n",
      "        max : str or :class:`~pyspark.sql.Column`\n",
      "            maximum value of the histogram\n",
      "        numBucket : str, :class:`~pyspark.sql.Column` or int\n",
      "            the number of buckets\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the bucket number into which the value would fall after being evaluated\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     (5.3, 0.2, 10.6, 5),\n",
      "        ...     (-2.1, 1.3, 3.4, 3),\n",
      "        ...     (8.1, 0.0, 5.7, 4),\n",
      "        ...     (-0.9, 5.2, 0.5, 2)],\n",
      "        ...     ['v', 'min', 'max', 'n'])\n",
      "        >>> df.select(width_bucket('v', 'min', 'max', 'n')).show()\n",
      "        +----------------------------+\n",
      "        |width_bucket(v, min, max, n)|\n",
      "        +----------------------------+\n",
      "        |                           3|\n",
      "        |                           0|\n",
      "        |                           5|\n",
      "        |                           3|\n",
      "        +----------------------------+\n",
      "    \n",
      "    window(timeColumn: 'ColumnOrName', windowDuration: str, slideDuration: Optional[str] = None, startTime: Optional[str] = None) -> pyspark.sql.column.Column\n",
      "        Bucketize rows into one or more time windows given a timestamp specifying column. Window\n",
      "        starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window\n",
      "        [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in\n",
      "        the order of months are not supported.\n",
      "        \n",
      "        The time column must be of :class:`pyspark.sql.types.TimestampType`.\n",
      "        \n",
      "        Durations are provided as strings, e.g. '1 second', '1 day 12 hours', '2 minutes'. Valid\n",
      "        interval strings are 'week', 'day', 'hour', 'minute', 'second', 'millisecond', 'microsecond'.\n",
      "        If the ``slideDuration`` is not provided, the windows will be tumbling windows.\n",
      "        \n",
      "        The startTime is the offset with respect to 1970-01-01 00:00:00 UTC with which to start\n",
      "        window intervals. For example, in order to have hourly tumbling windows that start 15 minutes\n",
      "        past the hour, e.g. 12:15-13:15, 13:15-14:15... provide `startTime` as `15 minutes`.\n",
      "        \n",
      "        The output column will be a struct called 'window' by default with the nested columns 'start'\n",
      "        and 'end', where 'start' and 'end' will be of :class:`pyspark.sql.types.TimestampType`.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timeColumn : :class:`~pyspark.sql.Column`\n",
      "            The column or the expression to use as the timestamp for windowing by time.\n",
      "            The time column must be of TimestampType or TimestampNTZType.\n",
      "        windowDuration : str\n",
      "            A string specifying the width of the window, e.g. `10 minutes`,\n",
      "            `1 second`. Check `org.apache.spark.unsafe.types.CalendarInterval` for\n",
      "            valid duration identifiers. Note that the duration is a fixed length of\n",
      "            time, and does not vary over time according to a calendar. For example,\n",
      "            `1 day` always means 86,400,000 milliseconds, not a calendar day.\n",
      "        slideDuration : str, optional\n",
      "            A new window will be generated every `slideDuration`. Must be less than\n",
      "            or equal to the `windowDuration`. Check\n",
      "            `org.apache.spark.unsafe.types.CalendarInterval` for valid duration\n",
      "            identifiers. This duration is likewise absolute, and does not vary\n",
      "            according to a calendar.\n",
      "        startTime : str, optional\n",
      "            The offset with respect to 1970-01-01 00:00:00 UTC with which to start\n",
      "            window intervals. For example, in order to have hourly tumbling windows that\n",
      "            start 15 minutes past the hour, e.g. 12:15-13:15, 13:15-14:15... provide\n",
      "            `startTime` as `15 minutes`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import datetime\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(datetime.datetime(2016, 3, 11, 9, 0, 7), 1)],\n",
      "        ... ).toDF(\"date\", \"val\")\n",
      "        >>> w = df.groupBy(window(\"date\", \"5 seconds\")).agg(sum(\"val\").alias(\"sum\"))\n",
      "        >>> w.select(w.window.start.cast(\"string\").alias(\"start\"),\n",
      "        ...          w.window.end.cast(\"string\").alias(\"end\"), \"sum\").collect()\n",
      "        [Row(start='2016-03-11 09:00:05', end='2016-03-11 09:00:10', sum=1)]\n",
      "    \n",
      "    window_time(windowColumn: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the event time from a window column. The column window values are produced\n",
      "        by window aggregating operators and are of type `STRUCT<start: TIMESTAMP, end: TIMESTAMP>`\n",
      "        where start is inclusive and end is exclusive. The event time of records produced by window\n",
      "        aggregating operators can be computed as ``window_time(window)`` and are\n",
      "        ``window.end - lit(1).alias(\"microsecond\")`` (as microsecond is the minimal supported event\n",
      "        time precision). The window column must be one produced by a window aggregating operator.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        windowColumn : :class:`~pyspark.sql.Column`\n",
      "            The window column of a window aggregate records.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Supports Spark Connect.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import datetime\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(datetime.datetime(2016, 3, 11, 9, 0, 7), 1)],\n",
      "        ... ).toDF(\"date\", \"val\")\n",
      "        \n",
      "        Group the data into 5 second time windows and aggregate as sum.\n",
      "        \n",
      "        >>> w = df.groupBy(window(\"date\", \"5 seconds\")).agg(sum(\"val\").alias(\"sum\"))\n",
      "        \n",
      "        Extract the window event time using the window_time function.\n",
      "        \n",
      "        >>> w.select(\n",
      "        ...     w.window.end.cast(\"string\").alias(\"end\"),\n",
      "        ...     window_time(w.window).cast(\"string\").alias(\"window_time\"),\n",
      "        ...     \"sum\"\n",
      "        ... ).collect()\n",
      "        [Row(end='2016-03-11 09:00:10', window_time='2016-03-11 09:00:09.999999', sum=1)]\n",
      "    \n",
      "    xpath(xml: 'ColumnOrName', path: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a string array of values within the nodes of xml that match the XPath expression.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [('<a><b>b1</b><b>b2</b><b>b3</b><c>c1</c><c>c2</c></a>',)], ['x'])\n",
      "        >>> df.select(xpath(df.x, lit('a/b/text()')).alias('r')).collect()\n",
      "        [Row(r=['b1', 'b2', 'b3'])]\n",
      "    \n",
      "    xpath_boolean(xml: 'ColumnOrName', path: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns true if the XPath expression evaluates to true, or if a matching node is found.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('<a><b>1</b></a>',)], ['x'])\n",
      "        >>> df.select(xpath_boolean(df.x, lit('a/b')).alias('r')).collect()\n",
      "        [Row(r=True)]\n",
      "    \n",
      "    xpath_double(xml: 'ColumnOrName', path: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a double value, the value zero if no match is found,\n",
      "        or NaN if a match is found but the value is non-numeric.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('<a><b>1</b><b>2</b></a>',)], ['x'])\n",
      "        >>> df.select(xpath_double(df.x, lit('sum(a/b)')).alias('r')).collect()\n",
      "        [Row(r=3.0)]\n",
      "    \n",
      "    xpath_float(xml: 'ColumnOrName', path: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a float value, the value zero if no match is found,\n",
      "        or NaN if a match is found but the value is non-numeric.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('<a><b>1</b><b>2</b></a>',)], ['x'])\n",
      "        >>> df.select(xpath_float(df.x, lit('sum(a/b)')).alias('r')).collect()\n",
      "        [Row(r=3.0)]\n",
      "    \n",
      "    xpath_int(xml: 'ColumnOrName', path: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns an integer value, or the value zero if no match is found,\n",
      "        or a match is found but the value is non-numeric.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('<a><b>1</b><b>2</b></a>',)], ['x'])\n",
      "        >>> df.select(xpath_int(df.x, lit('sum(a/b)')).alias('r')).collect()\n",
      "        [Row(r=3)]\n",
      "    \n",
      "    xpath_long(xml: 'ColumnOrName', path: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a long integer value, or the value zero if no match is found,\n",
      "        or a match is found but the value is non-numeric.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('<a><b>1</b><b>2</b></a>',)], ['x'])\n",
      "        >>> df.select(xpath_long(df.x, lit('sum(a/b)')).alias('r')).collect()\n",
      "        [Row(r=3)]\n",
      "    \n",
      "    xpath_number(xml: 'ColumnOrName', path: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a double value, the value zero if no match is found,\n",
      "        or NaN if a match is found but the value is non-numeric.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import pyspark.sql.functions as sf\n",
      "        >>> spark.createDataFrame(\n",
      "        ...     [('<a><b>1</b><b>2</b></a>',)], ['x']\n",
      "        ... ).select(sf.xpath_number('x', sf.lit('sum(a/b)'))).show()\n",
      "        +-------------------------+\n",
      "        |xpath_number(x, sum(a/b))|\n",
      "        +-------------------------+\n",
      "        |                      3.0|\n",
      "        +-------------------------+\n",
      "    \n",
      "    xpath_short(xml: 'ColumnOrName', path: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a short integer value, or the value zero if no match is found,\n",
      "        or a match is found but the value is non-numeric.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('<a><b>1</b><b>2</b></a>',)], ['x'])\n",
      "        >>> df.select(xpath_short(df.x, lit('sum(a/b)')).alias('r')).collect()\n",
      "        [Row(r=3)]\n",
      "    \n",
      "    xpath_string(xml: 'ColumnOrName', path: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the text contents of the first xml node that matches the XPath expression.\n",
      "        \n",
      "        .. versionadded:: 3.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('<a><b>b</b><c>cc</c></a>',)], ['x'])\n",
      "        >>> df.select(xpath_string(df.x, lit('a/c')).alias('r')).collect()\n",
      "        [Row(r='cc')]\n",
      "    \n",
      "    xxhash64(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Calculates the hash code of given columns using the 64-bit variant of the xxHash algorithm,\n",
      "        and returns the result as a long column. The hash computation uses an initial seed of 42.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            one or more columns to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hash value as long column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('ABC', 'DEF')], ['c1', 'c2'])\n",
      "        \n",
      "        Hash for one column\n",
      "        \n",
      "        >>> df.select(xxhash64('c1').alias('hash')).show()\n",
      "        +-------------------+\n",
      "        |               hash|\n",
      "        +-------------------+\n",
      "        |4105715581806190027|\n",
      "        +-------------------+\n",
      "        \n",
      "        Two or more columns\n",
      "        \n",
      "        >>> df.select(xxhash64('c1', 'c2').alias('hash')).show()\n",
      "        +-------------------+\n",
      "        |               hash|\n",
      "        +-------------------+\n",
      "        |3233247871021311208|\n",
      "        +-------------------+\n",
      "    \n",
      "    year(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the year of a given date/timestamp as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            year part of the date/timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(year('dt').alias('year')).collect()\n",
      "        [Row(year=2015)]\n",
      "    \n",
      "    years(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Partition transform function: A transform for timestamps and dates\n",
      "        to partition data into years.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date or timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            data partitioned by years.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").partitionedBy(  # doctest: +SKIP\n",
      "        ...     years(\"ts\")\n",
      "        ... ).createOrReplace()\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function can be used only in combination with\n",
      "        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n",
      "        method of the `DataFrameWriterV2`.\n",
      "    \n",
      "    zip_with(left: 'ColumnOrName', right: 'ColumnOrName', f: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]) -> pyspark.sql.column.Column\n",
      "        Merge two given arrays, element-wise, into a single array using a function.\n",
      "        If one array is shorter, nulls are appended at the end to match the length of the longer\n",
      "        array, before applying the function.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        left : :class:`~pyspark.sql.Column` or str\n",
      "            name of the first column or expression\n",
      "        right : :class:`~pyspark.sql.Column` or str\n",
      "            name of the second column or expression\n",
      "        f : function\n",
      "            a binary function ``(x1: Column, x2: Column) -> Column...``\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            array of calculated values derived by applying given function to each pair of arguments.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, [1, 3, 5, 8], [0, 2, 4, 6])], (\"id\", \"xs\", \"ys\"))\n",
      "        >>> df.select(zip_with(\"xs\", \"ys\", lambda x, y: x ** y).alias(\"powers\")).show(truncate=False)\n",
      "        +---------------------------+\n",
      "        |powers                     |\n",
      "        +---------------------------+\n",
      "        |[1.0, 9.0, 625.0, 262144.0]|\n",
      "        +---------------------------+\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1, [\"foo\", \"bar\"], [1, 2, 3])], (\"id\", \"xs\", \"ys\"))\n",
      "        >>> df.select(zip_with(\"xs\", \"ys\", lambda x, y: concat_ws(\"_\", x, y)).alias(\"xs_ys\")).show()\n",
      "        +-----------------+\n",
      "        |            xs_ys|\n",
      "        +-----------------+\n",
      "        |[foo_1, bar_2, 3]|\n",
      "        +-----------------+\n",
      "\n",
      "DATA\n",
      "    Callable = typing.Callable\n",
      "        Deprecated alias to collections.abc.Callable.\n",
      "        \n",
      "        Callable[[int], str] signifies a function that takes a single\n",
      "        parameter of type int and returns a str.\n",
      "        \n",
      "        The subscription syntax must always be used with exactly two\n",
      "        values: the argument list and the return type.\n",
      "        The argument list must be a list of types, a ParamSpec,\n",
      "        Concatenate or ellipsis. The return type must be a single type.\n",
      "        \n",
      "        There is no syntax to indicate optional or keyword arguments;\n",
      "        such function types are rarely used as callback types.\n",
      "    \n",
      "    Dict = typing.Dict\n",
      "        A generic version of dict.\n",
      "    \n",
      "    Iterable = typing.Iterable\n",
      "        A generic version of collections.abc.Iterable.\n",
      "    \n",
      "    List = typing.List\n",
      "        A generic version of list.\n",
      "    \n",
      "    Optional = typing.Optional\n",
      "        Optional[X] is equivalent to Union[X, None].\n",
      "    \n",
      "    TYPE_CHECKING = False\n",
      "    Tuple = typing.Tuple\n",
      "        Deprecated alias to builtins.tuple.\n",
      "        \n",
      "        Tuple[X, Y] is the cross-product type of X and Y.\n",
      "        \n",
      "        Example: Tuple[T1, T2] is a tuple of two elements corresponding\n",
      "        to type variables T1 and T2.  Tuple[int, float, str] is a tuple\n",
      "        of an int, a float and a string.\n",
      "        \n",
      "        To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].\n",
      "    \n",
      "    Type = typing.Type\n",
      "        Deprecated alias to builtins.type.\n",
      "        \n",
      "        builtins.type or typing.Type can be used to annotate class objects.\n",
      "        For example, suppose we have the following classes::\n",
      "        \n",
      "            class User: ...  # Abstract base for User classes\n",
      "            class BasicUser(User): ...\n",
      "            class ProUser(User): ...\n",
      "            class TeamUser(User): ...\n",
      "        \n",
      "        And a function that takes a class argument that's a subclass of\n",
      "        User and returns an instance of the corresponding class::\n",
      "        \n",
      "            U = TypeVar('U', bound=User)\n",
      "            def new_user(user_class: Type[U]) -> U:\n",
      "                user = user_class()\n",
      "                # (Here we could write the user object to a database)\n",
      "                return user\n",
      "        \n",
      "            joe = new_user(BasicUser)\n",
      "        \n",
      "        At this point the type checker knows that joe has type BasicUser.\n",
      "    \n",
      "    Union = typing.Union\n",
      "        Union type; Union[X, Y] means either X or Y.\n",
      "        \n",
      "        On Python 3.10 and higher, the | operator\n",
      "        can also be used to denote unions;\n",
      "        X | Y means the same thing to the type checker as Union[X, Y].\n",
      "        \n",
      "        To define a union, use e.g. Union[int, str]. Details:\n",
      "        - The arguments must be types and there must be at least one.\n",
      "        - None as an argument is a special case and is replaced by\n",
      "          type(None).\n",
      "        - Unions of unions are flattened, e.g.::\n",
      "        \n",
      "            assert Union[Union[int, str], float] == Union[int, str, float]\n",
      "        \n",
      "        - Unions of a single argument vanish, e.g.::\n",
      "        \n",
      "            assert Union[int] == int  # The constructor actually returns int\n",
      "        \n",
      "        - Redundant arguments are skipped, e.g.::\n",
      "        \n",
      "            assert Union[int, str, int] == Union[int, str]\n",
      "        \n",
      "        - When comparing unions, the argument order is ignored, e.g.::\n",
      "        \n",
      "            assert Union[int, str] == Union[str, int]\n",
      "        \n",
      "        - You cannot subclass or instantiate a union.\n",
      "        - You can use Optional[X] as a shorthand for Union[X, None].\n",
      "    \n",
      "    ValuesView = typing.ValuesView\n",
      "        A generic version of collections.abc.ValuesView.\n",
      "    \n",
      "    has_numpy = False\n",
      "\n",
      "FILE\n",
      "    c:\\users\\jeff\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\pyspark\\sql\\functions.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lendo um arquivo parquet\n",
    "df = spark.read.parquet('./DATASETS/LOGINS.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>cpf</th><th>email</th><th>senha</th><th>data_de_nascimento</th><th>estado</th><th>data_cadastro</th><th>ipv4</th><th>cor_favorita</th><th>profissao</th><th>telefone</th><th>pais</th></tr>\n",
       "<tr><td>981.507.362-12</td><td>pedro-lucas53@gma...</td><td>+7^7E%xFBc</td><td>2006-12-18</td><td>RR</td><td>2023-02-26</td><td>99.107.250.210</td><td>Roxo</td><td>Jogador De Golfe</td><td>31 7785-4046</td><td>Brasil</td></tr>\n",
       "<tr><td>493.705.168-75</td><td>rezendeisaac@hotm...</td><td>_O_2GRnGOe</td><td>1992-06-17</td><td>GO</td><td>2023-02-16</td><td>197.11.26.213</td><td>Ciano</td><td>Atleta De Arremes...</td><td>(031) 0803-6753</td><td>Brasil</td></tr>\n",
       "<tr><td>398.471.625-73</td><td>felipepires@uol.c...</td><td>*Aw5EOAvy9</td><td>1921-11-11</td><td>MG</td><td>2023-01-02</td><td>181.90.63.58</td><td>Azul</td><td>Papiloscopista</td><td>11 9674-0553</td><td>Brasil</td></tr>\n",
       "<tr><td>092.618.354-06</td><td>stellamoraes@bol....</td><td>mw0AWYAs#s</td><td>2021-06-01</td><td>AC</td><td>2023-01-08</td><td>26.121.127.94</td><td>Marrom</td><td>Aeromo&ccedil;a</td><td>+55 (071) 3033 9177</td><td>Brasil</td></tr>\n",
       "<tr><td>509.427.136-99</td><td>wcarvalho@ig.com.br</td><td>pGD%!2Pq5X</td><td>1969-10-28</td><td>AP</td><td>2023-02-14</td><td>76.184.52.163</td><td>Laranja</td><td>Fonoaudi&oacute;logo</td><td>+55 (071) 6272 2468</td><td>Brasil</td></tr>\n",
       "<tr><td>218.795.460-94</td><td>da-conceicaodavi-...</td><td>uhBbFxPA&amp;9</td><td>1986-05-19</td><td>MG</td><td>2023-03-07</td><td>192.93.0.24</td><td>Rosa</td><td>Taxista</td><td>+55 84 0652 9691</td><td>Brasil</td></tr>\n",
       "<tr><td>715.836.940-48</td><td>efreitas@bol.com.br</td><td>s#q9VZt&amp;xl</td><td>2018-04-20</td><td>MG</td><td>2023-01-13</td><td>76.251.188.148</td><td>Branco</td><td>Produtor De Audio...</td><td>+55 (084) 1363 0052</td><td>Brasil</td></tr>\n",
       "<tr><td>475.698.032-56</td><td>wnunes@bol.com.br</td><td>_8az1W%n7g</td><td>1996-05-12</td><td>SE</td><td>2023-02-04</td><td>139.196.176.154</td><td>Azul</td><td>Cadeirinha</td><td>(071) 1640-3388</td><td>Brasil</td></tr>\n",
       "<tr><td>217.639.540-99</td><td>jribeiro@bol.com.br</td><td>MEf1X7fj_0</td><td>2021-10-05</td><td>PA</td><td>2023-03-02</td><td>71.22.224.5</td><td>Marrom</td><td>Ge&oacute;logo</td><td>21 1432 4092</td><td>Brasil</td></tr>\n",
       "<tr><td>261.938.750-77</td><td>murilo05@gmail.com</td><td>Te&amp;gO7GkKs</td><td>1917-01-05</td><td>MT</td><td>2023-02-21</td><td>136.54.123.165</td><td>Marrom</td><td>T&eacute;cnico De Som</td><td>+55 (084) 5878-3346</td><td>Brasil</td></tr>\n",
       "<tr><td>520.831.796-68</td><td>joaquim57@ig.com.br</td><td>&amp;2E1NY+ARc</td><td>1912-05-25</td><td>BA</td><td>2023-01-25</td><td>78.196.255.126</td><td>Rosa</td><td>Esteticista</td><td>41 7914-3753</td><td>Brasil</td></tr>\n",
       "<tr><td>413.087.526-44</td><td>alexiada-rocha@ig...</td><td>@f@!Z!2c*2</td><td>1920-05-26</td><td>MS</td><td>2023-02-18</td><td>182.61.65.201</td><td>Laranja</td><td>Microfonista</td><td>(021) 3739-2944</td><td>Brasil</td></tr>\n",
       "<tr><td>509.287.143-14</td><td>pmendes@gmail.com</td><td>M+^XDBfe(2</td><td>1938-03-11</td><td>RR</td><td>2023-01-19</td><td>61.234.208.17</td><td>Verde Claro</td><td>Fiscal</td><td>+55 (084) 5940-1932</td><td>Brasil</td></tr>\n",
       "<tr><td>563.170.492-70</td><td>renan46@bol.com.br</td><td>Rp%2pVqfe$</td><td>1922-09-01</td><td>PA</td><td>2023-02-01</td><td>122.203.83.177</td><td>Verde Escuro</td><td>Contabilista</td><td>+55 41 6248 5773</td><td>Brasil</td></tr>\n",
       "<tr><td>098.712.346-78</td><td>manuelada-rosa@ho...</td><td>#hEIEOztQ3</td><td>2013-01-10</td><td>SE</td><td>2023-01-07</td><td>175.18.73.211</td><td>Violeta</td><td>Seguidor De Compras</td><td>51 0278 0564</td><td>Brasil</td></tr>\n",
       "<tr><td>345.709.261-34</td><td>usales@ig.com.br</td><td>b_8xaY$ozJ</td><td>1992-10-16</td><td>AM</td><td>2023-01-05</td><td>97.93.29.75</td><td>Rosa</td><td>Almirante</td><td>(084) 8993-4521</td><td>Brasil</td></tr>\n",
       "<tr><td>318.254.906-51</td><td>pereiranicolas@ho...</td><td>YA9I85Wb+2</td><td>1945-11-18</td><td>MA</td><td>2023-01-24</td><td>182.195.33.137</td><td>Amarelo</td><td>Consultor De Moda</td><td>(084) 4591 3795</td><td>Brasil</td></tr>\n",
       "<tr><td>610.287.453-62</td><td>cardosolivia@ig.c...</td><td>yXkDW7Ebh*</td><td>1915-11-15</td><td>GO</td><td>2023-01-22</td><td>195.194.63.184</td><td>Laranja</td><td>Barbeiro</td><td>+55 61 8967-9563</td><td>Brasil</td></tr>\n",
       "<tr><td>615.790.842-49</td><td>brenda39@ig.com.br</td><td>XS$84Hpsnw</td><td>1964-02-01</td><td>RJ</td><td>2023-02-16</td><td>21.252.226.29</td><td>Branco</td><td>Promotor De Vendas</td><td>+55 (021) 4751 2004</td><td>Brasil</td></tr>\n",
       "<tr><td>624.095.138-24</td><td>luizapeixoto@gmai...</td><td>TbKy82Kda$</td><td>1959-02-01</td><td>DF</td><td>2023-02-25</td><td>105.107.23.13</td><td>Ciano</td><td>Implantodontista</td><td>84 1293 1906</td><td>Brasil</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------+--------------------+----------+------------------+------+-------------+---------------+------------+--------------------+-------------------+------+\n",
       "|           cpf|               email|     senha|data_de_nascimento|estado|data_cadastro|           ipv4|cor_favorita|           profissao|           telefone|  pais|\n",
       "+--------------+--------------------+----------+------------------+------+-------------+---------------+------------+--------------------+-------------------+------+\n",
       "|981.507.362-12|pedro-lucas53@gma...|+7^7E%xFBc|        2006-12-18|    RR|   2023-02-26| 99.107.250.210|        Roxo|    Jogador De Golfe|       31 7785-4046|Brasil|\n",
       "|493.705.168-75|rezendeisaac@hotm...|_O_2GRnGOe|        1992-06-17|    GO|   2023-02-16|  197.11.26.213|       Ciano|Atleta De Arremes...|    (031) 0803-6753|Brasil|\n",
       "|398.471.625-73|felipepires@uol.c...|*Aw5EOAvy9|        1921-11-11|    MG|   2023-01-02|   181.90.63.58|        Azul|      Papiloscopista|       11 9674-0553|Brasil|\n",
       "|092.618.354-06|stellamoraes@bol....|mw0AWYAs#s|        2021-06-01|    AC|   2023-01-08|  26.121.127.94|      Marrom|            Aeromoça|+55 (071) 3033 9177|Brasil|\n",
       "|509.427.136-99| wcarvalho@ig.com.br|pGD%!2Pq5X|        1969-10-28|    AP|   2023-02-14|  76.184.52.163|     Laranja|       Fonoaudiólogo|+55 (071) 6272 2468|Brasil|\n",
       "|218.795.460-94|da-conceicaodavi-...|uhBbFxPA&9|        1986-05-19|    MG|   2023-03-07|    192.93.0.24|        Rosa|             Taxista|   +55 84 0652 9691|Brasil|\n",
       "|715.836.940-48| efreitas@bol.com.br|s#q9VZt&xl|        2018-04-20|    MG|   2023-01-13| 76.251.188.148|      Branco|Produtor De Audio...|+55 (084) 1363 0052|Brasil|\n",
       "|475.698.032-56|   wnunes@bol.com.br|_8az1W%n7g|        1996-05-12|    SE|   2023-02-04|139.196.176.154|        Azul|          Cadeirinha|    (071) 1640-3388|Brasil|\n",
       "|217.639.540-99| jribeiro@bol.com.br|MEf1X7fj_0|        2021-10-05|    PA|   2023-03-02|    71.22.224.5|      Marrom|             Geólogo|       21 1432 4092|Brasil|\n",
       "|261.938.750-77|  murilo05@gmail.com|Te&gO7GkKs|        1917-01-05|    MT|   2023-02-21| 136.54.123.165|      Marrom|      Técnico De Som|+55 (084) 5878-3346|Brasil|\n",
       "|520.831.796-68| joaquim57@ig.com.br|&2E1NY+ARc|        1912-05-25|    BA|   2023-01-25| 78.196.255.126|        Rosa|         Esteticista|       41 7914-3753|Brasil|\n",
       "|413.087.526-44|alexiada-rocha@ig...|@f@!Z!2c*2|        1920-05-26|    MS|   2023-02-18|  182.61.65.201|     Laranja|        Microfonista|    (021) 3739-2944|Brasil|\n",
       "|509.287.143-14|   pmendes@gmail.com|M+^XDBfe(2|        1938-03-11|    RR|   2023-01-19|  61.234.208.17| Verde Claro|              Fiscal|+55 (084) 5940-1932|Brasil|\n",
       "|563.170.492-70|  renan46@bol.com.br|Rp%2pVqfe$|        1922-09-01|    PA|   2023-02-01| 122.203.83.177|Verde Escuro|        Contabilista|   +55 41 6248 5773|Brasil|\n",
       "|098.712.346-78|manuelada-rosa@ho...|#hEIEOztQ3|        2013-01-10|    SE|   2023-01-07|  175.18.73.211|     Violeta| Seguidor De Compras|       51 0278 0564|Brasil|\n",
       "|345.709.261-34|    usales@ig.com.br|b_8xaY$ozJ|        1992-10-16|    AM|   2023-01-05|    97.93.29.75|        Rosa|           Almirante|    (084) 8993-4521|Brasil|\n",
       "|318.254.906-51|pereiranicolas@ho...|YA9I85Wb+2|        1945-11-18|    MA|   2023-01-24| 182.195.33.137|     Amarelo|   Consultor De Moda|    (084) 4591 3795|Brasil|\n",
       "|610.287.453-62|cardosolivia@ig.c...|yXkDW7Ebh*|        1915-11-15|    GO|   2023-01-22| 195.194.63.184|     Laranja|            Barbeiro|   +55 61 8967-9563|Brasil|\n",
       "|615.790.842-49|  brenda39@ig.com.br|XS$84Hpsnw|        1964-02-01|    RJ|   2023-02-16|  21.252.226.29|      Branco|  Promotor De Vendas|+55 (021) 4751 2004|Brasil|\n",
       "|624.095.138-24|luizapeixoto@gmai...|TbKy82Kda$|        1959-02-01|    DF|   2023-02-25|  105.107.23.13|       Ciano|    Implantodontista|       84 1293 1906|Brasil|\n",
       "+--------------+--------------------+----------+------------------+------+-------------+---------------+------------+--------------------+-------------------+------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cria uma nova coluna com o valor literal para todas as linhas\n",
    "df.withColumn('pais', F.lit('Brasil'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>cpf</th><th>email</th><th>senha</th><th>data_de_nascimento</th><th>estado</th><th>data_cadastro</th><th>ipv4</th><th>cor_favorita</th><th>profissao</th><th>telefone</th><th>pais</th><th>sigla_estado</th><th>num</th></tr>\n",
       "<tr><td>981.507.362-12</td><td>pedro-lucas53@gma...</td><td>+7^7E%xFBc</td><td>2006-12-18</td><td>RR</td><td>2023-02-26</td><td>99.107.250.210</td><td>Roxo</td><td>Jogador De Golfe</td><td>31 7785-4046</td><td>Brasil</td><td>RR</td><td>5</td></tr>\n",
       "<tr><td>493.705.168-75</td><td>rezendeisaac@hotm...</td><td>_O_2GRnGOe</td><td>1992-06-17</td><td>GO</td><td>2023-02-16</td><td>197.11.26.213</td><td>Ciano</td><td>Atleta De Arremes...</td><td>(031) 0803-6753</td><td>Brasil</td><td>GO</td><td>5</td></tr>\n",
       "<tr><td>398.471.625-73</td><td>felipepires@uol.c...</td><td>*Aw5EOAvy9</td><td>1921-11-11</td><td>MG</td><td>2023-01-02</td><td>181.90.63.58</td><td>Azul</td><td>Papiloscopista</td><td>11 9674-0553</td><td>Brasil</td><td>MG</td><td>5</td></tr>\n",
       "<tr><td>092.618.354-06</td><td>stellamoraes@bol....</td><td>mw0AWYAs#s</td><td>2021-06-01</td><td>AC</td><td>2023-01-08</td><td>26.121.127.94</td><td>Marrom</td><td>Aeromo&ccedil;a</td><td>+55 (071) 3033 9177</td><td>Brasil</td><td>AC</td><td>5</td></tr>\n",
       "<tr><td>509.427.136-99</td><td>wcarvalho@ig.com.br</td><td>pGD%!2Pq5X</td><td>1969-10-28</td><td>AP</td><td>2023-02-14</td><td>76.184.52.163</td><td>Laranja</td><td>Fonoaudi&oacute;logo</td><td>+55 (071) 6272 2468</td><td>Brasil</td><td>AP</td><td>5</td></tr>\n",
       "<tr><td>218.795.460-94</td><td>da-conceicaodavi-...</td><td>uhBbFxPA&amp;9</td><td>1986-05-19</td><td>MG</td><td>2023-03-07</td><td>192.93.0.24</td><td>Rosa</td><td>Taxista</td><td>+55 84 0652 9691</td><td>Brasil</td><td>MG</td><td>5</td></tr>\n",
       "<tr><td>715.836.940-48</td><td>efreitas@bol.com.br</td><td>s#q9VZt&amp;xl</td><td>2018-04-20</td><td>MG</td><td>2023-01-13</td><td>76.251.188.148</td><td>Branco</td><td>Produtor De Audio...</td><td>+55 (084) 1363 0052</td><td>Brasil</td><td>MG</td><td>5</td></tr>\n",
       "<tr><td>475.698.032-56</td><td>wnunes@bol.com.br</td><td>_8az1W%n7g</td><td>1996-05-12</td><td>SE</td><td>2023-02-04</td><td>139.196.176.154</td><td>Azul</td><td>Cadeirinha</td><td>(071) 1640-3388</td><td>Brasil</td><td>SE</td><td>5</td></tr>\n",
       "<tr><td>217.639.540-99</td><td>jribeiro@bol.com.br</td><td>MEf1X7fj_0</td><td>2021-10-05</td><td>PA</td><td>2023-03-02</td><td>71.22.224.5</td><td>Marrom</td><td>Ge&oacute;logo</td><td>21 1432 4092</td><td>Brasil</td><td>PA</td><td>5</td></tr>\n",
       "<tr><td>261.938.750-77</td><td>murilo05@gmail.com</td><td>Te&amp;gO7GkKs</td><td>1917-01-05</td><td>MT</td><td>2023-02-21</td><td>136.54.123.165</td><td>Marrom</td><td>T&eacute;cnico De Som</td><td>+55 (084) 5878-3346</td><td>Brasil</td><td>MT</td><td>5</td></tr>\n",
       "<tr><td>520.831.796-68</td><td>joaquim57@ig.com.br</td><td>&amp;2E1NY+ARc</td><td>1912-05-25</td><td>BA</td><td>2023-01-25</td><td>78.196.255.126</td><td>Rosa</td><td>Esteticista</td><td>41 7914-3753</td><td>Brasil</td><td>BA</td><td>5</td></tr>\n",
       "<tr><td>413.087.526-44</td><td>alexiada-rocha@ig...</td><td>@f@!Z!2c*2</td><td>1920-05-26</td><td>MS</td><td>2023-02-18</td><td>182.61.65.201</td><td>Laranja</td><td>Microfonista</td><td>(021) 3739-2944</td><td>Brasil</td><td>MS</td><td>5</td></tr>\n",
       "<tr><td>509.287.143-14</td><td>pmendes@gmail.com</td><td>M+^XDBfe(2</td><td>1938-03-11</td><td>RR</td><td>2023-01-19</td><td>61.234.208.17</td><td>Verde Claro</td><td>Fiscal</td><td>+55 (084) 5940-1932</td><td>Brasil</td><td>RR</td><td>5</td></tr>\n",
       "<tr><td>563.170.492-70</td><td>renan46@bol.com.br</td><td>Rp%2pVqfe$</td><td>1922-09-01</td><td>PA</td><td>2023-02-01</td><td>122.203.83.177</td><td>Verde Escuro</td><td>Contabilista</td><td>+55 41 6248 5773</td><td>Brasil</td><td>PA</td><td>5</td></tr>\n",
       "<tr><td>098.712.346-78</td><td>manuelada-rosa@ho...</td><td>#hEIEOztQ3</td><td>2013-01-10</td><td>SE</td><td>2023-01-07</td><td>175.18.73.211</td><td>Violeta</td><td>Seguidor De Compras</td><td>51 0278 0564</td><td>Brasil</td><td>SE</td><td>5</td></tr>\n",
       "<tr><td>345.709.261-34</td><td>usales@ig.com.br</td><td>b_8xaY$ozJ</td><td>1992-10-16</td><td>AM</td><td>2023-01-05</td><td>97.93.29.75</td><td>Rosa</td><td>Almirante</td><td>(084) 8993-4521</td><td>Brasil</td><td>AM</td><td>5</td></tr>\n",
       "<tr><td>318.254.906-51</td><td>pereiranicolas@ho...</td><td>YA9I85Wb+2</td><td>1945-11-18</td><td>MA</td><td>2023-01-24</td><td>182.195.33.137</td><td>Amarelo</td><td>Consultor De Moda</td><td>(084) 4591 3795</td><td>Brasil</td><td>MA</td><td>5</td></tr>\n",
       "<tr><td>610.287.453-62</td><td>cardosolivia@ig.c...</td><td>yXkDW7Ebh*</td><td>1915-11-15</td><td>GO</td><td>2023-01-22</td><td>195.194.63.184</td><td>Laranja</td><td>Barbeiro</td><td>+55 61 8967-9563</td><td>Brasil</td><td>GO</td><td>5</td></tr>\n",
       "<tr><td>615.790.842-49</td><td>brenda39@ig.com.br</td><td>XS$84Hpsnw</td><td>1964-02-01</td><td>RJ</td><td>2023-02-16</td><td>21.252.226.29</td><td>Branco</td><td>Promotor De Vendas</td><td>+55 (021) 4751 2004</td><td>Brasil</td><td>RJ</td><td>5</td></tr>\n",
       "<tr><td>624.095.138-24</td><td>luizapeixoto@gmai...</td><td>TbKy82Kda$</td><td>1959-02-01</td><td>DF</td><td>2023-02-25</td><td>105.107.23.13</td><td>Ciano</td><td>Implantodontista</td><td>84 1293 1906</td><td>Brasil</td><td>DF</td><td>5</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------+--------------------+----------+------------------+------+-------------+---------------+------------+--------------------+-------------------+------+------------+---+\n",
       "|           cpf|               email|     senha|data_de_nascimento|estado|data_cadastro|           ipv4|cor_favorita|           profissao|           telefone|  pais|sigla_estado|num|\n",
       "+--------------+--------------------+----------+------------------+------+-------------+---------------+------------+--------------------+-------------------+------+------------+---+\n",
       "|981.507.362-12|pedro-lucas53@gma...|+7^7E%xFBc|        2006-12-18|    RR|   2023-02-26| 99.107.250.210|        Roxo|    Jogador De Golfe|       31 7785-4046|Brasil|          RR|  5|\n",
       "|493.705.168-75|rezendeisaac@hotm...|_O_2GRnGOe|        1992-06-17|    GO|   2023-02-16|  197.11.26.213|       Ciano|Atleta De Arremes...|    (031) 0803-6753|Brasil|          GO|  5|\n",
       "|398.471.625-73|felipepires@uol.c...|*Aw5EOAvy9|        1921-11-11|    MG|   2023-01-02|   181.90.63.58|        Azul|      Papiloscopista|       11 9674-0553|Brasil|          MG|  5|\n",
       "|092.618.354-06|stellamoraes@bol....|mw0AWYAs#s|        2021-06-01|    AC|   2023-01-08|  26.121.127.94|      Marrom|            Aeromoça|+55 (071) 3033 9177|Brasil|          AC|  5|\n",
       "|509.427.136-99| wcarvalho@ig.com.br|pGD%!2Pq5X|        1969-10-28|    AP|   2023-02-14|  76.184.52.163|     Laranja|       Fonoaudiólogo|+55 (071) 6272 2468|Brasil|          AP|  5|\n",
       "|218.795.460-94|da-conceicaodavi-...|uhBbFxPA&9|        1986-05-19|    MG|   2023-03-07|    192.93.0.24|        Rosa|             Taxista|   +55 84 0652 9691|Brasil|          MG|  5|\n",
       "|715.836.940-48| efreitas@bol.com.br|s#q9VZt&xl|        2018-04-20|    MG|   2023-01-13| 76.251.188.148|      Branco|Produtor De Audio...|+55 (084) 1363 0052|Brasil|          MG|  5|\n",
       "|475.698.032-56|   wnunes@bol.com.br|_8az1W%n7g|        1996-05-12|    SE|   2023-02-04|139.196.176.154|        Azul|          Cadeirinha|    (071) 1640-3388|Brasil|          SE|  5|\n",
       "|217.639.540-99| jribeiro@bol.com.br|MEf1X7fj_0|        2021-10-05|    PA|   2023-03-02|    71.22.224.5|      Marrom|             Geólogo|       21 1432 4092|Brasil|          PA|  5|\n",
       "|261.938.750-77|  murilo05@gmail.com|Te&gO7GkKs|        1917-01-05|    MT|   2023-02-21| 136.54.123.165|      Marrom|      Técnico De Som|+55 (084) 5878-3346|Brasil|          MT|  5|\n",
       "|520.831.796-68| joaquim57@ig.com.br|&2E1NY+ARc|        1912-05-25|    BA|   2023-01-25| 78.196.255.126|        Rosa|         Esteticista|       41 7914-3753|Brasil|          BA|  5|\n",
       "|413.087.526-44|alexiada-rocha@ig...|@f@!Z!2c*2|        1920-05-26|    MS|   2023-02-18|  182.61.65.201|     Laranja|        Microfonista|    (021) 3739-2944|Brasil|          MS|  5|\n",
       "|509.287.143-14|   pmendes@gmail.com|M+^XDBfe(2|        1938-03-11|    RR|   2023-01-19|  61.234.208.17| Verde Claro|              Fiscal|+55 (084) 5940-1932|Brasil|          RR|  5|\n",
       "|563.170.492-70|  renan46@bol.com.br|Rp%2pVqfe$|        1922-09-01|    PA|   2023-02-01| 122.203.83.177|Verde Escuro|        Contabilista|   +55 41 6248 5773|Brasil|          PA|  5|\n",
       "|098.712.346-78|manuelada-rosa@ho...|#hEIEOztQ3|        2013-01-10|    SE|   2023-01-07|  175.18.73.211|     Violeta| Seguidor De Compras|       51 0278 0564|Brasil|          SE|  5|\n",
       "|345.709.261-34|    usales@ig.com.br|b_8xaY$ozJ|        1992-10-16|    AM|   2023-01-05|    97.93.29.75|        Rosa|           Almirante|    (084) 8993-4521|Brasil|          AM|  5|\n",
       "|318.254.906-51|pereiranicolas@ho...|YA9I85Wb+2|        1945-11-18|    MA|   2023-01-24| 182.195.33.137|     Amarelo|   Consultor De Moda|    (084) 4591 3795|Brasil|          MA|  5|\n",
       "|610.287.453-62|cardosolivia@ig.c...|yXkDW7Ebh*|        1915-11-15|    GO|   2023-01-22| 195.194.63.184|     Laranja|            Barbeiro|   +55 61 8967-9563|Brasil|          GO|  5|\n",
       "|615.790.842-49|  brenda39@ig.com.br|XS$84Hpsnw|        1964-02-01|    RJ|   2023-02-16|  21.252.226.29|      Branco|  Promotor De Vendas|+55 (021) 4751 2004|Brasil|          RJ|  5|\n",
       "|624.095.138-24|luizapeixoto@gmai...|TbKy82Kda$|        1959-02-01|    DF|   2023-02-25|  105.107.23.13|       Ciano|    Implantodontista|       84 1293 1906|Brasil|          DF|  5|\n",
       "+--------------+--------------------+----------+------------------+------+-------------+---------------+------------+--------------------+-------------------+------+------------+---+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cria uma nova coluna copiando outra coluna\n",
    "# df.withColumn('pais', F.lit('Brasil')).withColumn('sigla_estado', F.col('estado'))\n",
    "\n",
    "# Criando uma coluna numérica\n",
    "df.withColumn('pais', F.lit('Brasil')).withColumn('sigla_estado', F.col('estado')).withColumn('num', F.lit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>cpf</th><th>email</th><th>senha</th><th>data_de_nascimento</th><th>estado</th><th>data_cadastro</th><th>ipv4</th><th>cor_favorita</th><th>profissao</th><th>telefone</th><th>nome_estados</th><th>flag_rosa</th></tr>\n",
       "<tr><td>981.507.362-12</td><td>pedro-lucas53@gma...</td><td>+7^7E%xFBc</td><td>2006-12-18</td><td>RR</td><td>2023-02-26</td><td>99.107.250.210</td><td>Roxo</td><td>Jogador De Golfe</td><td>31 7785-4046</td><td>Outros</td><td>0</td></tr>\n",
       "<tr><td>493.705.168-75</td><td>rezendeisaac@hotm...</td><td>_O_2GRnGOe</td><td>1992-06-17</td><td>GO</td><td>2023-02-16</td><td>197.11.26.213</td><td>Ciano</td><td>Atleta De Arremes...</td><td>(031) 0803-6753</td><td>Outros</td><td>0</td></tr>\n",
       "<tr><td>398.471.625-73</td><td>felipepires@uol.c...</td><td>*Aw5EOAvy9</td><td>1921-11-11</td><td>MG</td><td>2023-01-02</td><td>181.90.63.58</td><td>Azul</td><td>Papiloscopista</td><td>11 9674-0553</td><td>Outros</td><td>0</td></tr>\n",
       "<tr><td>092.618.354-06</td><td>stellamoraes@bol....</td><td>mw0AWYAs#s</td><td>2021-06-01</td><td>AC</td><td>2023-01-08</td><td>26.121.127.94</td><td>Marrom</td><td>Aeromo&ccedil;a</td><td>+55 (071) 3033 9177</td><td>Acre</td><td>0</td></tr>\n",
       "<tr><td>509.427.136-99</td><td>wcarvalho@ig.com.br</td><td>pGD%!2Pq5X</td><td>1969-10-28</td><td>AP</td><td>2023-02-14</td><td>76.184.52.163</td><td>Laranja</td><td>Fonoaudi&oacute;logo</td><td>+55 (071) 6272 2468</td><td>Amap&aacute;</td><td>0</td></tr>\n",
       "<tr><td>218.795.460-94</td><td>da-conceicaodavi-...</td><td>uhBbFxPA&amp;9</td><td>1986-05-19</td><td>MG</td><td>2023-03-07</td><td>192.93.0.24</td><td>Rosa</td><td>Taxista</td><td>+55 84 0652 9691</td><td>Outros</td><td>1</td></tr>\n",
       "<tr><td>715.836.940-48</td><td>efreitas@bol.com.br</td><td>s#q9VZt&amp;xl</td><td>2018-04-20</td><td>MG</td><td>2023-01-13</td><td>76.251.188.148</td><td>Branco</td><td>Produtor De Audio...</td><td>+55 (084) 1363 0052</td><td>Outros</td><td>0</td></tr>\n",
       "<tr><td>475.698.032-56</td><td>wnunes@bol.com.br</td><td>_8az1W%n7g</td><td>1996-05-12</td><td>SE</td><td>2023-02-04</td><td>139.196.176.154</td><td>Azul</td><td>Cadeirinha</td><td>(071) 1640-3388</td><td>Outros</td><td>0</td></tr>\n",
       "<tr><td>217.639.540-99</td><td>jribeiro@bol.com.br</td><td>MEf1X7fj_0</td><td>2021-10-05</td><td>PA</td><td>2023-03-02</td><td>71.22.224.5</td><td>Marrom</td><td>Ge&oacute;logo</td><td>21 1432 4092</td><td>Outros</td><td>0</td></tr>\n",
       "<tr><td>261.938.750-77</td><td>murilo05@gmail.com</td><td>Te&amp;gO7GkKs</td><td>1917-01-05</td><td>MT</td><td>2023-02-21</td><td>136.54.123.165</td><td>Marrom</td><td>T&eacute;cnico De Som</td><td>+55 (084) 5878-3346</td><td>Outros</td><td>0</td></tr>\n",
       "<tr><td>520.831.796-68</td><td>joaquim57@ig.com.br</td><td>&amp;2E1NY+ARc</td><td>1912-05-25</td><td>BA</td><td>2023-01-25</td><td>78.196.255.126</td><td>Rosa</td><td>Esteticista</td><td>41 7914-3753</td><td>Bahia</td><td>1</td></tr>\n",
       "<tr><td>413.087.526-44</td><td>alexiada-rocha@ig...</td><td>@f@!Z!2c*2</td><td>1920-05-26</td><td>MS</td><td>2023-02-18</td><td>182.61.65.201</td><td>Laranja</td><td>Microfonista</td><td>(021) 3739-2944</td><td>Outros</td><td>0</td></tr>\n",
       "<tr><td>509.287.143-14</td><td>pmendes@gmail.com</td><td>M+^XDBfe(2</td><td>1938-03-11</td><td>RR</td><td>2023-01-19</td><td>61.234.208.17</td><td>Verde Claro</td><td>Fiscal</td><td>+55 (084) 5940-1932</td><td>Outros</td><td>0</td></tr>\n",
       "<tr><td>563.170.492-70</td><td>renan46@bol.com.br</td><td>Rp%2pVqfe$</td><td>1922-09-01</td><td>PA</td><td>2023-02-01</td><td>122.203.83.177</td><td>Verde Escuro</td><td>Contabilista</td><td>+55 41 6248 5773</td><td>Outros</td><td>0</td></tr>\n",
       "<tr><td>098.712.346-78</td><td>manuelada-rosa@ho...</td><td>#hEIEOztQ3</td><td>2013-01-10</td><td>SE</td><td>2023-01-07</td><td>175.18.73.211</td><td>Violeta</td><td>Seguidor De Compras</td><td>51 0278 0564</td><td>Outros</td><td>0</td></tr>\n",
       "<tr><td>345.709.261-34</td><td>usales@ig.com.br</td><td>b_8xaY$ozJ</td><td>1992-10-16</td><td>AM</td><td>2023-01-05</td><td>97.93.29.75</td><td>Rosa</td><td>Almirante</td><td>(084) 8993-4521</td><td>Amazonas</td><td>1</td></tr>\n",
       "<tr><td>318.254.906-51</td><td>pereiranicolas@ho...</td><td>YA9I85Wb+2</td><td>1945-11-18</td><td>MA</td><td>2023-01-24</td><td>182.195.33.137</td><td>Amarelo</td><td>Consultor De Moda</td><td>(084) 4591 3795</td><td>Outros</td><td>0</td></tr>\n",
       "<tr><td>610.287.453-62</td><td>cardosolivia@ig.c...</td><td>yXkDW7Ebh*</td><td>1915-11-15</td><td>GO</td><td>2023-01-22</td><td>195.194.63.184</td><td>Laranja</td><td>Barbeiro</td><td>+55 61 8967-9563</td><td>Outros</td><td>0</td></tr>\n",
       "<tr><td>615.790.842-49</td><td>brenda39@ig.com.br</td><td>XS$84Hpsnw</td><td>1964-02-01</td><td>RJ</td><td>2023-02-16</td><td>21.252.226.29</td><td>Branco</td><td>Promotor De Vendas</td><td>+55 (021) 4751 2004</td><td>Outros</td><td>0</td></tr>\n",
       "<tr><td>624.095.138-24</td><td>luizapeixoto@gmai...</td><td>TbKy82Kda$</td><td>1959-02-01</td><td>DF</td><td>2023-02-25</td><td>105.107.23.13</td><td>Ciano</td><td>Implantodontista</td><td>84 1293 1906</td><td>Outros</td><td>0</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------+--------------------+----------+------------------+------+-------------+---------------+------------+--------------------+-------------------+------------+---------+\n",
       "|           cpf|               email|     senha|data_de_nascimento|estado|data_cadastro|           ipv4|cor_favorita|           profissao|           telefone|nome_estados|flag_rosa|\n",
       "+--------------+--------------------+----------+------------------+------+-------------+---------------+------------+--------------------+-------------------+------------+---------+\n",
       "|981.507.362-12|pedro-lucas53@gma...|+7^7E%xFBc|        2006-12-18|    RR|   2023-02-26| 99.107.250.210|        Roxo|    Jogador De Golfe|       31 7785-4046|      Outros|        0|\n",
       "|493.705.168-75|rezendeisaac@hotm...|_O_2GRnGOe|        1992-06-17|    GO|   2023-02-16|  197.11.26.213|       Ciano|Atleta De Arremes...|    (031) 0803-6753|      Outros|        0|\n",
       "|398.471.625-73|felipepires@uol.c...|*Aw5EOAvy9|        1921-11-11|    MG|   2023-01-02|   181.90.63.58|        Azul|      Papiloscopista|       11 9674-0553|      Outros|        0|\n",
       "|092.618.354-06|stellamoraes@bol....|mw0AWYAs#s|        2021-06-01|    AC|   2023-01-08|  26.121.127.94|      Marrom|            Aeromoça|+55 (071) 3033 9177|        Acre|        0|\n",
       "|509.427.136-99| wcarvalho@ig.com.br|pGD%!2Pq5X|        1969-10-28|    AP|   2023-02-14|  76.184.52.163|     Laranja|       Fonoaudiólogo|+55 (071) 6272 2468|       Amapá|        0|\n",
       "|218.795.460-94|da-conceicaodavi-...|uhBbFxPA&9|        1986-05-19|    MG|   2023-03-07|    192.93.0.24|        Rosa|             Taxista|   +55 84 0652 9691|      Outros|        1|\n",
       "|715.836.940-48| efreitas@bol.com.br|s#q9VZt&xl|        2018-04-20|    MG|   2023-01-13| 76.251.188.148|      Branco|Produtor De Audio...|+55 (084) 1363 0052|      Outros|        0|\n",
       "|475.698.032-56|   wnunes@bol.com.br|_8az1W%n7g|        1996-05-12|    SE|   2023-02-04|139.196.176.154|        Azul|          Cadeirinha|    (071) 1640-3388|      Outros|        0|\n",
       "|217.639.540-99| jribeiro@bol.com.br|MEf1X7fj_0|        2021-10-05|    PA|   2023-03-02|    71.22.224.5|      Marrom|             Geólogo|       21 1432 4092|      Outros|        0|\n",
       "|261.938.750-77|  murilo05@gmail.com|Te&gO7GkKs|        1917-01-05|    MT|   2023-02-21| 136.54.123.165|      Marrom|      Técnico De Som|+55 (084) 5878-3346|      Outros|        0|\n",
       "|520.831.796-68| joaquim57@ig.com.br|&2E1NY+ARc|        1912-05-25|    BA|   2023-01-25| 78.196.255.126|        Rosa|         Esteticista|       41 7914-3753|       Bahia|        1|\n",
       "|413.087.526-44|alexiada-rocha@ig...|@f@!Z!2c*2|        1920-05-26|    MS|   2023-02-18|  182.61.65.201|     Laranja|        Microfonista|    (021) 3739-2944|      Outros|        0|\n",
       "|509.287.143-14|   pmendes@gmail.com|M+^XDBfe(2|        1938-03-11|    RR|   2023-01-19|  61.234.208.17| Verde Claro|              Fiscal|+55 (084) 5940-1932|      Outros|        0|\n",
       "|563.170.492-70|  renan46@bol.com.br|Rp%2pVqfe$|        1922-09-01|    PA|   2023-02-01| 122.203.83.177|Verde Escuro|        Contabilista|   +55 41 6248 5773|      Outros|        0|\n",
       "|098.712.346-78|manuelada-rosa@ho...|#hEIEOztQ3|        2013-01-10|    SE|   2023-01-07|  175.18.73.211|     Violeta| Seguidor De Compras|       51 0278 0564|      Outros|        0|\n",
       "|345.709.261-34|    usales@ig.com.br|b_8xaY$ozJ|        1992-10-16|    AM|   2023-01-05|    97.93.29.75|        Rosa|           Almirante|    (084) 8993-4521|    Amazonas|        1|\n",
       "|318.254.906-51|pereiranicolas@ho...|YA9I85Wb+2|        1945-11-18|    MA|   2023-01-24| 182.195.33.137|     Amarelo|   Consultor De Moda|    (084) 4591 3795|      Outros|        0|\n",
       "|610.287.453-62|cardosolivia@ig.c...|yXkDW7Ebh*|        1915-11-15|    GO|   2023-01-22| 195.194.63.184|     Laranja|            Barbeiro|   +55 61 8967-9563|      Outros|        0|\n",
       "|615.790.842-49|  brenda39@ig.com.br|XS$84Hpsnw|        1964-02-01|    RJ|   2023-02-16|  21.252.226.29|      Branco|  Promotor De Vendas|+55 (021) 4751 2004|      Outros|        0|\n",
       "|624.095.138-24|luizapeixoto@gmai...|TbKy82Kda$|        1959-02-01|    DF|   2023-02-25|  105.107.23.13|       Ciano|    Implantodontista|       84 1293 1906|      Outros|        0|\n",
       "+--------------+--------------------+----------+------------------+------+-------------+---------------+------------+--------------------+-------------------+------------+---------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando uma nova coluna com o nome dos estados por estenso\n",
    "\n",
    "(\n",
    "    df\n",
    "    .withColumn('nome_estados', F.when(df.estado == 'AC', 'Acre')\n",
    "                                 .when(df.estado == 'AL', 'Alagoas')\n",
    "                                 .when(df.estado == 'AP', 'Amapá')\n",
    "                                 .when(df.estado == \"AM\", \"Amazonas\")\n",
    "                                 .when(df.estado == \"BA\", \"Bahia\")\n",
    "                                 .when(df.estado == \"CE\", \"Ceará\")\n",
    "                                 # utilizado para todas as demais colunas\n",
    "                                 .otherwise('Outros')\n",
    "                )\n",
    "    .withColumn('flag_rosa', F.when(F.col('cor_favorita') == 'Rosa', 1).otherwise(0))\n",
    ")\n",
    "\n",
    "\n",
    "# determinando todos os estados com inteligencia artificial\n",
    "\n",
    "# (\n",
    "#     df\n",
    "#     .withColumn('nome_estados', F.when(df.estado == 'AC', 'Acre')\n",
    "#                                  .when(df.estado == 'AL', 'Alagoas')\n",
    "#                                  .when(df.estado == 'AP', 'Amapá')\n",
    "#                                  .when(df.estado == \"AM\", \"Amazonas\")\n",
    "#                                  .when(df.estado == \"BA\", \"Bahia\")\n",
    "#                                  .when(df.estado == \"CE\", \"Ceará\")\n",
    "#                                  .when(df.estado == \"DF\", \"Distrito Federal\")\n",
    "#                                  .when(df.estado == \"ES\", \"Espírito Santo\")\n",
    "#                                  .when(df.estado == \"GO\", \"Goiás\")\n",
    "#                                  .when(df.estado == \"MA\", \"Maranhão\")\n",
    "#                                  .when(df.estado == \"MT\", \"Mato Grosso\")\n",
    "#                                  .when(df.estado == \"MS\", \"Mato Grosso do Sul\")\n",
    "#                                  .when(df.estado == \"MG\", \"Minas Gerais\")\n",
    "#                                  .when(df.estado == \"PA\", \"Pará\")\n",
    "#                                  .when(df.estado == \"PB\", \"Paraíba\")\n",
    "#                                  .when(df.estado == \"PR\", \"Paraná\")\n",
    "#                                  .when(df.estado == \"PE\", \"Pernambuco\")\n",
    "#                                  .when(df.estado == \"PI\", \"Piauí\")\n",
    "#                                  .when(df.estado == \"RJ\", \"Rio de Janeiro\")\n",
    "#                                  .when(df.estado == \"RN\", \"Rio Grande do Norte\")\n",
    "#                                  .when(df.estado == \"RS\", \"Rio Grande do Sul\")\n",
    "#                                  .when(df.estado == \"RO\", \"Rondônia\")\n",
    "#                                  .when(df.estado == \"RR\", \"Roraima\")\n",
    "#                                  .when(df.estado == \"SC\", \"Santa Catarina\")\n",
    "#                                  .when(df.estado == \"SP\", \"São Paulo\")\n",
    "#                                  .when(df.estado == \"SE\", \"Sergipe\")\n",
    "#                                  .when(df.estado == \"TO\", \"Tocantins\")\n",
    "#                 )\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
